{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NodeRank Algorithm\n",
    "Includes code for requesting page content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "def getUA():\n",
    "    uastrings = [\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\",\\\n",
    "                \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1500.72 Safari/537.36\",\\\n",
    "                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10) AppleWebKit/600.1.25 (KHTML, like Gecko) Version/8.0 Safari/600.1.25\",\\\n",
    "                \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0\",\\\n",
    "                \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\",\\\n",
    "                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\",\\\n",
    "                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/600.1.17 (KHTML, like Gecko) Version/7.1 Safari/537.85.10\",\\\n",
    "                \"Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko\",\\\n",
    "                \"Mozilla/5.0 (Windows NT 6.3; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0\",\\\n",
    "                \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.104 Safari/537.36\"\\\n",
    "                ]\n",
    "\n",
    "    return random.choice(uastrings)\n",
    "\n",
    "def prepare_sentences(text):\n",
    "    \n",
    "    # Clean characters\n",
    "    text = \"\".join([t for t in text if t.isalnum() or t in string.punctuation or t == ' '])\n",
    "    \n",
    "    # Get rid of whitespace characters\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    text = \" \".join([s.strip() for s in sentences])\n",
    "    \n",
    "    # Fix puctuation spacing\n",
    "    text = re.sub(r\"(\\w{2,})([\\.\\!\\?]+)(\\w)\", r\"\\1\\2 \\3\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def find_children(n1, n2):\n",
    "    \n",
    "    children1 = [c for c in n1.children]\n",
    "    children2 = [c for c in n2.children]\n",
    "    \n",
    "    for i, children in enumerate([children1,children2]):\n",
    "        level = 1\n",
    "\n",
    "        while children:\n",
    "            lchildren = children\n",
    "            children = []\n",
    "            for ch in lchildren:\n",
    "                if not ch.name:\n",
    "                    continue\n",
    "                if ch == n1 or ch == n2:\n",
    "                    if i == 0:\n",
    "                        return len( nltk.sent_tokenize(n2.get_text()) ) * ( 1/math.exp(level) )\n",
    "                    else:\n",
    "                        return len( nltk.sent_tokenize(n1.get_text()) ) * ( 1/math.exp(level) )\n",
    "                                        \n",
    "                children.extend(ch.children)\n",
    "\n",
    "            level += 1\n",
    "            \n",
    "    return 0\n",
    "\n",
    "\n",
    "def find_content(html):\n",
    "    \n",
    "    # Get body and extract out non-content nodes\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    body = soup.find('body')\n",
    "    remove = [s.extract() for s in body([\"script\", \"style\",\"iframe\",\"noscript\",\"nav\",\"footer\",\"header\", \"svg\", \"h1\",\"h2\", \"h3\", \"h4\", \"h5\", \"xml\"])]\n",
    "    text = \"\"\n",
    "    \n",
    "    # Further clean nodes and build a lookup list\n",
    "    nodes = body.findAll()\n",
    "    nodes = [n for n in nodes if n.name and n.get_text()]\n",
    "    nodeix = [i for i,v in enumerate(nodes)]    \n",
    "\n",
    "    # Build similarity matrix and use number of sentences child resursion depth as metric\n",
    "    sim_mat = np.zeros([len(nodes), len(nodes)])\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(len(nodes)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = find_children(nodes[i], nodes[j])\n",
    "\n",
    "    # Run pagerank algorithm on matrix\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    \n",
    "    # Sort nodex by best and get text of best node\n",
    "    ranked_nodes = sorted(((scores[i],s) for i,s in enumerate(nodeix)), reverse=True)\n",
    "    text = nodes[ranked_nodes[0][1]].get_text()\n",
    "    \n",
    "    return prepare_sentences(text)\n",
    "\n",
    "\n",
    "def extract_content_noderank(url= None, html=None, timeout=10):\n",
    "    if url:\n",
    "        return find_content(extract_html(url, timeout))\n",
    "    elif html:\n",
    "        return find_content(html)\n",
    "    else:\n",
    "        raise Exception('Neither `url` nor `html` was suplied.')\n",
    "    \n",
    "\n",
    "def extract_html(url, timeout=10):\n",
    "    headers = {'user-agent': getUA()}\n",
    "    r = requests.get(url, headers = headers, verify=False, timeout=timeout)\n",
    "    html = r.content\n",
    "    return html\n",
    "\n",
    "#print(extract_content_pagerank(url, timeout=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Boilerpipe and Dragnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from boilerpipe.extract import Extractor\n",
    "from dragnet import extract_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample URLs taken from Hacker News on 2/12/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [\"https://remysharp.com/2019/02/12/cern-day-1\",\n",
    "\"https://www.nytimes.com/2019/02/12/magazine/climeworks-business-climate-change.html\",\n",
    "\"https://kishorepv.github.io/The-value-of-Incremental_learning/\",\n",
    "\"https://e360.yale.edu/digest/arborists-have-cloned-ancient-redwoods-from-their-massive-stumps\",\n",
    "\"https://alexanderperrin.com.au/triangles/ballooning/\",\n",
    "\"http://www.randomhacks.net/2005/10/11/amb-operator/\",\n",
    "\"https://www.zdnet.com/article/microsoft-security-chief-ie-is-not-a-browser-so-stop-using-it-as-your-default/\",\n",
    "\"https://github.com/Jeff-Ciesielski/synesthesia\",\n",
    "\"https://www.nowpublishers.com/article/Details/RBE-0092\",\n",
    "\"https://eng.uber.com/introducing-ludwig/\",\n",
    "\"https://www.jasonhickel.org/blog/2019/2/3/pinker-and-global-poverty\",\n",
    "\"https://www.nytimes.com/2019/02/11/health/artificial-intelligence-medical-diagnosis.html\",\n",
    "\"https://www.cnbc.com/2019/02/12/google-facebook-apple-news-should-be-regulated-uk-government-report.html\",\n",
    "\"https://techcrunch.com/2019/02/11/amazon-is-buying-home-mesh-router-startup-eero/\",\n",
    "\"http://www.greatdisasters.co.uk/the-de-havilland-comet/\",\n",
    "\"https://techcrunch.com/2019/02/11/google-docs-gets-an-api-for-task-automation/\",\n",
    "\"https://ephemeralnewyork.wordpress.com/2019/02/11/the-bobbed-hair-bandit-on-the-run-in-brooklyn/\",\n",
    "\"https://www.cbc.ca/news/technology/mars-one-bankrupt-1.5014522\",\n",
    "\"https://medium.com/@shnatsel/how-rusts-standard-library-was-vulnerable-for-years-and-nobody-noticed-aebf0503c3d6\",\n",
    "\"https://opensource.zalando.com/blog/2019/02/Open-Source-Harassment-Policy/\",\n",
    "\"https://blog.parse.ly/post/7689/analyst-demystify-traffic-google-sends-publishers/\",\n",
    "\"https://www.nytimes.com/2019/02/11/travel/northern-lights-tourism-in-sweden.html\",\n",
    "\"https://techcrunch.com/2019/02/11/us-iphone-users-spent-79-last-year-up-36-from-2017/\",\n",
    "\"https://blog.wolfram.com/2019/02/01/the-data-science-of-mathoverflow/\",\n",
    "\"http://www.cat-bus.com/2017/12/gadgetbahn/\",\n",
    "\"https://source.android.com/security/bulletin/2019-02-01.html\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Content Extractions for NodeRank, Dragnet, and BoilerPipe\n",
    "One of the key components in the design of NodeRank was the ability to run on AWS Lambda using the restrictive environments of Amazon Linux AMIs. In addition, Dragnet is difficult, if not impossible, to compile on Windows due to dependencies on GNU and Boilerpipe relies on a dependency to the JDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: https://remysharp.com/2019/02/12/cern-day-1\n",
      "NodeRank:\n",
      "This marks the beginning of a week long adventure in Geneva Switzerland at CERN, to work on a hackproject. The project is to rebuilding the very first web browser, aptly called WorldWideWeb (though shortly thereafter being renamed to Nexus, sincethe whole world wide web thing being a bigger deal). This browser was written by Sir Tim Berners-Lee in 1990 and the project marks the 30th anniversary of theweb. This event also reunites most of the team that made up the 2013 hack project to recreate the Line Mode Browser. On being asked if I was interested in returning, I jumped at the chance. It's CERN. There's some proper smarties rolling around here. Maybe some of that will rub off onme! The project is a quasi historical restoration mixed with simulation as we bring the original browser to the public via modern technology, specifically and ironically, via today'sbrowsers. The first day is always a lot of finding our feet. Trying to articulate (to ourselves) what the scope of the problem is, what we need to achieve and what we want toachieve. This time around is made a little trickier (or interesting?) as there's more mixed overlap in arrivals of team members. It'll only be a single day on Wednesday that we'll be at full capacity, but compared to 2013's efforts, we have a full week to pull the job off rather than 3days. The aim of the morning is to grasp exactly what this browser did, how it did it, and what it lookedlike. We've sourced videos, emulators and most importantly, and impressively, a NeXTcube machine straight from the museum has been delivered to our (war room? I want to say war room, but there's not much fighting going on)room. One of the main challenges we faced (and still face at time of writing) is that we want the WorldWideWeb to run on the NeXTcube - and oddly this machine has a number of browsers, but none of them are the WorldWideWeb :-\\Somehow we'll deliver the WorldWideWeb. app directly to the machineif only we could work out how to network the machineMy role in the team is code. Firstly the server side aspect to the simulation. Then once that's solved, any interaction in the browser where we'll simulate the NeXTcube desktop and opening windows to the WorldWideWeb. The server part is relatively small andinvolves:Proxying requests to collect HTML and return it the clientBlocking all internal .cern. ch sites with a handful of whitelisted exceptionsIn the returned HTML, strip out any unsupported tags (like IMG which appeared later in Mosaic - TIL: was named because it would fit together pieces like HTTP, FTP, Gopher and NNTP).This part was partially lifted from the original Line Mode Browser source and cleaned up for the 5½ years worth of new knowledge I had:)Then on with some traditional fondue with our team and some rest when tomorrow we might try to make some of the UI cometogether. Posted 12-Feb 2019 under personal.\n",
      "Sentences: 24\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "This marks the beginning of a week long adventure in Geneva Switzerland at CERN, to work on a hackproject. The project is to rebuilding the very first web browser, aptly called WorldWideWeb (though shortly thereafter being renamed to Nexus, sincethe whole world wide web thing being a bigger deal). This browser was written by Sir Tim Berners-Lee in 1990 and the project marks the 30th anniversary of theweb. This event also reunites most of the team that made up the 2013 hack project to recreate the Line Mode Browser. On being asked if I was interested in returning, I jumped at the chance. It's CERN. There's some proper smarties rolling around here. Maybe some of that will rub off onme! The project is a quasi historical restoration mixed with simulation as we bring the original browser to the public via modern technology, specifically and ironically, via today'sbrowsers. Day 1The first day is always a lot of finding our feet. Trying to articulate (to ourselves) what the scope of the problem is, what we need to achieve and what we want toachieve. This time around is made a little trickier (or interesting?) as there's more mixed overlap in arrivals of team members. It'll only be a single day on Wednesday that we'll be at full capacity, but compared to 2013's efforts, we have a full week to pull the job off rather than 3days. The WorldWideWebThe aim of the morning is to grasp exactly what this browser did, how it did it, and what it lookedlike. We've sourced videos , emulators and most importantly, and impressively, a NeXTcube machine straight from the museum has been delivered to our (war room? I want to say war room, but there's not much fighting going on)room. One of the main challenges we faced (and still face at time of writing) is that we want the WorldWideWeb to run on the NeXTcube - and oddly this machine has a number of browsers, but none of them are the WorldWideWeb :-\\Somehow we'll deliver the WorldWideWeb. app directly to the machineif only we could work out how to network the machineMy role in the team is code. Firstly the server side aspect to the simulation. Then once that's solved, any interaction in the browser where we'll simulate the NeXTcube desktop and opening windows to the WorldWideWeb. The server part is relatively small andinvolves: Proxying requests to collect HTML and return it the client Blocking all internal .cern. ch sites with a handful of whitelisted exceptions In the returned HTML, strip out any unsupported tags (like IMG which appeared later in Mosaic - TIL: was named because it would fit together pieces like HTTP, FTP, Gopher and NNTP).This part was partially lifted from the original Line Mode Browser source and cleaned up for the 5½ years worth of new knowledge I had:)Then on with some traditional fondue with our team and some rest when tomorrow we might try to make some of the UI cometogether. Posted 12-Feb 2019 under personal. Want more? Posts, web development learnings & insights, exclusive workshop and training discounts and more, direct to your inbox. I won't send you any spam, and you can unsubscribe at any time. Powered by ConvertKitNow you'll need check your email to confirm your subscription. There was an error submitting your subscription. Please try again.\n",
      "Sentences: 30\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "(edit)This marks the beginning of a week long adventure in Geneva Switzerland at CERN, to work on a hackproject. The project is to rebuilding the very first web browser, aptly called WorldWideWeb (though shortly thereafter being renamed to Nexus, sincethe whole world wide web thing being a bigger deal). This browser was written by Sir Tim Berners-Lee in 1990 and the project marks the 30th anniversary of theweb. This event also reunites most of the team that made up the 2013 hack project to recreate the Line Mode Browser. On being asked if I was interested in returning, I jumped at the chance. It's CERN. There's some proper smarties rolling around here. Maybe some of that will rub off onme! The project is a quasi historical restoration mixed with simulation as we bring the original browser to the public via modern technology, specifically and ironically, via today'sbrowsers. Day 1The first day is always a lot of finding our feet. Trying to articulate (to ourselves) what the scope of the problem is, what we need to achieve and what we want toachieve. This time around is made a little trickier (or interesting?) as there's more mixed overlap in arrivals of team members. It'll only be a single day on Wednesday that we'll be at full capacity, but compared to 2013's efforts, we have a full week to pull the job off rather than 3days. The WorldWideWebThe aim of the morning is to grasp exactly what this browser did, how it did it, and what it lookedlike. We've sourced videos , emulators and most importantly, and impressively, a NeXTcube machine straight from the museum has been delivered to our (war room? I want to say war room, but there's not much fighting going on)room. One of the main challenges we faced (and still face at time of writing) is that we want the WorldWideWeb to run on the NeXTcube - and oddly this machine has a number of browsers, but none of them are the WorldWideWeb :-\\Somehow we'll deliver the WorldWideWeb. app directly to the machineif only we could work out how to network the machineSimulationMy role in the team is code. Firstly the server side aspect to the simulation. Then once that's solved, any interaction in the browser where we'll simulate the NeXTcube desktop and opening windows to the WorldWideWeb. The server part is relatively small andinvolves:Proxying requests to collect HTML and return it the clientBlocking all internal .cern. ch sites with a handful of whitelisted exceptionsIn the returned HTML, strip out any unsupported tags (like IMG which appeared later in Mosaic - TIL: was named because it would fit together pieces like HTTP, FTP, Gopher and NNTP).This part was partially lifted from the original Line Mode Browser source and cleaned up for the 5½ years worth of new knowledge I had:)Then on with some traditional fondue with our team and some rest when tomorrow we might try to make some of the UI cometogether. Posted 12-Feb 2019 under personal. Want more? Posts, web development learnings & insights, exclusive workshop and training discounts and more, direct to your inbox. I won't send you any spam, and you can unsubscribe at any time. Powered by ConvertKitAwesome, thanks so much! Now you'll need check your email to confirm your subscription. There was an error submitting your subscription. Please try again. Your nameYour email addressWe use this field to detect spam bots. If you fill this in, you will be marked as a spammer. I'd like to receive the free command line mini course. Subscribe\n",
      "Sentences: 35\n",
      "\n",
      "\n",
      "Article: https://www.nytimes.com/2019/02/12/magazine/climeworks-business-climate-change.html\n",
      "NodeRank:\n",
      "Just over a century ago in Ludwigshafen, Germany, a scientist named Carl Bosch assembled a team of engineers to exploit a new technique in chemistry. A year earlier, another German chemist, Fritz Haber, hit upon a process to pull nitrogen (N) from the air and combine it with hydrogen (H) to produce tiny amounts of ammonia (NH₃). But Habers process was delicate, requiring the maintenance of high temperatures and high pressure. Bosch wanted to figure out how to adapt Habers discovery for commercial purposes  as we would say today, to scale it up. Anyone looking at the state of manufacturing in Europe around 1910, Bosch observed, could see that the task was daunting: The technology simply didnt exist. Over the next decade, however, Bosch and his team overcame a multitude of technological and metallurgical challenges. He chronicled them in his 1932 acceptance speech for the Nobel Prize for Chemistry  an honor he won because the Haber-Bosch process, as it came to be known, changed the world. His breakthrough made possible the production of ammonia on an industrial scale, providing the world with cheap and abundant fertilizer. The scientist and historian Vaclav Smil called Haber-Bosch the most important technical invention of the 20th century. Bosch had effectively removed the historical bounds on crop yields, so much so that he was widely credited with making bread from air. By some estimates, Boschs work made possible the lives of more than two billion human beings over the last 100 years. What the Haber-Bosch method had going for it, from the very start, was a ready market. Fertilizer was already in high demand, but it came primarily from limited natural reserves in far-flung locales  bird droppings scraped from remote islands near Peru, for instance, or mineral stores of nitrogen dug out of the Chilean desert. Because synthetic ammonia competed with existing products, it was able to follow a timeworn pattern of innovation. In much the same way that LEDs have supplanted fluorescent and incandescent bulbs (which in turn had displaced kerosene lamps and wax candles), a novel product or process often replaces something already in demand. If it is better or cheaper  and especially if it is better and cheaper  it usually wins in the marketplace. Haber-Bosch did exactly that. It may now be that another gas  carbon dioxide (CO₂)  can be removed from the air for commercial purposes, and that its removal could have a profound effect on the future of humanity. But its almost certainly too soon to say for sure. One sunny morning last October, several engineers from a Swiss firm called Climeworks ambled onto the roof of a power-generating waste-incineration plant in Hinwil, a village about 30 minutes outside Zurich. The technicians had in front of them 12 large devices, stacked in two rows of six, that resembled oversize front-loading clothes dryers. These were direct air capture machines, which soon would begin collecting carbon dioxide from air drawn in through their central ducts. Once trapped, the CO₂ would then be siphoned into large tanks and trucked to a local Coca-Cola bottler, where it would become the fizz in a soft drink. [Is It O.K. to Tinker With the Environment to Fight Climate Change? ]The machines themselves require a significant amount of energy. They depend on electric fans to pull air into the ducts and over a special material, known as a sorbent, laced with granules that chemically bind with CO₂; periodic blasts of heat then release the captured gas from the sorbent, with customized software managing the whole catch-and-release cycle. Climeworks had installed the machines on the roof of the power plant to tap into the plants low-carbon electricity and the heat from its incineration system. A few dozen yards away from the new installation sat an older stack of Climeworks machines, 18 in total, that had been whirring on the same rooftop for more than a year. So far, these machines had captured about 1,000 metric tons (or about 1,100 short tons) of carbon dioxide from the air and fed it, by pipeline, to an enormous greenhouse nearby, where it was plumping up tomatoes, eggplants and mâche. During a tour of the greenhouse, Paul Ruser, the manager, suggested I taste the results. Here, try one, he said, handing me a crisp, ripe cucumber he plucked from a nearby vine. It was the finest direct-air-capture cucumber Id ever had. Climeworkss rooftop plant represents something new in the world: the first direct-air-capture venture in history seeking to sell CO₂ by the ton. When the companys founders, Christoph Gebald and Jan Wurzbacher, began openly discussing their plans to build a business several years ago, they faced a deluge of skepticism. I would say nine out of 10 people reacted critically, Gebald told me. The first thing they said was: This will never work technically. And finally in 2017 we convinced them it works technically, since we built the big plant in Hinwil. But once we convinced them that it works technically, they would say, Well, it will never work economically. For the moment, skeptics of Climeworkss business plan are correct: The company is not turning a profit. To build and install the 18 units at Hinwil, hand-assembled in a second-floor workshop in Zurich, cost between $3 million and $4 million, which is the primary reason it costs the firm between $500 and $600 to remove a metric ton of CO₂ from the air. Even as the company has attracted about $50 million in private investments and grants, it faces the same daunting task that confronted Carl Bosch a century ago: How much can it bring costs down? And how fast can it scale up? Gebald and Wurzbacher believe the way to gain a commercial foothold is to sell their expensive CO₂ to agriculture or beverage companies. Not only do these companies require CO₂ anyway, some also seem willing to pay a premium for a vital ingredient they can use to help market their products as eco-friendly. Still, greenhouses and soda bubbles together represent a small global market  perhaps six million metric tons of CO₂ annually. And Gebald and Wurzbacher did not get into carbon capture to grow mâche or put bubbles in Fanta. They believe that over the next seven years they can bring expenses down to a level that would enable them to sell CO₂ into more lucrative markets. Air-captured CO₂ can be combined with hydrogen and then fashioned into any kind of fossil-fuel substitute you want. Instead of making bread from air, you can make fuels from air. Already, Climeworks and another company, Carbon Engineering, which is based in British Columbia, have moved aggressively on this idea; the Canadians have even lined up investors (including Bill Gates) to produce synthetic fuel at large industrial plants from air-captured CO₂. The ultimate goal for air capture, however, isnt to turn it into a product  at least not in the traditional sense. What Gebald and Wurzbacher really want to do is to pull vast amounts of CO₂ out of the atmosphere and bury it, forever, deep underground, and sell that service as an offset. Climeworkss captured CO₂ has already been injected deep into rock formations beneath Iceland; by the end of the year, the firm intends to deploy 50 units near Reykjavik to expand the operation. But at that point the company will be moving into uncharted economic territory  purveyors of a service that seems desperately needed to help slow climate change but does not, at present, replace anything on the consumer or industrial landscape. To complicate matters, a ton of buried CO₂ is not something that human beings or governments have shown much demand for. And so companies like Climeworks face a quandary: How do you sell something that never existed before, something that may never be cheap, into a market that is not yet real? Even the most enthusiastic believers in direct air capture stop short of describing it as a miracle technology. Its more frequently described as an old idea  scrubbers that remove CO₂ have been used in submarines since at least the 1950s  that is being radically upgraded for a variety of new applications. Its arguably the case, in fact, that when it comes to reducing our carbon emissions, direct air capture will be seen as an option thats too expensive and too modest in impact. The only way that direct air capture becomes meaningful is if we do all the other things we need to do promptly, Hal Harvey, a California energy analyst who studies climate-friendly technologies and policies, told me recently. Harvey and others make the case that the biggest, fastest and cheapest gains in addressing atmospheric carbon will come from switching our power grid to renewable energy or low-carbon electricity; from transitioning to electric vehicles and imposing stricter mileage regulations on gas-powered cars and trucks; and from requiring more energy-efficient buildings and appliances. In short, the best way to start making progress toward a decarbonized world is not to rev up millions of air capture machines right now. Its to stop putting CO₂ in the atmosphere in the first place. The future of carbon mitigation, however, is on a countdown timer, as atmospheric CO₂ concentrations have continued to rise. If the nations of the world were to continue on the current track, it would be impossible to meet the objectives of the 2016 Paris Agreement, which set a goal limiting warming to 2 degrees Celsius or, ideally, 1.5 degrees. And it would usher in a world of misery and economic hardship. Already, temperatures in some regions have climbed more than 1 degree Celsius, as a report by the Intergovernmental Panel on Climate Change noted last October. These temperature increases have led to an increase in droughts, heat waves, floods and biodiversity losses and make the chaos of 2 or 3 degrees additional warming seem inconceivable. A further problem is that maintaining todays emissions path for too long runs the risk of doing irreparable damage to the earths ecosystems  causing harm that no amount of technological innovation can make right. There is no reverse gear for natural systems, Harvey says. If they go, they go. If we defrost the tundra, its game over. The same might be said for the Greenland and West Antarctic ice sheets, or our coral reefs. Such resources have an asymmetry in their natural architectures: They can take thousands or millions of years to form, but could reach conditions of catastrophic decline in just a few decades. At the moment, global CO₂ emissions are about 37 billion metric tons per year, and were on track to raise temperatures by 3 degrees Celsius by 2100. To have a shot at maintaining a climate suitable for humans, the worlds nations most likely have to reduce CO₂ emissions drastically from the current level  to perhaps 15 billion or 20 billion metric tons per year by 2030; then, through some kind of unprecedented political and industrial effort, we need to bring carbon emissions to zero by around 2050. In this context, Climeworkss effort to collect 1,000 metric tons of CO₂ on a rooftop near Zurich might seem like bailing out the ocean one bucket at a time. Conceptually, however, its important. Last years I.P.C.C. report noted that it may be impossible to limit warming to 1.5 degrees by 2100 through only a rapid switch to clean energy, electric cars and the like. To preserve a livable environment we may also need to extract CO₂ from the atmosphere. As Wurzbacher put it, if you take all these numbers from the I.P.C.C., you end up with something like eight to 10 billion tons  gigatons  of CO₂ that need to be removed from the air every year, if we are serious about 1.5 or 2 degrees. There happens to be a name for things that can do this kind of extraction work: negative-emissions technologies, or NETs. Some NETs, like trees and plants, predate us and probably dont deserve the label. Through photosynthesis, our forests take extraordinary amounts of carbon dioxide from the atmosphere, and if we were to magnify efforts to reforest clear-cut areas  or plant new groves, a process known as afforestation  we could absorb billions more metric tons of carbon in future years. Whats more, we could grow crops specifically to absorb CO₂ and then burn them for power generation, with the intention of capturing the power-plant emissions and pumping them underground, a process known as bioenergy with carbon capture and storage, or BECCS. Other negative-emissions technologies include manipulating farmland soil or coastal wetlands so they will trap more atmospheric carbon and grinding up mineral formations so they will absorb CO₂ more readily, a process known as enhanced weathering. Negative emissions can be thought of as a form of time travel. Ever since the Industrial Revolution, human societies have produced an excess of CO₂, by taking carbon stores from deep inside the earth  in the form of coal, oil and gas  and from stores aboveground (mostly wood), then putting it into the atmosphere by burning it. It has become imperative to reverse the process  that is, take CO₂ out of the air and either restore it deep inside the earth or contain it within new surface ecosystems. This is certainly easier to prescribe than achieve. All of negative emission is hard  even afforestation or reforestation, Sally Benson, a professor of energy-resources engineering at Stanford, explains. Its not about saying, I want to plant a tree. Its about saying, We want to plant a billion trees. Nevertheless, such practices offer a glimmer of hope for meeting future emissions targets. We have to come to grips with the fact that we waited too long and that we took some options off the table, Michael Oppenheimer, a Princeton scientist who studies climate and policy, told me. As a result, NETs no longer seem to be just interesting ideas; they look like necessities. And as it happens, the Climeworks machines on the rooftop do the work each year of about 36,000 trees. Last fall, the National Academies of Sciences, Engineering and Medicine published a lengthy study on carbon removal. Stephen Pacala, a Princeton professor who led the authors, pointed out to me that negative-emissions technologies have various strengths and drawbacks, and that a portfolio approach  pursue them all, then see which are the best  may be the shrewdest bet. If costs for direct air capture can be reduced, Pacala says he sees great promise, especially if the machines can offset emissions from economic sectors that for technological reasons will transition to zero carbon much more slowly than others. Commercial aviation, for instance, wont be converted to running on solar power anytime soon. Jennifer Wilcox, a chemical-engineering professor at Worcester Polytechnic Institute, in Massachusetts, told me that air capture could likewise help counter the impact of several vital industries. There are process emissions that come from producing iron and steel, cement and glass, she says, and any time you make these materials, theres a chemical reaction that emits CO₂. Direct air capture could even lessen the impacts of the Haber-Bosch processes for making fertilizer; by some estimates, that industry now accounts for 3 percent of all CO₂ emissions. Pacala equates the challenges confronting Climeworks and Carbon Engineering to what the wind- and solar-power industries faced in the 1970s and 80s, when their products were expensive compared with fossil fuels. Those industries couldnt rely on demand from the private sector alone. But some policymakers perceived tremendous environmental and public benefits if they could surmount that hurdle. Government investments in research, along with state and federal tax credits, helped the young industries expand. Wind and solar are now the cheapest forms of energy in the right locations, Pacala says. The return on those investments, if you calculated it, would blow the doors off anything in your portfolio. Its like investing in early Apple. So its a spectacular story of success. And direct air capture is precisely the same kind of problem, in which the only barrier is that its too costly. [Thirty years ago, we had a chance to save the planet. Read about the decade we almost stopped climate change. ]Most of Climeworkss 60 employees work in a big industrial space in downtown Zurich, on two floors of a low-slung building that the company sublets from a German aerospace firm. Manufacturing operations are on the ground floor; the research labs are upstairs, along with a small suite of shared offices, a hallway kitchen and a hangout area. The place has the stark, casual feel of a tech start-up, with one exception: The walls are lined with oversize photos of pivotal moments in Climeworkss young history  its ungainly early prototypes; the opening of the first Hinwil plant that collected CO₂ for the greenhouse. Its a little bit by accident that we are based in Switzerland, Wurzbacher told me. He and Gebald both grew up in Germany and met as undergraduates at E.C.H., the Swiss Federal Institute of Technology, in Zurich. We met on Day 1, on the 20th of October of 2003, Gebald recalled. And on Day 1 we decided that wed have a company. Their aspiration was to be entrepreneurs, not to start a carbon-capture firm, but both men were drawn to research on renewable energy and reducing emissions. After they completed their masters projects, they decided to create a direct-air-capture prototype and go into business. Both took the title of company director. Helped by a number of small grants, Climeworks was incorporated in 2009. The two men were not alone in trying to chip away at decades of carbon emissions. An American start-up, Global Thermostat, now finishing its first commercial plant in Alabama, began working on air-capture machines in 2010. And almost from the start, Gebald and Wurzbacher found themselves in a friendly competition with David Keith, the Harvard engineering professor who had just started Carbon Engineering in British Columbia. Keiths company settled on a different air-capture technology  employing a higher-heat process, and a liquid solution to capture CO₂  to brew synthetic fuels. Climeworkss big advantage is that it can make smaller plants early, Keith told me: I am crazy jealous. Its because theyre using a modular design, and were not. On the other hand, Keith said he believes his firm is closer to building a big plant that could capture carbon at a more reasonable cost and produce substantial amounts of fuel. I dont see a path for them to match this. Gebald told me he thinks his and Keiths companies will each succeed with differing approaches. For now, what all the founders have in common is a belief that the cost of capturing a ton of carbon will soon drop sharply. ImageThe greenhouse in Hinwil where Climeworks uses carbon dioxide pulled from the air to grow fruits and vegetables. CreditLuca Locatelli for The New York TimesTheir view is not always shared by outside observers. M.I.T.s Howard Herzog, for instance, an engineer who has spent years looking at the potential for these machines, told me that he thinks the costs will remain between $600 and $1,000 per metric ton. Some of Herzogs reasons for skepticism are highly technical and relate to the physics of separating gases. Some are more easily grasped. He points out that because direct-air-capture machines have to move tremendous amounts of air through a filter or solution to glean a ton of CO₂  the gas, for all its global impact, makes up only about 0.04 percent of our atmosphere  the process necessitates large expenditures for energy and big equipment. What he has likewise observed, in analyzing similar industries that separate gases, suggests that translating spreadsheet projections for capturing CO₂ into real-world applications will reveal hidden costs. I think there has been a lot of hype about this, and its not going to revolutionize anything, he told me, adding that he thinks other negative-emissions technologies will prove cheaper. At best its going to be a bit player. Last year, when David Keith and his associates at Carbon Engineering published figures projecting that their carbon-capture technology could bring costs as low as $94 a metric ton, Herzog was not convinced. Keith nevertheless made the case to me that two new investors in Carbon Engineering  Chevron Technology Ventures and a subsidiary of Occidental Petroleum  scrutinized his companys numbers to an exhaustive degree and agreed the economics of the venture were solid enough to merit putting up substantial amounts in a $60 million investment round. Both Climeworks founders told me they agreed with Keiths cost estimates, and saw a similar downward curve for their own technology. Climeworkss current goal is to remove 1 percent of the worlds annual CO₂ emissions by the mid 2020s. Yet meeting such a benchmark, if its even possible, would require bringing the cost of direct air capture down by nearly an order of magnitude while maintaining and expanding their roster of clients substantially. At the moment, Wurzbacher and Gebald have planned for several generations of Climeworks machines, with each new model promising declining prices. Basically, we have a road map  $600, down to $400, down to $300 and $200 a ton, Wurzbacher said. This is over the next five years. Down to $200 we know quite well what were doing. And beyond $200, Wurzbacher suggested, things get murkier. To move below that price would depend on new developments in technology or manufacturing. Both founders told me they expect to reap enormous cost reductions from expanding production  activities that involve buying materials more cheaply in bulk and assembling units on automated factory lines instead of building them by hand, as is the case now. Design advances could wring out other costs. Maintenance is very expensive, Wurzbacher said. Right now, if we exchange the filters in the collectors, we have to rent a crane, and thats a lot of man-hours. In the next-generation units, we have improved that a lot, so relatively small design changes could cut the costs of maintenance by a factor of three. Climeworks also intends to derive savings from improvements to crucial materials, like the sorbent that catches the CO₂. At the moment, the companys technology requires that the temperature inside the units be raised periodically to about 100 degrees Celsius to release CO₂ from the sorbent so it can be drawn off and stored. If the process can be done at a lower temperature, the units will use less energy, and the life of the materials should be extended, further driving down costs. The companys ambitions for mass production may still seem extreme. To actually capture 1 percent of the worlds carbon emissions by 2025 would, by Gebalds calculations, require that Climeworks build 250,000 carbon-capture plants like the ones on the roof at Hinwil. That adds up to about 4.5 million carbon collectors. For a company that has only built 100 collectors (and has 14 small plants around Europe), its a staggering number. The Climeworks founders therefore try to think of their product as the automotive industry might  a piece of mass-produced technology and metal, not the carbon they hope to sequester. What were doing is gas separation, Wurzbacher said, and thats traditionally a process-industry business, like oil and gas. But we dont really see ourselves there. The founders note that Toyota makes more than 10 million cars annually. Every CO₂ collector has about the same weight and dimensions of a car  roughly two tons, and roughly 2 meters by 2 meters by 2 meters, Gebald said. And all the methods used to produce the CO₂ collectors could be well automated. So we have the automotive industry as a model for how to produce things in large quantities for low cost. The two men have already sought advice from Audi. They are also aware that the automotive industry perfected its methods over the course of 100 years. Climeworks, if it plans to have even a modest impact, doesnt have nearly as much time. In 1954, the economist Paul Samuelson put forward a theory that made a distinction between private-consumption goods  bread, cars, houses and the like  and commodities that existed apart from the usual laws of supply and demand. Modern global markets are obviously quite successful at pricing private goods we need and want. But the other type of commodity Samuelson was describing is something now known as a public good, which benefits everyone but is not bought, sold or consumed the same way. Definitions of a public good can vary, but the oft-used examples are lighthouses, national defenses and clean air. Direct air capture can no doubt create private goods, like soft-drink carbonation or fuels. What makes its value so difficult to estimate is that in burying CO₂ for a better atmosphere  and, almost certainly, a better future  its purveyors would also create a public good. The challenge with just collecting and burying CO₂ is that there isnt a market yet, Julio Friedmann, a former United States Energy Department official who now works at Columbia University, told me. What its really about is offering an environmental service for a fee. And what that means, in short, is that direct air captures success would be limited to the size of the market for private goods  soda fizz, greenhouse gas  unless governments decided to intervene and help fund the equivalent of several million (or more) lighthouses. An intervention could take a variety of forms. It could be large grants for research to find better sorbent materials, for instance, which would be similar to government investments that long ago helped nurture the solar- and wind-power industries. But help could also come by expanding regulations that already exist. A new and obscure United States tax provision, known as 45Q and signed last year by President Trump, offers a tax credit of up to $50 a ton for companies that bury CO₂ in geologic formations. The credit can benefit oil and gas firms that pump CO₂ underground during drilling work, as well as power plants that capture emissions directly from their smokestacks. Yet it could be used by Climeworks too, should it open plants in the United States  but only if it manages to remove and bury 100,000 tons of CO₂ per year. Governments can make carbon more expensive too. The Climeworks founders told me they dont believe their company will succeed on what they call climate impact scales unless the world puts significant prices on emissions, in the form of a carbon tax or carbon fee. Our goal is to make it possible to capture CO₂ from the air for below $100 per ton, Wurzbacher says. No one owns a crystal ball, but we think  and were quite confident  that by something like 2030 well have a global average price on carbon in the range of $100 to $150 a ton. There is optimism in this thinking, he admitted; at the moment, only a few European countries have made progress in assessing a high price on carbon, and in the United States, carbon taxes have been repudiated recently at the polls, most recently in Washington State. Still, if such prices became a reality, they could benefit the carbon extraction market in a variety of ways. A company that sells a product or uses a process that creates high emissions  an airline, for instance, or a steel maker  could be required to pay carbon-removal companies $100 per metric ton or more to offset their CO₂ output. Or a government might use carbon-tax proceeds to directly pay businesses to collect and bury CO₂. In the absence of any meaningful government action, perhaps a crusading billionaire could put all the money in his estate toward capturing CO₂ and stashing it in the earth. If carbon came to be properly priced, a global ledger would need to be kept by regulators so that air-capture machines could suck in and bury an amount equivalent to the CO₂ that emitters produce. Because CO₂ emissions mix quickly into the atmosphere, location would be mostly irrelevant, except for the need to situate plants near clean energy sources and suitable areas for sequestering the gas underground. A direct-air-capture plant in Iceland, in other words, could take in the same quantity of emissions produced by a Boeing 787 in Australia and thus negate its environmental impact. Whats more, there might not be limitations on the burial process. It doesnt cost too much to pump CO₂ underground, Stanfords Sally Benson says. Companies already sequester about 34 million metric tons of CO₂ in the ground every year, at a number of sites around the world, usually to enhance the oil-drilling process. The costs range from $2 to $15 per ton. So the bigger cost in all of this is the cost of carbon capture. Benson told me that various studies suggest that the earths capacity for CO₂ sequestration could be in the range of 25 trillion metric tons; burying, say, five billion metric tons of CO₂ a year is therefore within the realm of possibility. ImageA pilot project at a Swiss university that uses Climeworks equipment to make methane out of airborne CO₂. CreditLuca Locatelli for The New York TimesIn an imaginary, zero-carbon future, the revenue prospects for air-capture companies would probably be enormous. If we get to $100 to $150 a ton, Wurzbacher told me, then the market is almost infinite. It would be so large, he said, that even if his company went through an exponential expansion, he doubted it could serve all the potential clients. At such low prices, companies could potentially fold carbon offsets into their pricing  or be compelled to do so  leading to an explosion in the market. Christoph and me, we are always saying, we think that if this develops in a direction we think it does, we are not founding a company  were really founding a new industry, Wurzbacher said. He points to the work in Iceland  a collaborative effort, funded partly by the European Union  as the first step toward that industry. At the moment, a single Climeworks collector on a Reykjavik geothermal field takes in air and collects CO₂; after the gas is flushed from the machines filter, it is mixed with water, essentially forming hot seltzer. Then the liquid is injected into a basalt rock formation deep underground. Over the course of about two years, the CO₂ mineralizes, locking away the gas forever. At Climeworkss offices in Zurich, I asked Valentin Gutknecht, who was at the time the companys business-development manager, if he could bury in Iceland my emissions from my plane flight from the United States to Zurich. He had a written agreement he could print out and give me, but it wouldnt be cheap, he warned. The price was running about $600 a metric ton, meaning my flight would cost about an extra $700. But I was hardly the first person to ask him. The weekend before, Gutknecht told me, he received 900 unsolicited inquiries by email. Many were from potential customers who wanted to know how soon Climeworks could bury their CO₂ emissions, or how much a machine might cost them. I had the sense I was getting a glimpse of whats to come: A community of people  not large enough to make a difference, but nonetheless motivated  seemed ready to pay a premium to reverse their CO₂ emissions. Later, Wurzbacher told me he wants to offer a one click consumer service, perhaps in a year or two, which would expand what theyre doing in Iceland to individual customers and businesses. A Climeworks app could be installed on my smartphone, he explained. It could then be activated by my handsets location services. You fly over here to Europe, he explained, and the app tells you that you have just burned 1.7 tons of CO₂. Do you want to remove that? Well, Climeworks can remove it for you. Click here. Well charge your credit card. And then youll get a stone made from CO₂ for every ton you sequester. He sat back and sighed. That would be my dream, he said. Paradoxical though it may seem, its probable that synthetic fuels offer a more practical path to creating a viable business for direct air capture. The vast and constant market demand for fuel is why Carbon Engineering has staked its future on synthetics. The world currently burns about 100 million barrels of oil a day. David Keith told me he thinks that by 2050 the demand for transportation fuels will almost certainly be modified by the transition to electric vehicles. So lets say youd have to supply something like 50 million barrels a day in 2050 of fuels, he said. Thats still a monster market. Steve Oldham, Carbon Engineerings chief executive, added that direct-air-capture synthetics have an advantage over traditional fossil fuels: They wont have to spend a dime on exploration. If you were a brand-new company looking to make fuel, the cost of finding and then extracting fossil fuel is going to be really substantial, he says. Whereas our plants, you can build it right in the middle of California, wherever you have air and water. He told me that the companys first large-scale facility should be up and running by 2022, and will turn out at least 500 barrels a day of fuel feedstock  the raw material sent to refineries. Climeworks perceives a large market for fuels, too. In a town near Zurich called Rapperswil-Jona, the firm has installed a collector in a small plant, run by the local technical university, to produce methane. In a room about the size of a shipping container, the Climeworks machine takes in CO₂ through an air duct and sends it through a maze of pipes to combine it with hydrogen, which is derived from water using solar power. When I visited, the plant was a few weeks away from being operational, but the methane coming out of the works could replace gasoline in the engine of just about any car, bus or truck outfitted to run on natural gas. At a larger plant in Italy, Climeworks recently joined a consortium of European countries to produce synthetic methane that will be used by a local trucking fleet. With different tweaks and refinements, the process could be adapted for diesel, gasoline, jet fuel  or it could be piped directly to local neighborhoods as fuel for home furnaces. From an economic standpoint, synthetic fuels could allow producers to plug into a huge existing infrastructure  refineries, gas stations, cars, planes, trucks, homes, ships  and replace a product already in demand with something arguably better. But the new fuels are not necessarily cheaper. Carbon Engineering aspires to deliver its product at an ultimate retail price of about $1 per liter, or $3.75 per gallon. What would make the product competitive are regulations in California that now require fuel sellers to produce fuels of lower carbon intensity. To date this has meant blending gas and diesel with biofuels like ethanol, but it could soon mean carbon-capture synthetics too. In an expanding market, synthetic fuels could have curious effects. Since theyre made from airborne CO₂ and hydrogen and could be manufactured just about anywhere, they could rearrange the geopolitical order  tempering the power of a handful of countries that now control natural-gas and oil markets. The methane project in Rapperswil-Jona is especially suited for that countrys needs, Markus Friedl, a thermodynamics professor overseeing the project, told me, because Switzerland imports almost all of its natural gas, and its ability to generate energy from renewable sources is limited during the colder months. Carbon-capture-derived fuels, if they become cheap enough, could be a form of energy storage  made in summer, with solar or wind power, and used in winter  that carries a lower cost (and longer life) than batteries. From an environmental standpoint, air-capture fuels are not a utopian solution. Such fuels are carbon neutral, not carbon negative. They cant take CO₂ from our industrial past and put it back into the earth. If all the cars, trucks and planes of the year 2050 run on renewable fuels instead of fossil fuels, their CO₂ emissions would need to be removed from the air, recycled into the same product they originally burned through, and the cycle would need to repeat, ad infinitum, lest emissions increase. Even so, these fuels could present an enormous improvement. Transportation  currently the most significant source of emissions by sector in the United States  could cease to be a net emitter of CO₂. Just as crucial, the technology of direct air capture could scale up to become better and cheaper. A huge expansion would also involve huge complications. You start to get into really big challenges when you get to these big, large scales, Glen Peters, a research director at the Cicero Center for International Climate Research in Oslo, told me. If you can do one carbon-capture facility, where Carbon Engineering or Climeworks can build a big plant, great. You need to do that 5,000 times. And to capture a million tons of CO₂ with direct air capture, you need a small power plant just to run that facility. So if youre going to build one direct-air-capture facility every day for the next 30 years to get to some of these scenarios, then in addition, we have to build a new mini power plant every day as well. Its also the case that you have to address two extraordinary problems at the same time, Peters added. To reach 1.5 degrees, we need to halve emissions every decade, he said. That would mean persuading entire nations, like China and the United States, to switch from burning coal to using renewables at precisely the same time that we make immense investments in negative-emission technologies. And Peters pointed out that this would need to be done even as governments choose among competing priorities: health care, education and so on. The idea of bringing direct air capture up to 10 billion tons by the middle or later part of the century is such a herculean task it would require an industrial scale-up the likes of which the world has never seen, Princetons Stephen Pacala told me. And yet Pacala wasnt pessimistic about making a start. He seemed to think it was necessary for the federal government to begin with significant research and investments in the technology  to see how far and fast it could move forward, so that its ready as soon as possible. At Climeworks, Gebald and Wurzbacher spoke in similar terms, asserting that the conversations around climate challenges are moving beyond the choice between clean energy or carbon removal. Both will be necessary. Gebald and Wurzbacher seem less assured about the future of global policy than on the mechanics of scaling up. Some of that, they made clear, was related to their outlook as engineers, to what theyve gathered from observing companies like Audi and Apple. If the last century has proved anything, its that society is not always intent on acting quickly, at least in the political realm, to clean up our environment. But weve proved very good at building technology in mass quantities and making products and devices better and cheaper  especially when theres money to be made. For now, Gebald and Wurzbacher seemed to regard the climate challenge in mathematical terms. How many gigatons needed to be removed? How much would it cost per ton? How many Climeworks machines were required? Even if the figures were enormous, even if they appeared impossible, to see the future their way was to redefine the problem, to move away from the narrative of loss, to forget the multiplying stories of dying reefs and threatened coastlines  and to begin to imagine other possibilities.\n",
      "Sentences: 297\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "Climeworkss current goal is to remove 1 percent of the worlds annual CO₂ emissions by the mid 2020s. Yet meeting such a benchmark, if its even possible, would require bringing the cost of direct air capture down by nearly an order of magnitude while maintaining and expanding their roster of clients substantially. At the moment, Wurzbacher and Gebald have planned for several generations of Climeworks machines, with each new model promising declining prices. Basically, we have a road map  $600, down to $400, down to $300 and $200 a ton, Wurzbacher said. This is over the next five years. Down to $200 we know quite well what were doing. And beyond $200, Wurzbacher suggested, things get murkier. To move below that price would depend on new developments in technology or manufacturing. Both founders told me they expect to reap enormous cost reductions from expanding production  activities that involve buying materials more cheaply in bulk and assembling units on automated factory lines instead of building them by hand, as is the case now. Design advances could wring out other costs. Maintenance is very expensive, Wurzbacher said. Right now, if we exchange the filters in the collectors, we have to rent a crane, and thats a lot of man-hours. In the next-generation units, we have improved that a lot, so relatively small design changes could cut the costs of maintenance by a factor of three. Climeworks also intends to derive savings from improvements to crucial materials, like the sorbent that catches the CO₂. At the moment, the companys technology requires that the temperature inside the units be raised periodically to about 100 degrees Celsius to release CO₂ from the sorbent so it can be drawn off and stored. If the process can be done at a lower temperature, the units will use less energy, and the life of the materials should be extended, further driving down costs. The companys ambitions for mass production may still seem extreme. To actually capture 1 percent of the worlds carbon emissions by 2025 would, by Gebalds calculations, require that Climeworks build 250,000 carbon-capture plants like the ones on the roof at Hinwil. That adds up to about 4.5 million carbon collectors. For a company that has only built 100 collectors (and has 14 small plants around Europe), its a staggering number. The Climeworks founders therefore try to think of their product as the automotive industry might  a piece of mass-produced technology and metal, not the carbon they hope to sequester. What were doing is gas separation, Wurzbacher said, and thats traditionally a process-industry business, like oil and gas. But we dont really see ourselves there. The founders note that Toyota makes more than 10 million cars annually. Every CO₂ collector has about the same weight and dimensions of a car  roughly two tons, and roughly 2 meters by 2 meters by 2 meters, Gebald said. And all the methods used to produce the CO₂ collectors could be well automated. So we have the automotive industry as a model for how to produce things in large quantities for low cost. The two men have already sought advice from Audi. They are also aware that the automotive industry perfected its methods over the course of 100 years. Climeworks, if it plans to have even a modest impact, doesnt have nearly as much time. In 1954, the economist Paul Samuelson put forward a theory that made a distinction between private-consumption goods  bread, cars, houses and the like  and commodities that existed apart from the usual laws of supply and demand. Modern global markets are obviously quite successful at pricing private goods we need and want. But the other type of commodity Samuelson was describing is something now known as a public good, which benefits everyone but is not bought, sold or consumed the same way. Definitions of a public good can vary, but the oft-used examples are lighthouses, national defenses and clean air. Direct air capture can no doubt create private goods, like soft-drink carbonation or fuels. What makes its value so difficult to estimate is that in burying CO₂ for a better atmosphere  and, almost certainly, a better future  its purveyors would also create a public good. The challenge with just collecting and burying CO₂ is that there isnt a market yet, Julio Friedmann, a former United States Energy Department official who now works at Columbia University, told me. What its really about is offering an environmental service for a fee. And what that means, in short, is that direct air captures success would be limited to the size of the market for private goods  soda fizz, greenhouse gas  unless governments decided to intervene and help fund the equivalent of several million (or more) lighthouses. An intervention could take a variety of forms. It could be large grants for research to find better sorbent materials, for instance, which would be similar to government investments that long ago helped nurture the solar- and wind-power industries. But help could also come by expanding regulations that already exist. A new and obscure United States tax provision, known as 45Q and signed last year by President Trump, offers a tax credit of up to $50 a ton for companies that bury CO₂ in geologic formations. The credit can benefit oil and gas firms that pump CO₂ underground during drilling work, as well as power plants that capture emissions directly from their smokestacks. Yet it could be used by Climeworks too, should it open plants in the United States  but only if it manages to remove and bury 100,000 tons of CO₂ per year. Climeworks perceives a large market for fuels, too. In a town near Zurich called Rapperswil-Jona, the firm has installed a collector in a small plant, run by the local technical university, to produce methane. In a room about the size of a shipping container, the Climeworks machine takes in CO₂ through an air duct and sends it through a maze of pipes to combine it with hydrogen, which is derived from water using solar power. When I visited, the plant was a few weeks away from being operational, but the methane coming out of the works could replace gasoline in the engine of just about any car, bus or truck outfitted to run on natural gas. At a larger plant in Italy, Climeworks recently joined a consortium of European countries to produce synthetic methane that will be used by a local trucking fleet. With different tweaks and refinements, the process could be adapted for diesel, gasoline, jet fuel  or it could be piped directly to local neighborhoods as fuel for home furnaces. From an economic standpoint, synthetic fuels could allow producers to plug into a huge existing infrastructure  refineries, gas stations, cars, planes, trucks, homes, ships  and replace a product already in demand with something arguably better. But the new fuels are not necessarily cheaper. Carbon Engineering aspires to deliver its product at an ultimate retail price of about $1 per liter, or $3.75 per gallon. What would make the product competitive are regulations in California that now require fuel sellers to produce fuels of lower carbon intensity. To date this has meant blending gas and diesel with biofuels like ethanol, but it could soon mean carbon-capture synthetics too. In an expanding market, synthetic fuels could have curious effects. Since theyre made from airborne CO₂ and hydrogen and could be manufactured just about anywhere, they could rearrange the geopolitical order  tempering the power of a handful of countries that now control natural-gas and oil markets. The methane project in Rapperswil-Jona is especially suited for that countrys needs, Markus Friedl, a thermodynamics professor overseeing the project, told me, because Switzerland imports almost all of its natural gas, and its ability to generate energy from renewable sources is limited during the colder months. Carbon-capture-derived fuels, if they become cheap enough, could be a form of energy storage  made in summer, with solar or wind power, and used in winter  that carries a lower cost (and longer life) than batteries. From an environmental standpoint, air-capture fuels are not a utopian solution. Such fuels are carbon neutral, not carbon negative. They cant take CO₂ from our industrial past and put it back into the earth. If all the cars, trucks and planes of the year 2050 run on renewable fuels instead of fossil fuels, their CO₂ emissions would need to be removed from the air, recycled into the same product they originally burned through, and the cycle would need to repeat, ad infinitum, lest emissions increase. Even so, these fuels could present an enormous improvement. Transportation  currently the most significant source of emissions by sector in the United States  could cease to be a net emitter of CO₂. Just as crucial, the technology of direct air capture could scale up to become better and cheaper. A huge expansion would also involve huge complications. You start to get into really big challenges when you get to these big, large scales, Glen Peters, a research director at the Cicero Center for International Climate Research in Oslo, told me. If you can do one carbon-capture facility, where Carbon Engineering or Climeworks can build a big plant, great. You need to do that 5,000 times. And to capture a million tons of CO₂ with direct air capture, you need a small power plant just to run that facility. So if youre going to build one direct-air-capture facility every day for the next 30 years to get to some of these scenarios, then in addition, we have to build a new mini power plant every day as well. Its also the case that you have to address two extraordinary problems at the same time, Peters added. To reach 1.5 degrees, we need to halve emissions every decade, he said. That would mean persuading entire nations, like China and the United States, to switch from burning coal to using renewables at precisely the same time that we make immense investments in negative-emission technologies. And Peters pointed out that this would need to be done even as governments choose among competing priorities: health care, education and so on. The idea of bringing direct air capture up to 10 billion tons by the middle or later part of the century is such a herculean task it would require an industrial scale-up the likes of which the world has never seen, Princetons Stephen Pacala told me. And yet Pacala wasnt pessimistic about making a start. He seemed to think it was necessary for the federal government to begin with significant research and investments in the technology  to see how far and fast it could move forward, so that its ready as soon as possible. At Climeworks, Gebald and Wurzbacher spoke in similar terms, asserting that the conversations around climate challenges are moving beyond the choice between clean energy or carbon removal. Both will be necessary. Gebald and Wurzbacher seem less assured about the future of global policy than on the mechanics of scaling up. Some of that, they made clear, was related to their outlook as engineers, to what theyve gathered from observing companies like Audi and Apple. If the last century has proved anything, its that society is not always intent on acting quickly, at least in the political realm, to clean up our environment. But weve proved very good at building technology in mass quantities and making products and devices better and cheaper  especially when theres money to be made. For now, Gebald and Wurzbacher seemed to regard the climate challenge in mathematical terms. How many gigatons needed to be removed? How much would it cost per ton? How many Climeworks machines were required? Even if the figures were enormous, even if they appeared impossible, to see the future their way was to redefine the problem, to move away from the narrative of loss, to forget the multiplying stories of dying reefs and threatened coastlines  and to begin to imagine other possibilities.\n",
      "Sentences: 91\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "The Tiny Swiss Company That Thinks It Can Help Stop Climate ChangeImageChristoph Gebald, left, and Jan Wurzbacher, the founders of Climeworks, at their plant in Hinwil, Switzerland. CreditCreditLuca Locatelli for The New York TimesFeatureThe Tiny Swiss Company That Thinks It Can Help Stop Climate ChangeTwo European entrepreneurs think they can remove carbon from the air at prices cheap enough to matter. Christoph Gebald, left, and Jan Wurzbacher, the founders of Climeworks, at their plant in Hinwil, Switzerland. CreditCreditLuca Locatelli for The New York TimesSupported byBy Jon GertnerFeb. 12, 2019Just over a century ago in Ludwigshafen, Germany, a scientist named Carl Bosch assembled a team of engineers to exploit a new technique in chemistry. A year earlier, another German chemist, Fritz Haber, hit upon a process to pull nitrogen (N) from the air and combine it with hydrogen (H) to produce tiny amounts of ammonia (NH₃). But Habers process was delicate, requiring the maintenance of high temperatures and high pressure. Bosch wanted to figure out how to adapt Habers discovery for commercial purposes  as we would say today, to scale it up. Anyone looking at the state of manufacturing in Europe around 1910, Bosch observed, could see that the task was daunting: The technology simply didnt exist. Over the next decade, however, Bosch and his team overcame a multitude of technological and metallurgical challenges. He chronicled them in his 1932 acceptance speech for the Nobel Prize for Chemistry  an honor he won because the Haber-Bosch process, as it came to be known, changed the world. His breakthrough made possible the production of ammonia on an industrial scale, providing the world with cheap and abundant fertilizer. The scientist and historian Vaclav Smil called Haber-Bosch the most important technical invention of the 20th century. Bosch had effectively removed the historical bounds on crop yields, so much so that he was widely credited with making bread from air. By some estimates, Boschs work made possible the lives of more than two billion human beings over the last 100 years. What the Haber-Bosch method had going for it, from the very start, was a ready market. Fertilizer was already in high demand, but it came primarily from limited natural reserves in far-flung locales  bird droppings scraped from remote islands near Peru, for instance, or mineral stores of nitrogen dug out of the Chilean desert. Because synthetic ammonia competed with existing products, it was able to follow a timeworn pattern of innovation. In much the same way that LEDs have supplanted fluorescent and incandescent bulbs (which in turn had displaced kerosene lamps and wax candles), a novel product or process often replaces something already in demand. If it is better or cheaper  and especially if it is better and cheaper  it usually wins in the marketplace. Haber-Bosch did exactly that. It may now be that another gas  carbon dioxide (CO₂)  can be removed from the air for commercial purposes, and that its removal could have a profound effect on the future of humanity. But its almost certainly too soon to say for sure. One sunny morning last October, several engineers from a Swiss firm called Climeworks ambled onto the roof of a power-generating waste-incineration plant in Hinwil, a village about 30 minutes outside Zurich. The technicians had in front of them 12 large devices, stacked in two rows of six, that resembled oversize front-loading clothes dryers. These were direct air capture machines, which soon would begin collecting carbon dioxide from air drawn in through their central ducts. Once trapped, the CO₂ would then be siphoned into large tanks and trucked to a local Coca-Cola bottler, where it would become the fizz in a soft drink. [Is It O.K. to Tinker With the Environment to Fight Climate Change? ]The machines themselves require a significant amount of energy. They depend on electric fans to pull air into the ducts and over a special material, known as a sorbent, laced with granules that chemically bind with CO₂; periodic blasts of heat then release the captured gas from the sorbent, with customized software managing the whole catch-and-release cycle. Climeworks had installed the machines on the roof of the power plant to tap into the plants low-carbon electricity and the heat from its incineration system. A few dozen yards away from the new installation sat an older stack of Climeworks machines, 18 in total, that had been whirring on the same rooftop for more than a year. So far, these machines had captured about 1,000 metric tons (or about 1,100 short tons) of carbon dioxide from the air and fed it, by pipeline, to an enormous greenhouse nearby, where it was plumping up tomatoes, eggplants and mâche. During a tour of the greenhouse, Paul Ruser, the manager, suggested I taste the results. Here, try one, he said, handing me a crisp, ripe cucumber he plucked from a nearby vine. It was the finest direct-air-capture cucumber Id ever had. Climeworkss rooftop plant represents something new in the world: the first direct-air-capture venture in history seeking to sell CO₂ by the ton. When the companys founders, Christoph Gebald and Jan Wurzbacher, began openly discussing their plans to build a business several years ago, they faced a deluge of skepticism. I would say nine out of 10 people reacted critically, Gebald told me. The first thing they said was: This will never work technically. And finally in 2017 we convinced them it works technically, since we built the big plant in Hinwil. But once we convinced them that it works technically, they would say, Well, it will never work economically. For the moment, skeptics of Climeworkss business plan are correct: The company is not turning a profit. To build and install the 18 units at Hinwil, hand-assembled in a second-floor workshop in Zurich, cost between $3 million and $4 million, which is the primary reason it costs the firm between $500 and $600 to remove a metric ton of CO₂ from the air. Even as the company has attracted about $50 million in private investments and grants, it faces the same daunting task that confronted Carl Bosch a century ago: How much can it bring costs down? And how fast can it scale up? Gebald and Wurzbacher believe the way to gain a commercial foothold is to sell their expensive CO₂ to agriculture or beverage companies. Not only do these companies require CO₂ anyway, some also seem willing to pay a premium for a vital ingredient they can use to help market their products as eco-friendly. Still, greenhouses and soda bubbles together represent a small global market  perhaps six million metric tons of CO₂ annually. And Gebald and Wurzbacher did not get into carbon capture to grow mâche or put bubbles in Fanta. They believe that over the next seven years they can bring expenses down to a level that would enable them to sell CO₂ into more lucrative markets. Air-captured CO₂ can be combined with hydrogen and then fashioned into any kind of fossil-fuel substitute you want. Instead of making bread from air, you can make fuels from air. Already, Climeworks and another company, Carbon Engineering, which is based in British Columbia, have moved aggressively on this idea; the Canadians have even lined up investors (including Bill Gates) to produce synthetic fuel at large industrial plants from air-captured CO₂. The ultimate goal for air capture, however, isnt to turn it into a product  at least not in the traditional sense. What Gebald and Wurzbacher really want to do is to pull vast amounts of CO₂ out of the atmosphere and bury it, forever, deep underground, and sell that service as an offset. Climeworkss captured CO₂ has already been injected deep into rock formations beneath Iceland; by the end of the year, the firm intends to deploy 50 units near Reykjavik to expand the operation. But at that point the company will be moving into uncharted economic territory  purveyors of a service that seems desperately needed to help slow climate change but does not, at present, replace anything on the consumer or industrial landscape. To complicate matters, a ton of buried CO₂ is not something that human beings or governments have shown much demand for. And so companies like Climeworks face a quandary: How do you sell something that never existed before, something that may never be cheap, into a market that is not yet real? Even the most enthusiastic believers in direct air capture stop short of describing it as a miracle technology. Its more frequently described as an old idea  scrubbers that remove CO₂ have been used in submarines since at least the 1950s  that is being radically upgraded for a variety of new applications. Its arguably the case, in fact, that when it comes to reducing our carbon emissions, direct air capture will be seen as an option thats too expensive and too modest in impact. The only way that direct air capture becomes meaningful is if we do all the other things we need to do promptly, Hal Harvey, a California energy analyst who studies climate-friendly technologies and policies, told me recently. Harvey and others make the case that the biggest, fastest and cheapest gains in addressing atmospheric carbon will come from switching our power grid to renewable energy or low-carbon electricity; from transitioning to electric vehicles and imposing stricter mileage regulations on gas-powered cars and trucks; and from requiring more energy-efficient buildings and appliances. In short, the best way to start making progress toward a decarbonized world is not to rev up millions of air capture machines right now. Its to stop putting CO₂ in the atmosphere in the first place. The future of carbon mitigation, however, is on a countdown timer, as atmospheric CO₂ concentrations have continued to rise. If the nations of the world were to continue on the current track, it would be impossible to meet the objectives of the 2016 Paris Agreement, which set a goal limiting warming to 2 degrees Celsius or, ideally, 1.5 degrees. And it would usher in a world of misery and economic hardship. Already, temperatures in some regions have climbed more than 1 degree Celsius, as a report by the Intergovernmental Panel on Climate Change noted last October. These temperature increases have led to an increase in droughts, heat waves, floods and biodiversity losses and make the chaos of 2 or 3 degrees additional warming seem inconceivable. A further problem is that maintaining todays emissions path for too long runs the risk of doing irreparable damage to the earths ecosystems  causing harm that no amount of technological innovation can make right. There is no reverse gear for natural systems, Harvey says. If they go, they go. If we defrost the tundra, its game over. The same might be said for the Greenland and West Antarctic ice sheets, or our coral reefs. Such resources have an asymmetry in their natural architectures: They can take thousands or millions of years to form, but could reach conditions of catastrophic decline in just a few decades. At the moment, global CO₂ emissions are about 37 billion metric tons per year, and were on track to raise temperatures by 3 degrees Celsius by 2100. To have a shot at maintaining a climate suitable for humans, the worlds nations most likely have to reduce CO₂ emissions drastically from the current level  to perhaps 15 billion or 20 billion metric tons per year by 2030; then, through some kind of unprecedented political and industrial effort, we need to bring carbon emissions to zero by around 2050. In this context, Climeworkss effort to collect 1,000 metric tons of CO₂ on a rooftop near Zurich might seem like bailing out the ocean one bucket at a time. Conceptually, however, its important. Last years I.P.C.C. report noted that it may be impossible to limit warming to 1.5 degrees by 2100 through only a rapid switch to clean energy, electric cars and the like. To preserve a livable environment we may also need to extract CO₂ from the atmosphere. As Wurzbacher put it, if you take all these numbers from the I.P.C.C., you end up with something like eight to 10 billion tons  gigatons  of CO₂ that need to be removed from the air every year, if we are serious about 1.5 or 2 degrees. There happens to be a name for things that can do this kind of extraction work: negative-emissions technologies, or NETs. Some NETs, like trees and plants, predate us and probably dont deserve the label. Through photosynthesis, our forests take extraordinary amounts of carbon dioxide from the atmosphere, and if we were to magnify efforts to reforest clear-cut areas  or plant new groves, a process known as afforestation  we could absorb billions more metric tons of carbon in future years. Whats more, we could grow crops specifically to absorb CO₂ and then burn them for power generation, with the intention of capturing the power-plant emissions and pumping them underground, a process known as bioenergy with carbon capture and storage, or BECCS. Other negative-emissions technologies include manipulating farmland soil or coastal wetlands so they will trap more atmospheric carbon and grinding up mineral formations so they will absorb CO₂ more readily, a process known as enhanced weathering. Negative emissions can be thought of as a form of time travel. Ever since the Industrial Revolution, human societies have produced an excess of CO₂, by taking carbon stores from deep inside the earth  in the form of coal, oil and gas  and from stores aboveground (mostly wood), then putting it into the atmosphere by burning it. It has become imperative to reverse the process  that is, take CO₂ out of the air and either restore it deep inside the earth or contain it within new surface ecosystems. This is certainly easier to prescribe than achieve. All of negative emission is hard  even afforestation or reforestation, Sally Benson, a professor of energy-resources engineering at Stanford, explains. Its not about saying, I want to plant a tree. Its about saying, We want to plant a billion trees. Nevertheless, such practices offer a glimmer of hope for meeting future emissions targets. We have to come to grips with the fact that we waited too long and that we took some options off the table, Michael Oppenheimer, a Princeton scientist who studies climate and policy, told me. As a result, NETs no longer seem to be just interesting ideas; they look like necessities. And as it happens, the Climeworks machines on the rooftop do the work each year of about 36,000 trees. Last fall, the National Academies of Sciences, Engineering and Medicine published a lengthy study on carbon removal. Stephen Pacala, a Princeton professor who led the authors, pointed out to me that negative-emissions technologies have various strengths and drawbacks, and that a portfolio approach  pursue them all, then see which are the best  may be the shrewdest bet. If costs for direct air capture can be reduced, Pacala says he sees great promise, especially if the machines can offset emissions from economic sectors that for technological reasons will transition to zero carbon much more slowly than others. Commercial aviation, for instance, wont be converted to running on solar power anytime soon. Jennifer Wilcox, a chemical-engineering professor at Worcester Polytechnic Institute, in Massachusetts, told me that air capture could likewise help counter the impact of several vital industries. There are process emissions that come from producing iron and steel, cement and glass, she says, and any time you make these materials, theres a chemical reaction that emits CO₂. Direct air capture could even lessen the impacts of the Haber-Bosch processes for making fertilizer; by some estimates, that industry now accounts for 3 percent of all CO₂ emissions. Pacala equates the challenges confronting Climeworks and Carbon Engineering to what the wind- and solar-power industries faced in the 1970s and 80s, when their products were expensive compared with fossil fuels. Those industries couldnt rely on demand from the private sector alone. But some policymakers perceived tremendous environmental and public benefits if they could surmount that hurdle. Government investments in research, along with state and federal tax credits, helped the young industries expand. Wind and solar are now the cheapest forms of energy in the right locations, Pacala says. The return on those investments, if you calculated it, would blow the doors off anything in your portfolio. Its like investing in early Apple. So its a spectacular story of success. And direct air capture is precisely the same kind of problem, in which the only barrier is that its too costly. [Thirty years ago, we had a chance to save the planet. Read about the decade we almost stopped climate change. ]Most of Climeworkss 60 employees work in a big industrial space in downtown Zurich, on two floors of a low-slung building that the company sublets from a German aerospace firm. Manufacturing operations are on the ground floor; the research labs are upstairs, along with a small suite of shared offices, a hallway kitchen and a hangout area. The place has the stark, casual feel of a tech start-up, with one exception: The walls are lined with oversize photos of pivotal moments in Climeworkss young history  its ungainly early prototypes; the opening of the first Hinwil plant that collected CO₂ for the greenhouse. Its a little bit by accident that we are based in Switzerland, Wurzbacher told me. He and Gebald both grew up in Germany and met as undergraduates at E.C.H., the Swiss Federal Institute of Technology, in Zurich. We met on Day 1, on the 20th of October of 2003, Gebald recalled. And on Day 1 we decided that wed have a company. Their aspiration was to be entrepreneurs, not to start a carbon-capture firm, but both men were drawn to research on renewable energy and reducing emissions. After they completed their masters projects, they decided to create a direct-air-capture prototype and go into business. Both took the title of company director. Helped by a number of small grants, Climeworks was incorporated in 2009. The two men were not alone in trying to chip away at decades of carbon emissions. An American start-up, Global Thermostat, now finishing its first commercial plant in Alabama, began working on air-capture machines in 2010. And almost from the start, Gebald and Wurzbacher found themselves in a friendly competition with David Keith, the Harvard engineering professor who had just started Carbon Engineering in British Columbia. Keiths company settled on a different air-capture technology  employing a higher-heat process, and a liquid solution to capture CO₂  to brew synthetic fuels. Climeworkss big advantage is that it can make smaller plants early, Keith told me: I am crazy jealous. Its because theyre using a modular design, and were not. On the other hand, Keith said he believes his firm is closer to building a big plant that could capture carbon at a more reasonable cost and produce substantial amounts of fuel. I dont see a path for them to match this. Gebald told me he thinks his and Keiths companies will each succeed with differing approaches. For now, what all the founders have in common is a belief that the cost of capturing a ton of carbon will soon drop sharply. ImageThe greenhouse in Hinwil where Climeworks uses carbon dioxide pulled from the air to grow fruits and vegetables. CreditLuca Locatelli for The New York TimesTheir view is not always shared by outside observers. M.I.T.s Howard Herzog, for instance, an engineer who has spent years looking at the potential for these machines, told me that he thinks the costs will remain between $600 and $1,000 per metric ton. Some of Herzogs reasons for skepticism are highly technical and relate to the physics of separating gases. Some are more easily grasped. He points out that because direct-air-capture machines have to move tremendous amounts of air through a filter or solution to glean a ton of CO₂  the gas, for all its global impact, makes up only about 0.04 percent of our atmosphere  the process necessitates large expenditures for energy and big equipment. What he has likewise observed, in analyzing similar industries that separate gases, suggests that translating spreadsheet projections for capturing CO₂ into real-world applications will reveal hidden costs. I think there has been a lot of hype about this, and its not going to revolutionize anything, he told me, adding that he thinks other negative-emissions technologies will prove cheaper. At best its going to be a bit player. Last year, when David Keith and his associates at Carbon Engineering published figures projecting that their carbon-capture technology could bring costs as low as $94 a metric ton, Herzog was not convinced. Keith nevertheless made the case to me that two new investors in Carbon Engineering  Chevron Technology Ventures and a subsidiary of Occidental Petroleum  scrutinized his companys numbers to an exhaustive degree and agreed the economics of the venture were solid enough to merit putting up substantial amounts in a $60 million investment round. Both Climeworks founders told me they agreed with Keiths cost estimates, and saw a similar downward curve for their own technology. Climeworkss current goal is to remove 1 percent of the worlds annual CO₂ emissions by the mid 2020s. Yet meeting such a benchmark, if its even possible, would require bringing the cost of direct air capture down by nearly an order of magnitude while maintaining and expanding their roster of clients substantially. At the moment, Wurzbacher and Gebald have planned for several generations of Climeworks machines, with each new model promising declining prices. Basically, we have a road map  $600, down to $400, down to $300 and $200 a ton, Wurzbacher said. This is over the next five years. Down to $200 we know quite well what were doing. And beyond $200, Wurzbacher suggested, things get murkier. To move below that price would depend on new developments in technology or manufacturing. Both founders told me they expect to reap enormous cost reductions from expanding production  activities that involve buying materials more cheaply in bulk and assembling units on automated factory lines instead of building them by hand, as is the case now. Design advances could wring out other costs. Maintenance is very expensive, Wurzbacher said. Right now, if we exchange the filters in the collectors, we have to rent a crane, and thats a lot of man-hours. In the next-generation units, we have improved that a lot, so relatively small design changes could cut the costs of maintenance by a factor of three. Climeworks also intends to derive savings from improvements to crucial materials, like the sorbent that catches the CO₂. At the moment, the companys technology requires that the temperature inside the units be raised periodically to about 100 degrees Celsius to release CO₂ from the sorbent so it can be drawn off and stored. If the process can be done at a lower temperature, the units will use less energy, and the life of the materials should be extended, further driving down costs. The companys ambitions for mass production may still seem extreme. To actually capture 1 percent of the worlds carbon emissions by 2025 would, by Gebalds calculations, require that Climeworks build 250,000 carbon-capture plants like the ones on the roof at Hinwil. That adds up to about 4.5 million carbon collectors. For a company that has only built 100 collectors (and has 14 small plants around Europe), its a staggering number. The Climeworks founders therefore try to think of their product as the automotive industry might  a piece of mass-produced technology and metal, not the carbon they hope to sequester. What were doing is gas separation, Wurzbacher said, and thats traditionally a process-industry business, like oil and gas. But we dont really see ourselves there. The founders note that Toyota makes more than 10 million cars annually. Every CO₂ collector has about the same weight and dimensions of a car  roughly two tons, and roughly 2 meters by 2 meters by 2 meters, Gebald said. And all the methods used to produce the CO₂ collectors could be well automated. So we have the automotive industry as a model for how to produce things in large quantities for low cost. The two men have already sought advice from Audi. They are also aware that the automotive industry perfected its methods over the course of 100 years. Climeworks, if it plans to have even a modest impact, doesnt have nearly as much time. In 1954, the economist Paul Samuelson put forward a theory that made a distinction between private-consumption goods  bread, cars, houses and the like  and commodities that existed apart from the usual laws of supply and demand. Modern global markets are obviously quite successful at pricing private goods we need and want. But the other type of commodity Samuelson was describing is something now known as a public good, which benefits everyone but is not bought, sold or consumed the same way. Definitions of a public good can vary, but the oft-used examples are lighthouses, national defenses and clean air. Direct air capture can no doubt create private goods, like soft-drink carbonation or fuels. What makes its value so difficult to estimate is that in burying CO₂ for a better atmosphere  and, almost certainly, a better future  its purveyors would also create a public good. The challenge with just collecting and burying CO₂ is that there isnt a market yet, Julio Friedmann, a former United States Energy Department official who now works at Columbia University, told me. What its really about is offering an environmental service for a fee. And what that means, in short, is that direct air captures success would be limited to the size of the market for private goods  soda fizz, greenhouse gas  unless governments decided to intervene and help fund the equivalent of several million (or more) lighthouses. An intervention could take a variety of forms. It could be large grants for research to find better sorbent materials, for instance, which would be similar to government investments that long ago helped nurture the solar- and wind-power industries. But help could also come by expanding regulations that already exist. A new and obscure United States tax provision, known as 45Q and signed last year by President Trump, offers a tax credit of up to $50 a ton for companies that bury CO₂ in geologic formations. The credit can benefit oil and gas firms that pump CO₂ underground during drilling work, as well as power plants that capture emissions directly from their smokestacks. Yet it could be used by Climeworks too, should it open plants in the United States  but only if it manages to remove and bury 100,000 tons of CO₂ per year. Governments can make carbon more expensive too. The Climeworks founders told me they dont believe their company will succeed on what they call climate impact scales unless the world puts significant prices on emissions, in the form of a carbon tax or carbon fee. Our goal is to make it possible to capture CO₂ from the air for below $100 per ton, Wurzbacher says. No one owns a crystal ball, but we think  and were quite confident  that by something like 2030 well have a global average price on carbon in the range of $100 to $150 a ton. There is optimism in this thinking, he admitted; at the moment, only a few European countries have made progress in assessing a high price on carbon, and in the United States, carbon taxes have been repudiated recently at the polls, most recently in Washington State. Still, if such prices became a reality, they could benefit the carbon extraction market in a variety of ways. A company that sells a product or uses a process that creates high emissions  an airline, for instance, or a steel maker  could be required to pay carbon-removal companies $100 per metric ton or more to offset their CO₂ output. Or a government might use carbon-tax proceeds to directly pay businesses to collect and bury CO₂. In the absence of any meaningful government action, perhaps a crusading billionaire could put all the money in his estate toward capturing CO₂ and stashing it in the earth. If carbon came to be properly priced, a global ledger would need to be kept by regulators so that air-capture machines could suck in and bury an amount equivalent to the CO₂ that emitters produce. Because CO₂ emissions mix quickly into the atmosphere, location would be mostly irrelevant, except for the need to situate plants near clean energy sources and suitable areas for sequestering the gas underground. A direct-air-capture plant in Iceland, in other words, could take in the same quantity of emissions produced by a Boeing 787 in Australia and thus negate its environmental impact. Whats more, there might not be limitations on the burial process. It doesnt cost too much to pump CO₂ underground, Stanfords Sally Benson says. Companies already sequester about 34 million metric tons of CO₂ in the ground every year, at a number of sites around the world, usually to enhance the oil-drilling process. The costs range from $2 to $15 per ton. So the bigger cost in all of this is the cost of carbon capture. Benson told me that various studies suggest that the earths capacity for CO₂ sequestration could be in the range of 25 trillion metric tons; burying, say, five billion metric tons of CO₂ a year is therefore within the realm of possibility. ImageA pilot project at a Swiss university that uses Climeworks equipment to make methane out of airborne CO₂. CreditLuca Locatelli for The New York TimesIn an imaginary, zero-carbon future, the revenue prospects for air-capture companies would probably be enormous. If we get to $100 to $150 a ton, Wurzbacher told me, then the market is almost infinite. It would be so large, he said, that even if his company went through an exponential expansion, he doubted it could serve all the potential clients. At such low prices, companies could potentially fold carbon offsets into their pricing  or be compelled to do so  leading to an explosion in the market. Christoph and me, we are always saying, we think that if this develops in a direction we think it does, we are not founding a company  were really founding a new industry, Wurzbacher said. He points to the work in Iceland  a collaborative effort, funded partly by the European Union  as the first step toward that industry. At the moment, a single Climeworks collector on a Reykjavik geothermal field takes in air and collects CO₂; after the gas is flushed from the machines filter, it is mixed with water, essentially forming hot seltzer. Then the liquid is injected into a basalt rock formation deep underground. Over the course of about two years, the CO₂ mineralizes, locking away the gas forever. At Climeworkss offices in Zurich, I asked Valentin Gutknecht, who was at the time the companys business-development manager, if he could bury in Iceland my emissions from my plane flight from the United States to Zurich. He had a written agreement he could print out and give me, but it wouldnt be cheap, he warned. The price was running about $600 a metric ton, meaning my flight would cost about an extra $700. But I was hardly the first person to ask him. The weekend before, Gutknecht told me, he received 900 unsolicited inquiries by email. Many were from potential customers who wanted to know how soon Climeworks could bury their CO₂ emissions, or how much a machine might cost them. I had the sense I was getting a glimpse of whats to come: A community of people  not large enough to make a difference, but nonetheless motivated  seemed ready to pay a premium to reverse their CO₂ emissions. Later, Wurzbacher told me he wants to offer a one click consumer service, perhaps in a year or two, which would expand what theyre doing in Iceland to individual customers and businesses. A Climeworks app could be installed on my smartphone, he explained. It could then be activated by my handsets location services. You fly over here to Europe, he explained, and the app tells you that you have just burned 1.7 tons of CO₂. Do you want to remove that? Well, Climeworks can remove it for you. Click here. Well charge your credit card. And then youll get a stone made from CO₂ for every ton you sequester. He sat back and sighed. That would be my dream, he said. Paradoxical though it may seem, its probable that synthetic fuels offer a more practical path to creating a viable business for direct air capture. The vast and constant market demand for fuel is why Carbon Engineering has staked its future on synthetics. The world currently burns about 100 million barrels of oil a day. David Keith told me he thinks that by 2050 the demand for transportation fuels will almost certainly be modified by the transition to electric vehicles. So lets say youd have to supply something like 50 million barrels a day in 2050 of fuels, he said. Thats still a monster market. Steve Oldham, Carbon Engineerings chief executive, added that direct-air-capture synthetics have an advantage over traditional fossil fuels: They wont have to spend a dime on exploration. If you were a brand-new company looking to make fuel, the cost of finding and then extracting fossil fuel is going to be really substantial, he says. Whereas our plants, you can build it right in the middle of California, wherever you have air and water. He told me that the companys first large-scale facility should be up and running by 2022, and will turn out at least 500 barrels a day of fuel feedstock  the raw material sent to refineries. Climeworks perceives a large market for fuels, too. In a town near Zurich called Rapperswil-Jona, the firm has installed a collector in a small plant, run by the local technical university, to produce methane. In a room about the size of a shipping container, the Climeworks machine takes in CO₂ through an air duct and sends it through a maze of pipes to combine it with hydrogen, which is derived from water using solar power. When I visited, the plant was a few weeks away from being operational, but the methane coming out of the works could replace gasoline in the engine of just about any car, bus or truck outfitted to run on natural gas. At a larger plant in Italy, Climeworks recently joined a consortium of European countries to produce synthetic methane that will be used by a local trucking fleet. With different tweaks and refinements, the process could be adapted for diesel, gasoline, jet fuel  or it could be piped directly to local neighborhoods as fuel for home furnaces. From an economic standpoint, synthetic fuels could allow producers to plug into a huge existing infrastructure  refineries, gas stations, cars, planes, trucks, homes, ships  and replace a product already in demand with something arguably better. But the new fuels are not necessarily cheaper. Carbon Engineering aspires to deliver its product at an ultimate retail price of about $1 per liter, or $3.75 per gallon. What would make the product competitive are regulations in California that now require fuel sellers to produce fuels of lower carbon intensity. To date this has meant blending gas and diesel with biofuels like ethanol, but it could soon mean carbon-capture synthetics too. In an expanding market, synthetic fuels could have curious effects. Since theyre made from airborne CO₂ and hydrogen and could be manufactured just about anywhere, they could rearrange the geopolitical order  tempering the power of a handful of countries that now control natural-gas and oil markets. The methane project in Rapperswil-Jona is especially suited for that countrys needs, Markus Friedl, a thermodynamics professor overseeing the project, told me, because Switzerland imports almost all of its natural gas, and its ability to generate energy from renewable sources is limited during the colder months. Carbon-capture-derived fuels, if they become cheap enough, could be a form of energy storage  made in summer, with solar or wind power, and used in winter  that carries a lower cost (and longer life) than batteries. From an environmental standpoint, air-capture fuels are not a utopian solution. Such fuels are carbon neutral, not carbon negative. They cant take CO₂ from our industrial past and put it back into the earth. If all the cars, trucks and planes of the year 2050 run on renewable fuels instead of fossil fuels, their CO₂ emissions would need to be removed from the air, recycled into the same product they originally burned through, and the cycle would need to repeat, ad infinitum, lest emissions increase. Even so, these fuels could present an enormous improvement. Transportation  currently the most significant source of emissions by sector in the United States  could cease to be a net emitter of CO₂. Just as crucial, the technology of direct air capture could scale up to become better and cheaper. A huge expansion would also involve huge complications. You start to get into really big challenges when you get to these big, large scales, Glen Peters, a research director at the Cicero Center for International Climate Research in Oslo, told me. If you can do one carbon-capture facility, where Carbon Engineering or Climeworks can build a big plant, great. You need to do that 5,000 times. And to capture a million tons of CO₂ with direct air capture, you need a small power plant just to run that facility. So if youre going to build one direct-air-capture facility every day for the next 30 years to get to some of these scenarios, then in addition, we have to build a new mini power plant every day as well. Its also the case that you have to address two extraordinary problems at the same time, Peters added. To reach 1.5 degrees, we need to halve emissions every decade, he said. That would mean persuading entire nations, like China and the United States, to switch from burning coal to using renewables at precisely the same time that we make immense investments in negative-emission technologies. And Peters pointed out that this would need to be done even as governments choose among competing priorities: health care, education and so on. The idea of bringing direct air capture up to 10 billion tons by the middle or later part of the century is such a herculean task it would require an industrial scale-up the likes of which the world has never seen, Princetons Stephen Pacala told me. And yet Pacala wasnt pessimistic about making a start. He seemed to think it was necessary for the federal government to begin with significant research and investments in the technology  to see how far and fast it could move forward, so that its ready as soon as possible. At Climeworks, Gebald and Wurzbacher spoke in similar terms, asserting that the conversations around climate challenges are moving beyond the choice between clean energy or carbon removal. Both will be necessary. Gebald and Wurzbacher seem less assured about the future of global policy than on the mechanics of scaling up. Some of that, they made clear, was related to their outlook as engineers, to what theyve gathered from observing companies like Audi and Apple. If the last century has proved anything, its that society is not always intent on acting quickly, at least in the political realm, to clean up our environment. But weve proved very good at building technology in mass quantities and making products and devices better and cheaper  especially when theres money to be made. For now, Gebald and Wurzbacher seemed to regard the climate challenge in mathematical terms. How many gigatons needed to be removed? How much would it cost per ton? How many Climeworks machines were required? Even if the figures were enormous, even if they appeared impossible, to see the future their way was to redefine the problem, to move away from the narrative of loss, to forget the multiplying stories of dying reefs and threatened coastlines  and to begin to imagine other possibilities. Jon Gertner writes frequently for the magazine about science and technology. He last wrote about Teslas effort to build self-driving cars. A version of this article appears in print on , on Page 48 of the Sunday Magazine with the headline: The Carbon Chase. Order Reprints | Todays Paper | SubscribeRelated Coverage\n",
      "Sentences: 305\n",
      "\n",
      "\n",
      "Article: https://kishorepv.github.io/The-value-of-Incremental_learning/\n",
      "NodeRank:\n",
      "A few days back I read an article about how Bitcoins price can be sufficiently explained by Metcalfes law. As you read through the post, resist the temptation to click the links to learn more about Bitcoin, Metcalfes law etc. (Maybe you should read about Bitcoin if you have not heard of it). I have provided all the information you need for your first read on this post. Bitcoin is a cryptocurrency and worldwide payment system. It is the first decentralized digital currency, as the system works without a central bank or single administrator. The network is peer-to-peer and transactions take place between users directly, without an intermediary. These transactions are verified by network nodes through the use of cryptography and recorded in a public distributed ledger called a blockchain. Bitcoin was invented by an unknown person or group of people under the name Satoshi Nakamoto and released as open-source software in 2009. Metcalfes law states that the value of a telecommunications network is proportional to the square of the number of connected users of the system (n^2)Following the traditional way, this is how I read the article. I start the article and partway thorough it when I come across Metcalfes law, I read on Wikipedia about Metcalfes law. Then while reading about Metcalfes law, learning that there are other network laws, I start to read a bit about them. Then I come back to the article and then after reading another portion of the article, I go back to other sites to learn a bit more about Metcalfes law. Then I diverge to another topic that says that nlog(n) is a better approximation for the value of a network than the n^2 suggested by Metcalfes law. There I learn that there are other network laws. I branch out again to learn about these laws. After spending a long time in the rabbit hole, I come back and complete the original article. This long path to completing the article makes sense if the knowledge of how to assign value to networks is going to be useful to me. Is it really worth learning about a bunch of other network laws and a replacement law for Metcalfes law? Well, if I am having a good use for it, then a little breadth is good. This leads back to the question - how do we know that this information is going to be useful in the long run? We cannot know for sure. But we can have a good heuristic to know if this information is useful. The frequency of the use of this information is a heuristic indicator of value of this information. The frequency of use is not know before hand. Only after a period of time do we know how frequently we found Metcalfes (or the nlog(n)) law useful. If the information in the article was not something we could use, then we had spent a lot of time on it when the time could be allocated for something better. A better approach to learn this would be like this. Read the article. When you come across things you do not know about (Metcalfes law etc), know what it is -  just sufficient to understand the article. In this case, the information in the quoted text above is enough. Finish the article. Remember at this point you do not know about the other network laws or the nlogn(n) unless you already knew it. Later you may find no value for this information. Good that you did spend time on other things that added value to you. On the flip-side, you might find use of this information. You realize that you can use that information to better estimate the value of other crypto coins, which can help you make better investment decisions. You use Metcalfes law to determine the value of another crypto currency (say Ethereum). Now is the time to learn a bit more about Metcalfes law. Equipped with the new information, reevaluate the value of Ethereum if you learnt anything more by exploring Metcalfe s law. You might have learnt that it cannot be a strong mental model, but a good heuristic to keep in mind. Ethereum is another crypto currency, which is decentralized like Bitcoin. It allows users to create smart contracts, which can be understood as building applications on top of the Ethereum blockchain. You may use the law again to estimate the value of another crypto currency (say Monero). Now is the time to learn more about it. At this time you would explore the assumption in Metcalfes law - all users contribute equally to the network. You try to learn if this assumption is valid in the setting of a crypto currency and based that you re-evaluate the value of Monero. Monero is a crypto currency like Bitcoin. It provides features for protecting the privacy of the users. Beyond this point if you dont apply the knowledge anymore, then you dont learn about it. But if you find it was useful in determining the value for Ether and Monero, you apply it again (,say for Ripple). You now should spend some more time learning about this law and you would most likely end up learning about the nlog(n) replacement law or other network laws. This process goes on, with additional for every new application of the law. Now that you got an idea of where this is headed, lets dive into the subject of this post - Incremental Learning. What is Incremental Learning? It is a concept in Machine Learning which refers to training a model with additional input, to extend the learning of the model. In computer science, incremental learning is a method of machine learning, in which input data is continuously used to extend the existing models knowledge i.e. to further train the model. But in this post, Incremental Learning refers to a practical and effective method of continuous learning. To begin with, lets look at how we learn something new. These are the steps we typically follow:A need to learn a new concept/technology/subject arisesCollect resources (books, courses, experts) and start learningLearn till a sufficient proficiency is reached to achieve the goal in 1.If we need to gain more expertise in the subject matter, we dig into more details when the need arises. This need-based additional learning is problematic. The problems it gives rise to are:In most cases, we need to learn a big chunk of subject matter in a short time. The new learning will produce new insights and pops out mistakes in the implementations of earlier learning. Implementation is the application of learning. For instance, it is a programming language you are learning, your implementation is the application you build with that language. In the article mentioned earlier, the implementation is the use of the knowledge of Metcalfes law to assign value to Ether,Monreo,Ripple etcWhen we need to learn a big chunk, it would normally be time limited. That being the case, we do not get time to digest the information we consume and hence can lead to superficial or incomplete learning. On learning a subject matter in depth, one also learns that some past implementation can be improved. This demands time and that is scarce. To overcome these problems and get new benefits here is a better approach to learning:Here are the step and following that I will tell why it should be done this way and the benefit it provides. Need to learn a new concept/technology/subject arises (same as before)Collect resources (books, courses, experts) and start learning (same as before)Learn till a sufficient proficiency is reached to achieve the goal in 1. (same as before)Every time the implementation is used/revisited learn one new concept in that subject matter and apply that to the current implementationThis simple approach is effective. The steps 1-3 are the same as before. We normally stop learning once we have the desired proficiency. The more we use an implementation, the need to learn more about it and improve the current implementation becomes important. But we dont realize the need till the time when our current knowledge fails us. So we need an approach to learn continuously, and also only the things that matter. We cannot tell ahead in time what matters the most in the future. A good heuristic to know the importance of subject matter is the frequency of use of its implementation. Therefore the more we use an implementation, the more important its subject matter is. The most important implementation is used more frequently than others. We have a better grasp of the subject matter that we use often. Also, we know less about the less important subject matters. By following this approach, we have spend time where it is relevant. This approach offers some advantages over the traditional approach:In the traditional approach, as we cannot determine the exact utility of a subject matter, we cannot dedicate proper amount of time for it. Here, using a good heuristic, we can spend time on learning proportional to the use of the subject matter. The time limited requirement to learn in-depth about the subject-matter is avoided in this approach as we have already learnt what we need to. Also, here we learn small chunks in short periods of time. Hence learning is improved. We implement the new learning as we learn it and results in a better implementation than improving the old implementation at once, after a period of time. This approach does not suffer from the time crunch suffered by the traditional approach. Also, applying the new information we obtain is a better way to commit it to memory. Try this approach to continuous learning and voice your results in the comment box.\n",
      "Sentences: 94\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "A few days back I read an article about how Bitcoins price can be sufficiently explained by Metcalfes law. As you read through the post, resist the temptation to click the links to learn more about Bitcoin, Metcalfes law etc. (Maybe you should read about Bitcoin if you have not heard of it). I have provided all the information you need for your first read on this post. Bitcoin is a cryptocurrency and worldwide payment system. It is the first decentralized digital currency, as the system works without a central bank or single administrator. The network is peer-to-peer and transactions take place between users directly, without an intermediary. These transactions are verified by network nodes through the use of cryptography and recorded in a public distributed ledger called a blockchain. Bitcoin was invented by an unknown person or group of people under the name Satoshi Nakamoto and released as open-source software in 2009. Metcalfes law states that the value of a telecommunications network is proportional to the square of the number of connected users of the system (n^2)Following the traditional way, this is how I read the article. I start the article and partway thorough it when I come across Metcalfes law, I read on Wikipedia about Metcalfes law. Then while reading about Metcalfes law, learning that there are other network laws, I start to read a bit about them. Then I come back to the article and then after reading another portion of the article, I go back to other sites to learn a bit more about Metcalfes law. Then I diverge to another topic that says that nlog(n) is a better approximation for the value of a network than the n^2 suggested by Metcalfes law. There I learn that there are other network laws. I branch out again to learn about these laws. After spending a long time in the rabbit hole, I come back and complete the original article. This long path to completing the article makes sense if the knowledge of how to assign value to networks is going to be useful to me. Is it really worth learning about a bunch of other network laws and a replacement law for Metcalfes law? Well, if I am having a good use for it, then a little breadth is good. This leads back to the question - how do we know that this information is going to be useful in the long run? We cannot know for sure. But we can have a good heuristic to know if this information is useful. The frequency of the use of this information is a heuristic indicator of value of this information. The frequency of use is not know before hand. Only after a period of time do we know how frequently we found Metcalfes (or the nlog(n) ) law useful. If the information in the article was not something we could use, then we had spent a lot of time on it when the time could be allocated for something better. A better approach to learn this would be like this. Read the article . When you come across things you do not know about (Metcalfes law etc), know what it is - just sufficient to understand the article. In this case, the information in the quoted text above is enough. Finish the article. Remember at this point you do not know about the other network laws or the nlogn(n) unless you already knew it. Later you may find no value for this information. Good that you did spend time on other things that added value to you. On the flip-side, you might find use of this information. You realize that you can use that information to better estimate the value of other crypto coins, which can help you make better investment decisions. You use Metcalfes law to determine the value of another crypto currency (say Ethereum ). Now is the time to learn a bit more about Metcalfes law . Equipped with the new information, reevaluate the value of Ethereum if you learnt anything more by exploring Metcalfe s law. You might have learnt that it cannot be a strong mental model , but a good heuristic to keep in mind. Ethereum is another crypto currency, which is decentralized like Bitcoin. It allows users to create smart contracts, which can be understood as building applications on top of the Ethereum blockchain. You may use the law again to estimate the value of another crypto currency (say Monero). Now is the time to learn more about it. At this time you would explore the assumption in Metcalfes law - all users contribute equally to the network . You try to learn if this assumption is valid in the setting of a crypto currency and based that you re-evaluate the value of Monero. Monero is a crypto currency like Bitcoin. It provides features for protecting the privacy of the users. Beyond this point if you dont apply the knowledge anymore, then you dont learn about it. But if you find it was useful in determining the value for Ether and Monero, you apply it again (,say for Ripple). You now should spend some more time learning about this law and you would most likely end up learning about the nlog(n) replacement law or other network laws . This process goes on, with additional for every new application of the law. Now that you got an idea of where this is headed, lets dive into the subject of this post - Incremental Learning .Incremental LearningWhat is Incremental Learning? It is a concept in Machine Learning which refers to training a model with additional input, to extend the learning of the model. In computer science, incremental learning is a method of machine learning, in which input data is continuously used to extend the existing models knowledge i.e. to further train the model. But in this post, Incremental Learning refers to a practical and effective method of continuous learning. To begin with, lets look at how we learn something new. These are the steps we typically follow: A need to learn a new concept/technology/subject arises Collect resources (books, courses, experts) and start learning Learn till a sufficient proficiency is reached to achieve the goal in 1.If we need to gain more expertise in the subject matter, we dig into more details when the need arises. This need-based additional learning is problematic. The problems it gives rise to are: In most cases, we need to learn a big chunk of subject matter in a short time. The new learning will produce new insights and pops out mistakes in the implementations of earlier learning. Implementation is the application of learning. For instance, it is a programming language you are learning, your implementation is the application you build with that language. In the article mentioned earlier, the implementation is the use of the knowledge of Metcalfes law to assign value to Ether,Monreo,Ripple etcWhen we need to learn a big chunk, it would normally be time limited. That being the case, we do not get time to digest the information we consume and hence can lead to superficial or incomplete learning. On learning a subject matter in depth, one also learns that some past implementation can be improved. This demands time and that is scarce. To overcome these problems and get new benefits here is a better approach to learning: Here are the step and following that I will tell why it should be done this way and the benefit it provides. Need to learn a new concept/technology/subject arises (same as before) Collect resources (books, courses, experts) and start learning (same as before) Learn till a sufficient proficiency is reached to achieve the goal in 1. (same as before) Every time the implementation is used/revisited learn one new concept in that subject matter and apply that to the current implementationThis simple approach is effective. The steps 1-3 are the same as before. We normally stop learning once we have the desired proficiency. The more we use an implementation, the need to learn more about it and improve the current implementation becomes important. But we dont realize the need till the time when our current knowledge fails us. So we need an approach to learn continuously, and also only the things that matter. We cannot tell ahead in time what matters the most in the future. A good heuristic to know the importance of subject matter is the frequency of use of its implementation. Therefore the more we use an implementation, the more important its subject matter is. The most important implementation is used more frequently than others. We have a better grasp of the subject matter that we use often. Also, we know less about the less important subject matters. By following this approach, we have spend time where it is relevant. This approach offers some advantages over the traditional approach: In the traditional approach, as we cannot determine the exact utility of a subject matter, we cannot dedicate proper amount of time for it. Here, using a good heuristic, we can spend time on learning proportional to the use of the subject matter. The time limited requirement to learn in-depth about the subject-matter is avoided in this approach as we have already learnt what we need to. Also, here we learn small chunks in short periods of time. Hence learning is improved. We implement the new learning as we learn it and results in a better implementation than improving the old implementation at once, after a period of time. This approach does not suffer from the time crunch suffered by the traditional approach. Also, applying the new information we obtain is a better way to commit it to memory. Try this approach to continuous learning and voice your results in the comment box.\n",
      "Sentences: 93\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "Incremental Learning - a continuous learning approachJanuary  1, 2018A few days back I read an article about how Bitcoins price can be sufficiently explained by Metcalfes law. As you read through the post, resist the temptation to click the links to learn more about Bitcoin, Metcalfes law etc. (Maybe you should read about Bitcoin if you have not heard of it). I have provided all the information you need for your first read on this post. Bitcoin is a cryptocurrency and worldwide payment system. It is the first decentralized digital currency, as the system works without a central bank or single administrator. The network is peer-to-peer and transactions take place between users directly, without an intermediary. These transactions are verified by network nodes through the use of cryptography and recorded in a public distributed ledger called a blockchain. Bitcoin was invented by an unknown person or group of people under the name Satoshi Nakamoto and released as open-source software in 2009. Metcalfes law states that the value of a telecommunications network is proportional to the square of the number of connected users of the system (n^2)Following the traditional way, this is how I read the article. I start the article and partway thorough it when I come across Metcalfes law, I read on Wikipedia about Metcalfes law. Then while reading about Metcalfes law, learning that there are other network laws, I start to read a bit about them. Then I come back to the article and then after reading another portion of the article, I go back to other sites to learn a bit more about Metcalfes law. Then I diverge to another topic that says that nlog(n) is a better approximation for the value of a network than the n^2 suggested by Metcalfes law. There I learn that there are other network laws. I branch out again to learn about these laws. After spending a long time in the rabbit hole, I come back and complete the original article. This long path to completing the article makes sense if the knowledge of how to assign value to networks is going to be useful to me. Is it really worth learning about a bunch of other network laws and a replacement law for Metcalfes law? Well, if I am having a good use for it, then a little breadth is good. This leads back to the question - how do we know that this information is going to be useful in the long run? We cannot know for sure. But we can have a good heuristic to know if this information is useful. The frequency of the use of this information is a heuristic indicator of value of this information. The frequency of use is not know before hand. Only after a period of time do we know how frequently we found Metcalfes (or the nlog(n)) law useful. If the information in the article was not something we could use, then we had spent a lot of time on it when the time could be allocated for something better. A better approach to learn this would be like this. Read the article . When you come across things you do not know about (Metcalfes law etc), know what it is -  just sufficient to understand the article. In this case, the information in the quoted text above is enough. Finish the article. Remember at this point you do not know about the other network laws or the nlogn(n) unless you already knew it. Later you may find no value for this information. Good that you did spend time on other things that added value to you. On the flip-side, you might find use of this information. You realize that you can use that information to better estimate the value of other crypto coins, which can help you make better investment decisions. You use Metcalfes law to determine the value of another crypto currency (say Ethereum ). Now is the time to learn a bit more about Metcalfes law . Equipped with the new information, reevaluate the value of Ethereum if you learnt anything more by exploring Metcalfe s law. You might have learnt that it cannot be a strong mental model , but a good heuristic to keep in mind. Ethereum is another crypto currency, which is decentralized like Bitcoin. It allows users to create smart contracts, which can be understood as building applications on top of the Ethereum blockchain. You may use the law again to estimate the value of another crypto currency (say Monero). Now is the time to learn more about it. At this time you would explore the assumption in Metcalfes law - all users contribute equally to the network. You try to learn if this assumption is valid in the setting of a crypto currency and based that you re-evaluate the value of Monero. Monero is a crypto currency like Bitcoin. It provides features for protecting the privacy of the users. Beyond this point if you dont apply the knowledge anymore, then you dont learn about it. But if you find it was useful in determining the value for Ether and Monero, you apply it again (,say for Ripple). You now should spend some more time learning about this law and you would most likely end up learning about the nlog(n) replacement law or other network laws .This process goes on, with additional for every new application of the law. Now that you got an idea of where this is headed, lets dive into the subject of this post - Incremental Learning. Incremental LearningWhat is Incremental Learning? It is a concept in Machine Learning which refers to training a model with additional input, to extend the learning of the model. In computer science, incremental learning is a method of machine learning, in which input data is continuously used to extend the existing models knowledge i.e. to further train the model. But in this post, Incremental Learning refers to a practical and effective method of continuous learning. To begin with, lets look at how we learn something new. These are the steps we typically follow:A need to learn a new concept/technology/subject arisesCollect resources (books, courses, experts) and start learningLearn till a sufficient proficiency is reached to achieve the goal in 1.If we need to gain more expertise in the subject matter, we dig into more details when the need arises. This need-based additional learning is problematic. The problems it gives rise to are:In most cases, we need to learn a big chunk of subject matter in a short time. The new learning will produce new insights and pops out mistakes in the implementations of earlier learning. Implementation is the application of learning. For instance, it is a programming language you are learning, your implementation is the application you build with that language. In the article mentioned earlier, the implementation is the use of the knowledge of Metcalfes law to assign value to Ether,Monreo,Ripple etcWhen we need to learn a big chunk, it would normally be time limited. That being the case, we do not get time to digest the information we consume and hence can lead to superficial or incomplete learning. On learning a subject matter in depth, one also learns that some past implementation can be improved. This demands time and that is scarce. To overcome these problems and get new benefits here is a better approach to learning: Here are the step and following that I will tell why it should be done this way and the benefit it provides. Need to learn a new concept/technology/subject arises (same as before)Collect resources (books, courses, experts) and start learning (same as before)Learn till a sufficient proficiency is reached to achieve the goal in 1. (same as before)Every time the implementation is used/revisited learn one new concept in that subject matter and apply that to the current implementationThis simple approach is effective. The steps 1-3 are the same as before. We normally stop learning once we have the desired proficiency. The more we use an implementation, the need to learn more about it and improve the current implementation becomes important. But we dont realize the need till the time when our current knowledge fails us. So we need an approach to learn continuously, and also only the things that matter. We cannot tell ahead in time what matters the most in the future. A good heuristic to know the importance of subject matter is the frequency of use of its implementation. Therefore the more we use an implementation, the more important its subject matter is. The most important implementation is used more frequently than others. We have a better grasp of the subject matter that we use often. Also, we know less about the less important subject matters. By following this approach, we have spend time where it is relevant. This approach offers some advantages over the traditional approach:In the traditional approach, as we cannot determine the exact utility of a subject matter, we cannot dedicate proper amount of time for it. Here, using a good heuristic, we can spend time on learning proportional to the use of the subject matter. The time limited requirement to learn in-depth about the subject-matter is avoided in this approach as we have already learnt what we need to. Also, here we learn small chunks in short periods of time. Hence learning is improved. We implement the new learning as we learn it and results in a better implementation than improving the old implementation at once, after a period of time. This approach does not suffer from the time crunch suffered by the traditional approach. Also, applying the new information we obtain is a better way to commit it to memory. Try this approach to continuous learning and voice your results in the comment box.\n",
      "Sentences: 93\n",
      "\n",
      "\n",
      "Article: https://e360.yale.edu/digest/arborists-have-cloned-ancient-redwoods-from-their-massive-stumps\n",
      "NodeRank:\n",
      "A cloned sapling of an ancient redwood tree. Archangel Ancient Tree Archive A team of arborists has successfully cloned and grown saplings from the stumps of some of the worlds oldest and largest coast redwoods, some of which were 3,000 years old and measured 35 feet in diameter when they were cut down in the 19th and 20th centuries. Earlier this month, 75 of the cloned saplings were planted at the Presidio national park in San Francisco. The initiative is run by the Archangel Ancient Tree Archive, a nonprofit working to reestablish ancient redwood forests to help combat climate change. Coastal redwoods, which can grow an average 10 feet per year, sequester 250 tons of carbon dioxide from the atmosphere over their lives, compared to 1 ton for an average tree. Were excited to set the standard for environmental recovery, David Milarch, a fourth-generation arborist and co-founder of the Archangel Ancient Tree Archive, said in a statement. These trees have the capacity to fight climate change and revitalize forests and our ecology in a way we havent seen before. Today, giant stumps of ancient redwoods dot the landscape from Oregon to northern California, reminders of the old-growth forest that used to stretch across the Pacific Northwest. Many arborists assumed these stumps were dead, but Milarch and his son, Jake, discovered living tissue growing from the trees roots, material known as baseless or stump sprouts. The Milarchs collected DNA from stumps of five giant coast redwoods, all larger than the largest tree living today. These included a giant sequoia known as General Sherman with a 25-foot diameter. They then used this genetic material to grow dozens of saplings, clones of the ancient trees, a process that takes approximately two-and-a-half-years. The Archangel Ancient Tree Archive has already planted nearly 100 of these saplings in the Eden Project garden in Cornwall, England, a couple hundred in Oregon, and is organizing further groves of saplings in nine other countries. These saplings have extraordinary potential to purify our air, water, and soil for generations to come, Milarch said. We hope [the San Francisco] super grove, which has the capability to become an eternal forest, is allowed to grow unmolested by manmade or natural disasters and thus propagate forever. The Fieldbrook Stump, in California, not long after it was felled in 1890. Cuttings from it have been used to create the new cloned saplings planted recently in San Francisco. Ericson Collection, Humboldt State University Library\n",
      "Sentences: 18\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "A team of arborists has successfully cloned and grown saplings from the stumps of some of the worlds oldest and largest coast redwoods, some of which were 3,000 years old and measured 35 feet in diameter when they were cut down in the 19 th and 20 th centuries. Earlier this month, 75 of the cloned saplings were planted at the Presidio national park in San Francisco. The initiative is run by the Archangel Ancient Tree Archive , a nonprofit working to reestablish ancient redwood forests to help combat climate change. Coastal redwoods, which can grow an average 10 feet per year, sequester 250 tons of carbon dioxide from the atmosphere over their lives, compared to 1 ton for an average tree. Were excited to set the standard for environmental recovery, David Milarch, a fourth-generation arborist and co-founder of the Archangel Ancient Tree Archive, said in a statement . These trees have the capacity to fight climate change and revitalize forests and our ecology in a way we havent seen before. Today, giant stumps of ancient redwoods dot the landscape from Oregon to northern California, reminders of the old-growth forest that used to stretch across the Pacific Northwest. Many arborists assumed these stumps were dead, but Milarch and his son, Jake, discovered living tissue growing from the trees roots, material known as baseless or stump sprouts. The Milarchs collected DNA from stumps of five giant coast redwoods, all larger than the largest tree living today. These included a giant sequoia known as General Sherman with a 25-foot diameter. They then used this genetic material to grow dozens of saplings, clones of the ancient trees, a process that takes approximately two-and-a-half-years. The Archangel Ancient Tree Archive has already planted nearly 100 of these saplings in the Eden Project garden in Cornwall, England, a couple hundred in Oregon, and is organizing further groves of saplings in nine other countries .These saplings have extraordinary potential to purify our air, water, and soil for generations to come, Milarch said. We hope [the San Francisco] super grove, which has the capability to become an eternal forest, is allowed to grow unmolested by manmade or natural disasters and thus propagate forever.\n",
      "Sentences: 13\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "A cloned sapling of an ancient redwood tree. Archangel Ancient Tree ArchiveA team of arborists has successfully cloned and grown saplings from the stumps of some of the worlds oldest and largest coast redwoods, some of which were 3,000 years old and measured 35 feet in diameter when they were cut down in the 19th and 20th centuries. Earlier this month, 75 of the cloned saplings were planted at the Presidio national park in San Francisco. The initiative is run by the Archangel Ancient Tree Archive , a nonprofit working to reestablish ancient redwood forests to help combat climate change. Coastal redwoods, which can grow an average 10 feet per year, sequester 250 tons of carbon dioxide from the atmosphere over their lives, compared to 1 ton for an average tree. Were excited to set the standard for environmental recovery, David Milarch, a fourth-generation arborist and co-founder of the Archangel Ancient Tree Archive, said in a statement . These trees have the capacity to fight climate change and revitalize forests and our ecology in a way we havent seen before. Today, giant stumps of ancient redwoods dot the landscape from Oregon to northern California, reminders of the old-growth forest that used to stretch across the Pacific Northwest. Many arborists assumed these stumps were dead, but Milarch and his son, Jake, discovered living tissue growing from the trees roots, material known as baseless or stump sprouts. The Milarchs collected DNA from stumps of five giant coast redwoods, all larger than the largest tree living today. These included a giant sequoia known as General Sherman with a 25-foot diameter. They then used this genetic material to grow dozens of saplings, clones of the ancient trees, a process that takes approximately two-and-a-half-years. The Archangel Ancient Tree Archive has already planted nearly 100 of these saplings in the Eden Project garden in Cornwall, England, a couple hundred in Oregon, and is organizing further groves of saplings in nine other countries .These saplings have extraordinary potential to purify our air, water, and soil for generations to come, Milarch said. We hope [the San Francisco] super grove, which has the capability to become an eternal forest, is allowed to grow unmolested by manmade or natural disasters and thus propagate forever. The Fieldbrook Stump, in California, not long after it was felled in 1890. Cuttings from it have been used to create the new cloned saplings planted recently in San Francisco. Ericson Collection, Humboldt State University Library\n",
      "Sentences: 17\n",
      "\n",
      "\n",
      "Article: https://alexanderperrin.com.au/triangles/ballooning/\n",
      "NodeRank:\n",
      "More Works | Source Code\n",
      "Sentences: 1\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "\n",
      "Sentences: 0\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "\n",
      "Sentences: 0\n",
      "\n",
      "\n",
      "Article: http://www.randomhacks.net/2005/10/11/amb-operator/\n",
      "NodeRank:\n",
      "# A list of places we can \"rewind\" to# if we encounter amb with no# arguments.$backtrack_points = []# Rewind to our most recent backtrack# point. def backtrack  if $backtrack_points. empty? raise \"Can't backtrack\"  else    $backtrack_points. pop. call  endend# Recursive implementation of the# amb operator. def amb *choices  # Fail if we have no arguments. backtrack if choices. empty? callcc {|cc|    # cc contains the \"current    # continuation\". When called,    # it will make the program    # rewind to the end of this block. $backtrack_points. push cc    # Return our first argument. return choices[0]  }  # We only get here if we backtrack  # using the stored value of cc,  # above. We call amb recursively  # with the arguments we didn't use. amb *choices[1...choices. length]end# Backtracking beyond a call to cut# is strictly forbidden. def cut  $backtrack_points = []end\n",
      "Sentences: 18\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "Back in 1961, John McCarthy (the inventor of LISP) described an interesting mathematical operator called amb . Essentially, amb hates to be called with no arguments, and can look into the future to keep that from happening. Here's how it might look in Ruby.# amb will (appear to) choose values # for x and y that prevent future # trouble. x = amb 1 , 2 , 3 y = amb 4 , 5 , 6 # Ooops! If x*y isn't 8, amb would # get angry. You wouldn't like # amb when it's angry. amb if x * y != 8 # Sure enough, x is 2 and y is 4. puts x , yOf course, amb can't actually see the future. However, it can rewind into the past whenever it sees trouble, and try a different coice. So, how could we implement this function? As it turns out, we need continuations. Here's a basic implementation in Ruby.# A list of places we can \"rewind\" to # if we encounter amb with no # arguments. $backtrack_points = [] # Rewind to our most recent backtrack # point. def backtrack if $backtrack_points . empty? raise \"Can't backtrack\" else $backtrack_points . pop . call end end # Recursive implementation of the # amb operator. def amb * choices # Fail if we have no arguments. backtrack if choices . empty? callcc { | cc | # cc contains the \"current # continuation\". When called, # it will make the program # rewind to the end of this block. $backtrack_points . push cc # Return our first argument. return choices [ 0 ] } # We only get here if we backtrack # using the stored value of cc, # above. We call amb recursively # with the arguments we didn't use. amb * choices [ 1 ... choices . length ] end # Backtracking beyond a call to cut # is strictly forbidden. def cut $backtrack_points = [] endIf you'd like a fun, non-technical overview of continuations, see the explanation at RubyGarden .\n",
      "Sentences: 29\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "Random code snippets, projects and musings about software from Eric Kidd, a developer and entrepreneur. You're welcome to contact me !McCarthy's Ambiguous OperatorOct 11, 2005 â by Eric KiddBack in 1961, John McCarthy (the inventor of LISP) described an interesting mathematical operator called amb. Essentially, amb hates to be called with no arguments, and can look into the future to keep that from happening. Here's how it might look in Ruby.# amb will (appear to) choose values # for x and y that prevent future # trouble. x = amb 1, 2, 3 y = amb 4, 5, 6 # Ooops! If x*y isn't 8, amb would # get angry. You wouldn't like # amb when it's angry. amb if x*y != 8 # Sure enough, x is 2 and y is 4. puts x, yOf course, amb can't actually see the future. However, it can rewind into the past whenever it sees trouble, and try a different coice. So, how could we implement this function? As it turns out, we need continuations. Here's a basic implementation in Ruby.# A list of places we can \"rewind\" to # if we encounter amb with no # arguments. $backtrack_points = [] # Rewind to our most recent backtrack # point. def backtrack if $backtrack_points. empty? raise \"Can't backtrack\" else $backtrack_points. pop. call end end # Recursive implementation of the # amb operator. def amb *choices # Fail if we have no arguments. backtrack if choices. empty? callcc {|cc| # cc contains the \"current # continuation\". When called, # it will make the program # rewind to the end of this block. $backtrack_points. push cc # Return our first argument. return choices[0] } # We only get here if we backtrack # using the stored value of cc, # above. We call amb recursively # with the arguments we didn't use. amb *choices[1...choices. length] end # Backtracking beyond a call to cut # is strictly forbidden. def cut $backtrack_points = [] endIf you'd like a fun, non-technical overview of continuations, see the explanation at RubyGarden .More postsWant to contact me about this article? Or if you're looking for something else to read, here's a list of popular posts .Dan Piponi wrote on Feb 22, 2006:I have an implementation of \"amb\" that can be used in C here . Thanks for giving a name to this operator for me - I never knew what to call it. Ben          wrote on Jan 17, 2007:Thats really interesting. It means that you could write a constraint DSL in Ruby or any other language which implements continuations. I thought constraint engines were hard... Cheers BenBen          wrote on Jan 17, 2007:Wow, I just implemented the send+more=money solver with amb:# define variables m = 1 n = amb(0, 2, 3, 4, 5, 6, 7, 8, 9) o = amb(0, 2, 3, 4, 5, 6, 7, 8, 9) r = amb(0, 2, 3, 4, 5, 6, 7, 8, 9) s = amb(2, 3, 4, 5, 6, 7, 8, 9) y = amb(0, 2, 3, 4, 5, 6, 7, 8, 9) e = amb(0, 2, 3, 4, 5, 6, 7, 8, 9) d = amb(0, 2, 3, 4, 5, 6, 7, 8, 9)  # reduce the problem space amb unless(d+e == y || d+e == y+10) amb unless(s+m+1 >= 10)  # The actual problem send = s*1000 + e*100 + n*10 + d more = m*1000 + o*100 + n*10 + e money = m*10000 + o*1000 + n*100 + e*10 + y amb unless(send+more == money)  # p \"m=#{m}, n=#{n},  o=#{o}, r=#{r}, s=#{s}, y=#{y}, e=#{e}, d=#{d},\"  # Make sure all variables are different a = [m, n, o, r, s, y, e, d] t = Array. new(10, 0) a.each { |v| amb unless (t[v]+=1)<2 }  # stop cut  p \"Final solution: m=#{m}, n=#{n},  o=#{o}, r=#{r}, s=#{s}, y=#{y}, e=#{e}, d=#{d},\"Amb is pretty cool! BenEric Kidd          wrote on Jan 21, 2007:Wow! That's very cool. If you're into constraint languages, you might also enjoy Oz , which can do heavily-optimized searches of constraint problems. The big advantage of Oz: Before choosing which path to investigate, it tries to infer as much information as possible (aka \"propagation\"). And then it makes very intelligent guesses about exactly which \"guess\" to make next (aka \"distribution\"). Since a backtracking search takes O(2^N) time (yikes! ), it's worth doing a _lot_ of work before actually branching. And the most popular implementation of Oz can actually spread the search over an entire cluster of computers! Christian Theil Have wrote on Jun 08, 2007:\"Since a backtracking search takes O(2^N) time (yikes! ), itâs worth doing a lot of work before actually branching.\" This really depends on the nature of the problem. If you are only going to backtrack a little, it might not be worth spend cycles on forward-checking or ensuring arc consistency. Propagation can also be quite costly. But sure, in most cases, amb is not a very efficient way solving constraint problems.. Doug Auclair wrote on Nov 28, 2007:@Christian, you are quite right! For this particular problem (SEND+MORE=MONEY), eliminated selected digits narrows the search-space considerably, speeding up the calculation by a factor of ~50:  http://www. cotilliongroup. com/arts/DCG. html  (done with Prolog's definite clause grammars, which have a similar feel to Haskell's monads)  And, I suppose, one can use, e.g., Gwydion Dylan's matrix library (http://www. opendylan. org/gdref/gdlibs/libs-matrix-operations. html), along the lines of a gausse-jordan elimination, to solve the system of equations linearly? Perhaps not apropos to this problem, but handles several kinds of constraint problems. Doug Auclair wrote on Nov 29, 2007:The Haskell version of SEND+MORE=MONEY in the amb-style:> sendmory = [(s,e,n,d,m,o,r,y) | s <- pos, e <- all, n <- all, d <- all, >                                 m <- [1], o <- all, r <- all, y <- all, >                                 y == d + e || y == d + e - 10, >                                 s + m + 1 >= 10,  >                                 num [s,e,n,d] + num [m,o,r,e] >                                       == num [m,o,n,e,y], >                                 allDiff [s,e,n,d,m,o,r,y]] >   where >     pos = [2..9] >     all = 0:pos  > num :: [Int] -> Int > num = foldl ((+). (*10)) 0  > allDiff :: [Int] -> Bool > allDiff [] = True > allDiff (x:xs) = notElem x xs && allDiff xs... note that a definition for amb is unnecessary, as list compression (as a MonadPlus) already makes choice. The above, as already pointed out, is not very efficient, so using a state monad to narrow the selection in-place:> sendmory' :: StateT [Int] [] [Int] > sendmory' = -- do [m] <- item >             --    verify (m == 1) >             -- the first two lines can be written 'do let m = 1' >             -- iff the passed in list does not contain the number 1 >             do let m = 1 >                [s] <- item >                verify (s + m + 1 >= 10) >                [e] <- item >                [d] <- item >                [y] <- item >                verify (y == d + e || y == d + e - 10) >                [n] <- item >                [o] <- item >                [r] <- item >                verify (num [s,e,n,d] + num [m,o,r,e] >                                    == num [m,o,n,e,y]) >                -- item obviates allDiff >                return [s,e,n,d,m,o,r,y] >  > -- split list into all combinations of one element and rest > splits :: [a] -> [([a],[a])] > splits l = splitAccum [] l where >   splitAccum ys []     = [] >   splitAccum ys (x:xs) = ([x],xs++ys) : splitAccum (x:ys) xs  > choose :: StateT [a] [] [a] > choose = StateT $ \\s -> splits s   > item :: StateT [Int] [] [Int] > item = choose  > verify :: Bool -> StateT [Int] [] () > verify p = StateT $ \\s -> if p then [((), s)] else []splits, choose, and verify are thanks to Dirk Thierbach on comp. lang. haskell. This version is 200x faster than the amb-style one -- 10x speed up for redundant digit elimination and on top of that a 20x speed up from placing the guards as far up in the computation as possible. Although it requires some helper functions to implement nondeterminism with elimination, sendmory' retains the declarative feel of it's amb-styled counterpart. backtrack          wrote on Sep 09, 2008:There's also goal directed computation as implemented in the Icon language, though you probably know about it. person-b wrote on Jul 03, 2009:This reminds me of Damian Conway's Positronic::Variables CPAN Perl module. He wrote it for his (hilarious - highly recommended) talk on \"Temporally Quaquaversal Virtual Nanomachine Programming In Multiple Topologically Connected Quantum-Relativistic Parallel Timespaces... Made Easy!\" (http://blip. tv/file/1145545/). It actually did look into the future. Random Hacks\n",
      "Sentences: 73\n",
      "\n",
      "\n",
      "Article: https://www.zdnet.com/article/microsoft-security-chief-ie-is-not-a-browser-so-stop-using-it-as-your-default/\n",
      "NodeRank:\n",
      "Is Internet Explorer (IE) a browser? According to Microsoft, no. Today, it's a 'compatibility solution' for enterprise customers to deal with legacy sites that should be updated for modern browsers. Top cloud providers 2019: AWS, Microsoft Azure, Google Cloud; IBM makes hybrid move; Salesforce dominates SaaSGood enough 5G fixed-wireless broadband could change everythingRussia to disconnect from the internet as part of a planned testAn Apple store salesman questioned my intelligence, says customerChris Jackson, Microsoft's worldwide lead for cybersecurity, really doesn't want enterprise customers to use IE for all web traffic, even though for some organizations that would be the easiest option. Companies in that situation are willing to take on 'technical debt', such as paying for extended support for a legacy software, but that habit needs to stop in the case of IE, argues Jackson ina new blog post, 'The perils of using Internet Explorer as your default browser'.ALSO: Microsoft says you shouldn't buy its awful softwareThe main gist of Jackson's argument is you should only use IE selectively for internal sites that need it, pointing to tools like Enterprise Mode Site List in IE 11 that help customers make the transition and limit IE use to where it's needed. Jackson doesn't mention anywhere that customers should use Edge, the soon-to-be Chromium-based browser. Nor does he suggest using Chrome or Firefox, only that most developers aren't testing sites for IE. \"I'm not here to enforce any browser on anyone. Windows gives you a choice in your browser, and you should choose the one that best meets your needs,\" he replied to one commenter. Jackson doesn't even consider IE to be a browser, at least in the modern, standards-based sense. \"You see, Internet Explorer is a compatibility solution,\" wrote Jackson in the blog. \"We're not supporting new web standards for it and, while many sites work fine, developers by and large just aren't testing for Internet Explorer these days. They're testing on modern browsers. \"So, if we continued our previous approach, you would end up in a scenario where, by optimizing for the things you have, you end up not being able to use new apps as they come out. As new apps are coming out with greater frequency, what we want to help you do is avoid having to miss out on a progressively larger portion of the web.\" Jackson admits that Microsoft is partly to blame for customers' willingness to take on technical debt. In particular, he singles out Internet Explorer 6, released in 2001, the year of Microsoft's IE-Windows antitrust settlement in the US. All the Chromium-based browsersSEE FULL GALLERY1 - 5 of 14                            NEXT      PREV                        In a section called 'Creating technical debt by default', Jackson notes: \"In the past, Internet Explorer was optimized for simplicity at the expense of technical debt. Looking all the way back to Internet Explorer 6, the very concept of 'standards mode' vs 'quirks mode' comes from this 'easy button' approach.\" More effort on the part of IT teams was required to get to standards mode, which made \"getting modern\" an opt-in choice. SEE:20 pro tips to make Windows 10 work the way you want(free PDF)Jackson explains that as IE began to support more standards, Microsoft also realized it risked breaking applications written for an older interpretation of the standards. \"So, with Internet Explorer 8 (IE8), we added IE8 standards, but also kept Internet Explorer 7 (IE7) standards. That meant, for sites in the internet zone, it would default to IE8 standards, but, for sites in the local intranet zone, it would default to IE7 standards,\" explains Jackson, noting this was also an 'easy button' solution. \"As you can see, by going with the 'technical debt by default' approach, we ended up in a scenario whereby if you create a brand-new webpage today, run it in the local intranet zone, and don't add any additional markup, you will end up using a 1999 implementation of web standards by default. Yikes.\" Microsoft makes final push to rid world of Internet Explorer 10 Enterprise customers running Windows Server 2012 have one year to change from IE10 to IE11. Microsoft confirms that Chrome extensions will run on new Edge browser  Microsoft's Chromium-based Edge browser could close the extension gap. Mozilla: Why Microsoft Edge's switch to Google's Chromium is bad news Microsoft's move probably won't help Edge and it's also bad for the open web, say Mozilla, Vivaldi. Microsoft's Edge to morph into a Chromium-based, cross-platform browser Microsoft is going to remake its Edge desktop browser by using Chromium components and by bringing it to Windows 7, 8.1 and macOS, in addition to Windows 10. Microsoft Edge: What went wrong, what's next Microsoft's grand browser experiment flopped in the marketplace, so the company is turning to an unlikely successor: the open-source Chromium project. Can it succeed where EdgeHTML failed? Apple killing off web passwords? Safari trials WebAuthn logins on macOS Safari could join Firefox, Chrome, and Edge support for Web Authentication. Microsoft reportedly looking to ditch Edge for Chromium The world of web renderers could be down a significant member should Microsoft can its EdgeHTML renderer. Google is raiding Firefox for Chrome's next UI features Tab groups and a scrollable tabs bar are coming to Chrome. When? Yet unknown. How to use Vivaldi Quick CommandsTechRepublic Why you should stop pointing and clicking your way around an interface and instead Vivaldi's new Quick Commands feature. Google cracks down on malicious Chrome extensionsCNET A more rigorous review process that includes more humans seeks to better scrutinize extensions that demand lots of power.\n",
      "Sentences: 39\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "Is Internet Explorer (IE) a browser? According to Microsoft, no. Today, it's a 'compatibility solution' for enterprise customers to deal with legacy sites that should be updated for modern browsers. Chris Jackson, Microsoft's worldwide lead for cybersecurity, really doesn't want enterprise customers to use IE for all web traffic, even though for some organizations that would be the easiest option. Companies in that situation are willing to take on 'technical debt', such as paying for extended support for a legacy software, but that habit needs to stop in the case of IE, argues Jackson in a new blog post , 'The perils of using Internet Explorer as your default browser'.The main gist of Jackson's argument is you should only use IE selectively for internal sites that need it, pointing to tools like Enterprise Mode Site List in IE 11 that help customers make the transition and limit IE use to where it's needed. Jackson doesn't mention anywhere that customers should use Edge, the soon-to-be Chromium-based browser . Nor does he suggest using Chrome or Firefox, only that most developers aren't testing sites for IE. \"I'm not here to enforce any browser on anyone. Windows gives you a choice in your browser, and you should choose the one that best meets your needs,\" he replied to one commenter. Jackson doesn't even consider IE to be a browser, at least in the modern, standards-based sense. \"You see, Internet Explorer is a compatibility solution,\" wrote Jackson in the blog. \"We're not supporting new web standards for it and, while many sites work fine, developers by and large just aren't testing for Internet Explorer these days. They're testing on modern browsers. \"So, if we continued our previous approach, you would end up in a scenario where, by optimizing for the things you have, you end up not being able to use new apps as they come out. As new apps are coming out with greater frequency, what we want to help you do is avoid having to miss out on a progressively larger portion of the web. \"Jackson admits that Microsoft is partly to blame for customers' willingness to take on technical debt. In particular, he singles out Internet Explorer 6, released in 2001, the year of Microsoft's IE-Windows antitrust settlement in the US .1 - 5 of 14 NEXT PREVIn a section called 'Creating technical debt by default', Jackson notes: \"In the past, Internet Explorer was optimized for simplicity at the expense of technical debt. Looking all the way back to Internet Explorer 6, the very concept of 'standards mode' vs 'quirks mode' comes from this 'easy button' approach. \"More effort on the part of IT teams was required to get to standards mode, which made \"getting modern\" an opt-in choice. SEE: 20 pro tips to make Windows 10 work the way you want (free PDF)Jackson explains that as IE began to support more standards, Microsoft also realized it risked breaking applications written for an older interpretation of the standards. \"So, with Internet Explorer 8 (IE8), we added IE8 standards, but also kept Internet Explorer 7 (IE7) standards. That meant, for sites in the internet zone, it would default to IE8 standards, but, for sites in the local intranet zone, it would default to IE7 standards,\" explains Jackson, noting this was also an 'easy button' solution. \"As you can see, by going with the 'technical debt by default' approach, we ended up in a scenario whereby if you create a brand-new webpage today, run it in the local intranet zone, and don't add any additional markup, you will end up using a 1999 implementation of web standards by default. Yikes. \"Previous and related coverageEnterprise customers running Windows Server 2012 have one year to change from IE10 to IE11. Microsoft's Chromium-based Edge browser could close the extension gap. Microsoft's move probably won't help Edge and it's also bad for the open web, say Mozilla, Vivaldi. Microsoft is going to remake its Edge desktop browser by using Chromium components and by bringing it to Windows 7, 8.1 and macOS, in addition to Windows 10. Microsoft's grand browser experiment flopped in the marketplace, so the company is turning to an unlikely successor: the open-source Chromium project. Can it succeed where EdgeHTML failed? Safari could join Firefox, Chrome, and Edge support for Web Authentication. The world of web renderers could be down a significant member should Microsoft can its EdgeHTML renderer. Tab groups and a scrollable tabs bar are coming to Chrome. When? Yet unknown. Why you should stop pointing and clicking your way around an interface and instead Vivaldi's new Quick Commands feature. A more rigorous review process that includes more humans seeks to better scrutinize extensions that demand lots of power.\n",
      "Sentences: 37\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "An Apple store salesman questioned my intelligence, says customerChris Jackson, Microsoft's worldwide lead for cybersecurity, really doesn't want enterprise customers to use IE for all web traffic, even though for some organizations that would be the easiest option. Companies in that situation are willing to take on 'technical debt', such as paying for extended support for a legacy software, but that habit needs to stop in the case of IE, argues Jackson in a new blog post , 'The perils of using Internet Explorer as your default browser'.ALSO: Microsoft says you shouldn't buy its awful software The main gist of Jackson's argument is you should only use IE selectively for internal sites that need it, pointing to tools like Enterprise Mode Site List in IE 11 that help customers make the transition and limit IE use to where it's needed. Jackson doesn't mention anywhere that customers should use Edge, the soon-to-be Chromium-based browser . Nor does he suggest using Chrome or Firefox, only that most developers aren't testing sites for IE. \"I'm not here to enforce any browser on anyone. Windows gives you a choice in your browser, and you should choose the one that best meets your needs,\" he replied to one commenter. Jackson doesn't even consider IE to be a browser, at least in the modern, standards-based sense. \"You see, Internet Explorer is a compatibility solution,\" wrote Jackson in the blog. \"We're not supporting new web standards for it and, while many sites work fine, developers by and large just aren't testing for Internet Explorer these days. They're testing on modern browsers. \"So, if we continued our previous approach, you would end up in a scenario where, by optimizing for the things you have, you end up not being able to use new apps as they come out. As new apps are coming out with greater frequency, what we want to help you do is avoid having to miss out on a progressively larger portion of the web. \"Jackson admits that Microsoft is partly to blame for customers' willingness to take on technical debt. In particular, he singles out Internet Explorer 6, released in 2001, the year of Microsoft's IE-Windows antitrust settlement in the US .\n",
      "Sentences: 14\n",
      "\n",
      "\n",
      "Article: https://github.com/Jeff-Ciesielski/synesthesia\n",
      "NodeRank:\n",
      "My career has been mostly in the embedded space, and while this arenais largely dominated by C (which I have a great affinity for), onething I've always enjoyed is playing with interesting languages thatwork on small targets to scratch my language polyglot itch. Nim has been my weapon of choice for this lately, but I had beentoying around with the idea of writing a forth interpreter/compiler inNim to work on small embedded targets. While I was working on my first draft (which I don't think will eversee the light of day since it's so dreadful), it struck me that theself-modifying and compile-time-evaluation nature of forth programswere a very good fit for nim's compile time macro system, and thatit would be a really neat project to implement a forth->nim compileras nim macros, which could then be compiled targeting embedded devicesto produce efficient native machine code rather than interpreting onthe fly. To that end, I decided that a proof of concept was in order, anddecided that brainfuck would be a great target for a first attemptgiven its simplicity, and the wealth of knowledge on the subject on theinternet and great siteslike esolangs. Once I got started, I found that there was also a bunch of greatinformation about optimizing BF, so I figured \"why not implement someof that too?\" and it just sort of ran away from me. Whew, sorry about that novel, but before going any further, I'd liketo thank the proprieters of the following sites for their excellentdescriptions of various optimizations as they were critical for theoutcome of this project:http://calmerthanyouare. org/2015/01/07/optimizing-brainfuck. htmlhttps://www. nayuki. io/page/optimizing-brainfuck-compilerNim compiler (v 0.18) (I recommend using the excellent choosenim)Some brainfuck source code you'd like to compileInstall the nim compiler (see above)Clone the repoType nimble install(I plan to eventually upload this to the nimple package directory)To compile, use the -c flag like so:synesthesia -c mendel. bfBy default, the compiler will generate an a.out file in the currentdirectory. If you'd like to specify an alternative output file, onecan be specified with the -o flag;synesthesia -c mendel. bf -o mendelbrotsynesthesia also includes an optimizing brainfuck interpreter. Tointerpret a file, use the -i flag:synesthesia -i mendel. bfNim includes a number of useful properties that uniquely position itfor this sort of project. The first is its hygienic macro systemwhich allows for compile time code generation. The second is the ability to execute 'pure' code at compile time (purebeing code that doesn't use FFI). Not everything works (I've foundnested generators to fail pretty interestingly), but the vast majorityof the nim language can be used. Combining this with the Macro/ASTgeneration system allows one to perform interesting transforms on ASTnodes. Finally, nim allows one to read files at compile time and act on theircontents. In the past, I've used this to generate register defnitionsfor microcontrollers from their header files, but in this instance,this functionality is used to slurp the BF source file and iterateover its contents. (Note before reading further: I'm hardly an expert on compilerconstruction, so please be gentle if I use incorrect terminology :) )For simplicity, we generate a very simple nim source file containingthe imports required to use the compiler module, and a call tosynesthesia. compile(<path/to/bf/source>). We then call out to thenim compiler with this file as the target to begin compilation. This file is compiled with the release and optimize-for-size flagsapplied (size optimization tends to produce faster code than speedoptimization due to the nature of the code generated)Once a BF source file has been opened and the contents read into asequence of characters, this sequence is iterated over and eachrelevant character is converted into an object: BFToken. BFTokenis a variant type (i.e. it includes a kind field, think taggedunions in c).For example, the '>' character causes the AP (memory cell index) tobe incremented by one, and '<' causes it to be decremented by one. Given that, we can conclude that we need an ApAdjust token for +1,and another for -1. With variant types, we can simply include anamt field in the bfsApAdjust token, and generate an appropriatevariant when each token is encountered. (The same idea goes for memory adjustment with the bfsMemAdjustvariant)A full listing of charcter => token mappings can be found insrc/synesthesiapkg/common. nimsynesthesia implements a set of peephole optimizersthat are applied to the resulting list of tokens. Some of these optimizations areobvious from the top level BF source (coalescing adjustments forexample), while others work best if applied after other optimizationshave already been made (dead adjustments / combining memory sets)A full accounting of the optimizations applied can be found insrc/synesthesiapkg/optimizer. nim, but to give the reader an idea ofthe sorts of things that are going on:Adjacent AP and Mem adjustments (i.e. >>>>> or +++) can besquished into single instructions (ap + 5 and mem[ap] + 3accordingly). We use the amt field in the object to track thetotal amount. Note that this works by tracking the total amount, so+++--- becomes mem[ap] + 0Dead adjustments can be eliminated, so any ap or mem adjustment withan amt of 0 can simply be removed from the set of instructions. Clearing the current memory cell is a common pattern in BF [-].Rather than sitting in a loop and decrementing the current celluntil it hits zero, one can simply translate this to mem[ap] = 0,which is constant time. More interesting optimizations include things like transforming loopsinto multiplication instructions and deferring AP adjustments by usingoffsets. Once all optimizations are applied, AST generation can begin. For themost part, ast generation is pretty strait forward, tokens are simplytransformed into NimNode objects representing their underlying purpose. For example:bfsApAdjust(amount) => ap += amountbfsMemAdjust(offset, amount) => mem[ap + offset] += amountbfsPrint => putChar(mem[ap])One notable exception to this is bfsBlock and bfsBlockEnd (i.e. loops in BF).synesthesia implements blocks as while loops (sort of, but weuse if => doWhile for performance reasons)As we need to keep track of loops, we maintain a stack of 'blocks'during compilation. As other tokens are decoded, their NimNodes areadded to the top block in the stack (i.e. their statements exist underthe lexical scope of the last known open loop). When a new block isencountered ([ in BF), we generate a while loop scope and push itonto the stack. When a block ends (]), we pop the block off thestack and continue on. Once all AST nodes have been generated, the resulting nim code (whichwe never see) is compiled to C, and then to machine code. The synesthesia compiler is licensed under the GPLv2. Any resultingbinaries are licensed at the creator's discretion.\n",
      "Sentences: 50\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "How this came aboutMy career has been mostly in the embedded space, and while this arena is largely dominated by C (which I have a great affinity for), one thing I've always enjoyed is playing with interesting languages that work on small targets to scratch my language polyglot itch. Nim has been my weapon of choice for this lately, but I had been toying around with the idea of writing a forth interpreter/compiler in Nim to work on small embedded targets. While I was working on my first draft (which I don't think will ever see the light of day since it's so dreadful), it struck me that the self-modifying and compile-time-evaluation nature of forth programs were a very good fit for nim's compile time macro system, and that it would be a really neat project to implement a forth->nim compiler as nim macros, which could then be compiled targeting embedded devices to produce efficient native machine code rather than interpreting on the fly. To that end, I decided that a proof of concept was in order, and decided that brainfuck would be a great target for a first attempt given its simplicity, and the wealth of knowledge on the subject on the internet and great sites like esolangs .Once I got started, I found that there was also a bunch of great information about optimizing BF, so I figured \"why not implement some of that too?\" and it just sort of ran away from me. Whew, sorry about that novel, but before going any further, I'd like to thank the proprieters of the following sites for their excellent descriptions of various optimizations as they were critical for the outcome of this project: http://calmerthanyouare. org/2015/01/07/optimizing-brainfuck. html https://www. nayuki. io/page/optimizing-brainfuck-compilerRequirements Nim compiler (v 0.18) (I recommend using the excellent choosenim ) Some brainfuck source code you'd like to compileInstallation: Install the nim compiler (see above) Clone the repo Type nimble install(I plan to eventually upload this to the nimple package directory)Compiling BF filesTo compile, use the -c flag like so:synesthesia -c mendel. bfBy default, the compiler will generate an a.out file in the current directory. If you'd like to specify an alternative output file, one can be specified with the -o flag;synesthesia -c mendel. bf -o mendelbrotInterpreting BF filessynesthesia also includes an optimizing brainfuck interpreter. To interpret a file, use the -i flag:synesthesia -i mendel. bfHow compilation worksNim includes a number of useful properties that uniquely position it for this sort of project. The first is its hygienic macro system which allows for compile time code generation. The second is the ability to execute 'pure' code at compile time (pure being code that doesn't use FFI). Not everything works (I've found nested generators to fail pretty interestingly), but the vast majority of the nim language can be used. Combining this with the Macro/AST generation system allows one to perform interesting transforms on AST nodes. Finally, nim allows one to read files at compile time and act on their contents. In the past, I've used this to generate register defnitions for microcontrollers from their header files, but in this instance, this functionality is used to slurp the BF source file and iterate over its contents. (Note before reading further: I'm hardly an expert on compiler construction, so please be gentle if I use incorrect terminology :) )Step 0: Generate a temp source fileFor simplicity, we generate a very simple nim source file containing the imports required to use the compiler module, and a call to synesthesia. compile(<path/to/bf/source>) . We then call out to the nim compiler with this file as the target to begin compilation. This file is compiled with the release and optimize-for-size flags applied (size optimization tends to produce faster code than speed optimization due to the nature of the code generated)Step 1: Transformation to a list of tokensOnce a BF source file has been opened and the contents read into a sequence of characters, this sequence is iterated over and each relevant character is converted into an object: BFToken . BFToken is a variant type (i.e. it includes a kind field, think tagged unions in c).For example, the '>' character causes the AP (memory cell index) to be incremented by one, and '<' causes it to be decremented by one. Given that, we can conclude that we need an ApAdjust token for +1, and another for -1. With variant types, we can simply include an amt field in the bfsApAdjust token, and generate an appropriate variant when each token is encountered. (The same idea goes for memory adjustment with the bfsMemAdjust variant)A full listing of charcter => token mappings can be found in src/synesthesiapkg/common. nimStep 2: Optimizationsynesthesia implements a set of peephole optimizers that are applied to the resulting list of tokens. Some of these optimizations are obvious from the top level BF source (coalescing adjustments for example), while others work best if applied after other optimizations have already been made (dead adjustments / combining memory sets)A full accounting of the optimizations applied can be found in src/synesthesiapkg/optimizer. nim , but to give the reader an idea of the sorts of things that are going on: Adjacent AP and Mem adjustments (i.e. >>>>> or +++ ) can be squished into single instructions ( ap + 5 and mem[ap] + 3 accordingly). We use the amt field in the object to track the total amount. Note that this works by tracking the total amount, so +++--- becomes mem[ap] + 0 Dead adjustments can be eliminated, so any ap or mem adjustment with an amt of 0 can simply be removed from the set of instructions. Clearing the current memory cell is a common pattern in BF [-] . Rather than sitting in a loop and decrementing the current cell until it hits zero, one can simply translate this to mem[ap] = 0 , which is constant time. More interesting optimizations include things like transforming loops into multiplication instructions and deferring AP adjustments by using offsets. Step 3: AST GenerationOnce all optimizations are applied, AST generation can begin. For the most part, ast generation is pretty strait forward, tokens are simply transformed into NimNode objects representing their underlying purpose. For example: bfsApAdjust(amount) => ap += amount bfsMemAdjust(offset, amount) => mem[ap + offset] += amount bfsPrint => putChar(mem[ap])One notable exception to this is bfsBlock and bfsBlockEnd (i.e. loops in BF).synesthesia implements blocks as while loops (sort of, but we use if => doWhile for performance reasons)As we need to keep track of loops, we maintain a stack of 'blocks' during compilation. As other tokens are decoded, their NimNodes are added to the top block in the stack (i.e. their statements exist under the lexical scope of the last known open loop). When a new block is encountered ( [ in BF), we generate a while loop scope and push it onto the stack. When a block ends ( ] ), we pop the block off the stack and continue on. Once all AST nodes have been generated, the resulting nim code (which we never see) is compiled to C, and then to machine code. The synesthesia compiler is licensed under the GPLv2. Any resulting binaries are licensed at the creator's discretion.\n",
      "Sentences: 50\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "Nim compiler (v 0.18) (I recommend using the excellent choosenim )Some brainfuck source code you'd like to compileUseInstall the nim compiler (see above)Clone the repo(I plan to eventually upload this to the nimple package directory)Compiling BF filesTo compile, use the -c flag like so:synesthesia -c mendel. bfBy default, the compiler will generate an a.out file in the current directory. If you'd like to specify an alternative output file, one can be specified with the -o flag;synesthesia -c mendel. bf -o mendelbrotInterpreting BF filessynesthesia also includes an optimizing brainfuck interpreter. To interpret a file, use the -i flag:synesthesia -i mendel. bfHow compilation worksNim includes a number of useful properties that uniquely position it for this sort of project. The first is its hygienic macro system which allows for compile time code generation. The second is the ability to execute 'pure' code at compile time (pure being code that doesn't use FFI). Not everything works (I've found nested generators to fail pretty interestingly), but the vast majority of the nim language can be used. Combining this with the Macro/AST generation system allows one to perform interesting transforms on AST nodes. Finally, nim allows one to read files at compile time and act on their contents. In the past, I've used this to generate register defnitions for microcontrollers from their header files, but in this instance, this functionality is used to slurp the BF source file and iterate over its contents. (Note before reading further: I'm hardly an expert on compiler construction, so please be gentle if I use incorrect terminology :) )Step 0: Generate a temp source fileFor simplicity, we generate a very simple nim source file containing the imports required to use the compiler module, and a call to synesthesia. compile(<path/to/bf/source>). We then call out to the nim compiler with this file as the target to begin compilation. This file is compiled with the release and optimize-for-size flags applied (size optimization tends to produce faster code than speed optimization due to the nature of the code generated)Step 1: Transformation to a list of tokensOnce a BF source file has been opened and the contents read into a sequence of characters, this sequence is iterated over and each relevant character is converted into an object: BFToken. BFToken is a variant type (i.e. it includes a kind field, think tagged unions in c).For example, the '>' character causes the AP (memory cell index) to be incremented by one, and '<' causes it to be decremented by one. Given that, we can conclude that we need an ApAdjust token for +1, and another for -1. With variant types, we can simply include an amt field in the bfsApAdjust token, and generate an appropriate variant when each token is encountered. (The same idea goes for memory adjustment with the bfsMemAdjust variant)A full listing of charcter => token mappings can be found in src/synesthesiapkg/common. nimStep 2: Optimizationsynesthesia implements a set of peephole optimizers that are applied to the resulting list of tokens. Some of these optimizations are obvious from the top level BF source (coalescing adjustments for example), while others work best if applied after other optimizations have already been made (dead adjustments / combining memory sets)A full accounting of the optimizations applied can be found in src/synesthesiapkg/optimizer. nim, but to give the reader an idea of the sorts of things that are going on:Adjacent AP and Mem adjustments (i.e. >>>>> or +++) can be squished into single instructions (ap + 5 and mem[ap] + 3 accordingly). We use the amt field in the object to track the total amount. Note that this works by tracking the total amount, so +++--- becomes mem[ap] + 0Dead adjustments can be eliminated, so any ap or mem adjustment with an amt of 0 can simply be removed from the set of instructions. Clearing the current memory cell is a common pattern in BF [-]. Rather than sitting in a loop and decrementing the current cell until it hits zero, one can simply translate this to mem[ap] = 0, which is constant time. More interesting optimizations include things like transforming loops into multiplication instructions and deferring AP adjustments by using offsets. Step 3: AST GenerationOnce all optimizations are applied, AST generation can begin. For the most part, ast generation is pretty strait forward, tokens are simply transformed into NimNode objects representing their underlying purpose. For example:bfsMemAdjust(offset, amount) => mem[ap + offset] += amountbfsPrint => putChar(mem[ap])One notable exception to this is bfsBlock and bfsBlockEnd (i.e. loops in BF).synesthesia implements blocks as while loops (sort of, but we use if => doWhile for performance reasons)As we need to keep track of loops, we maintain a stack of 'blocks' during compilation. As other tokens are decoded, their NimNodes are added to the top block in the stack (i.e. their statements exist under the lexical scope of the last known open loop). When a new block is encountered ([ in BF), we generate a while loop scope and push it onto the stack. When a block ends (]), we pop the block off the stack and continue on. Once all AST nodes have been generated, the resulting nim code (which we never see) is compiled to C, and then to machine code. LicenseThe synesthesia compiler is licensed under the GPLv2. Any resulting binaries are licensed at the creator's discretion. 2019 GitHub, Inc.\n",
      "Sentences: 42\n",
      "\n",
      "\n",
      "Article: https://www.nowpublishers.com/article/Details/RBE-0092\n",
      "NodeRank:\n",
      "Select the format to use for exporting the citation. BibTeXRISExportClose Review of Behavioral Economics >    Vol 5 > Issue 3-4Gerd Gigerenzer, Max Planck Institute for Human Development, Germany, gigerenzer@mpib-berlin. mpg. de                Suggested Citation            Gerd Gigerenzer (2018), \"The Bias Bias in Behavioral Economics\", Review of Behavioral Economics: Vol. 5: No. 3-4, pp 303-336. http://dx. doi. org/10. 1561/105. 00000092            ExportPublished: 31 Dec 2018 2018 G. GigerenzerSubjectsBehavioral EconomicsKeywordsBehavioral economics,Biases,Bounded Rationality,Imperfect information Inactive download button? 1 Title = 3 Formats? Citing? Open Access This is published under the terms of CC-BY. In this article: 1. Part I: The Irrationality Argument2. Part II: Case Studies in the Bias BiasReferences Behavioral economics began with the intention of eliminating the        psychological blind spot in rational choice theory and ended up        portraying psychology as the study of irrationality. In its portrayal,        people have systematic cognitive biases that are not only as persistent        as visual illusions but also costly in real lifemeaning that        governmental paternalism is called upon to steer people with the        help of nudges. These biases have since attained the status of        truisms. In contrast, I show that such a view of human nature is        tainted by a bias bias, the tendency to spot biases even when there        are none. This may occur by failing to notice when small sample        statistics differ from large sample statistics, mistaking peoples random        error for systematic error, or confusing intelligent inferences        with logical errors. Unknown to most economists, much of psychological        research reveals a different portrayal, where people appear        to have largely fine-tuned intuitions about chance, frequency, and        framing. A systematic review of the literature shows little evidence        that the alleged biases are potentially costly in terms of less health,        wealth, or happiness. Getting rid of the bias bias is a precondition        for psychology to play a positive role in economics. DOI:10. 1561/105. 00000092CompanionReview of Behavioral Economics, Volume 5, Issue 3-4 Special issue Paternalism: Articles OveriewSee the other articles that are part of this special issue.\n",
      "Sentences: 26\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "Review of Behavioral Economics, Volume 5, Issue 3-4 Special issue Paternalism: Articles Overiew See the other articles that are part of this special issue.\n",
      "Sentences: 1\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "The Bias Bias in Behavioral EconomicsGerd Gigerenzer, Max Planck Institute for Human Development, Germany, gigerenzer@mpib-berlin. mpg. deSuggested CitationGerd Gigerenzer (2018), \"The Bias Bias in Behavioral Economics\", Review of Behavioral Economics: Vol. 5: No. 3-4, pp 303-336. http://dx. doi. org/10. 1561/105. 00000092ExportBehavioral economics,Biases,Bounded Rationality,Imperfect informationThis is published under the terms of CC-BY. In this article:1. Part I: The Irrationality Argument2. Part II: Case Studies in the Bias BiasReferencesAbstractBehavioral economics began with the intention of eliminating the         psychological blind spot in rational choice theory and ended up         portraying psychology as the study of irrationality. In its portrayal,         people have systematic cognitive biases that are not only as persistent         as visual illusions but also costly in real lifemeaning that         governmental paternalism is called upon to steer people with the         help of nudges. These biases have since attained the status of         truisms. In contrast, I show that such a view of human nature is         tainted by a bias bias, the tendency to spot biases even when there         are none. This may occur by failing to notice when small sample         statistics differ from large sample statistics, mistaking peoples random         error for systematic error, or confusing intelligent inferences         with logical errors. Unknown to most economists, much of psychological         research reveals a different portrayal, where people appear         to have largely fine-tuned intuitions about chance, frequency, and         framing. A systematic review of the literature shows little evidence         that the alleged biases are potentially costly in terms of less health,         wealth, or happiness. Getting rid of the bias bias is a precondition         for psychology to play a positive role in economics. DOI:10. 1561/105. 00000092\n",
      "Sentences: 22\n",
      "\n",
      "\n",
      "Article: https://eng.uber.com/introducing-ludwig/\n",
      "NodeRank:\n",
      "Over the last decade, deep learning models have proven highly effective at performing a wide variety of machine learning tasks in vision, speech, and language. At Uber we are using these models for a variety of tasks, including customer support, object detection, improving maps, streamlining chat communications, forecasting, and preventing fraud. Many open source libraries, including TensorFlow, PyTorch, CNTK, MXNET, and Chainer, among others, have implemented the building blocks needed to build such models, allowing for faster and less error-prone development. This, in turn, has propelled the adoption of such models both by the machine learning research community and by industry practitioners, resulting in fast progress in both architecture design and industrial solutions. At Uber AI, we decided to avoid reinventing the wheel and to develop packages built on top of the strong foundations open source libraries provide. To this end, in 2017 we released Pyro, a deep probabilistic programming language built on PyTorch, and continued to improve it with the help of the open source community. Another major open source AI tool created by Uber is Horovod, a framework hosted by the LF Deep Learning Foundation that allows distributed training of deep learning models over multiple GPUs and several machines. Extending our commitment to making deep learning more accessible, we are releasing Ludwig, an open source, deep learning toolbox built on top of TensorFlow that allows users to train and test deep learning models without writing code. Ludwig is unique in its ability to help make deep learning easier to understand for non-experts and enable faster model improvement iteration cycles for experienced machine learning developers and researchers alike. By using Ludwig, experts and researchers can simplify the prototyping process and streamline data processing so that they can focus on developing deep learning architectures rather than data wrangling. We have been developing Ludwig internally at Uber over the past two years to streamline and simplify the use of deep learning models in applied projects, as they usually require comparisons among different architectures and fast iteration. We have witnessed its value to several of Ubers own projects, including our Customer Obsession Ticket Assistant (COTA), information extraction from driver licenses, identification of points of interest during conversations between driver-partners and riders, food delivery time prediction, and much more. For this reason we decided to release it as open source, as we believe there is no other solution currently available with the same ease of use and flexibility. We originally designed Ludwig as a generic tool for simplifying the model development and comparison process when dealing with new applied machine learning problems. In order to do so, we drew inspiration from other machine learning software: from Wekaand MLlib, the idea of working directly with raw data and providing a certain number of pre-built models; from Caffe, the declarative nature of the definition file; and from scikit-learn, its simple programmatic API. This mix of influences makes it a pretty different tool from the usual deep learning libraries that provide tensor algebra primitives and few other utilities to code models, while at the same time making it more general than other specialized libraries like PyText, StanfordNLP, AllenNLP, and OpenCV. Ludwig provides a set of model architectures that can be combined together to create an end-to-end model for a given use case. As an analogy, if deep learning libraries provide the building blocks to make your building, Ludwig provides the buildings to make your city, and you can chose among the available buildings or add your own building to the set of available ones. The core design principles we baked into the toolbox are:No coding required: no coding skills are required to train a model and use it for obtaining predictions. Generality: a new data type-based approach to deep learning model design that makes the tool usable across many different use cases. Flexibility: experienced users have extensive control over model building and training, while newcomers will find it easy to use. Extensibility: easy to add new model architecture and new feature data types. Understandability: deep learning model internals are often considered black boxes, but we provide standard visualizations to understand their performance and compare their predictions. Ludwig allows its users to train a deep learning model by providing just a tabular file (like CSV) containing the data and a YAML configuration file that specifies which columns of the tabular file are input features and which are output target variables. The simplicity of the configuration file enables faster prototyping, potentially reducing hours of coding down to a few minutes. If more than one output target variable is specified, Ludwig will perform multi-task learning, learning to predict all the outputs simultaneously, a task that usually requires custom code. The model definition can contain additional information, in particular preprocessing information for each feature in the dataset, which encoder or decoder to use for each feature, architectural parameters for each encoder and decoder, and training parameters. Default values of preprocessing, training, and various model architecture parameters are chosen based on our experience or are adapted from the academic literature, allowing novices to easily train complex models. At the same time, the ability to set each of them individually in the model configuration file offers full flexibility to experts. Each model trained with Ludwig is saved and can be loaded at a later time to obtain predictions on new data. As an example, models can be loaded in a serving environment to provide predictions in software applications. Figure 1: Several input and output features may be specified in Ludwigs model description file, and their combination covers many machine learning tasks. The main new idea that Ludwig introduces is the notion of data type-specific encoders and decoders, which results in a highly modularized and extensible architecture: each type of data supported (text, images, categories, and so on) has a specific preprocessing function. In short, encoders map the raw data to tensors, and decoders map tensors to the raw data. With this design, the user has access to combiners (glue components of the architecture) that combine the tensors from all input encoders, process them, and return the tensors to be used for the output decoders. For instance, Ludwigs default concat combiner concatenates the outputs of different encoders, passes them through fully connected layers, and provides the final activation as input for output decoders. Other combiners are available for other use cases, and many more can be easily added by implementing a simple function interface. By composing these data type-specific components, users can make Ludwig train models on a wide variety of tasks. For example, by combining a text encoder and a category decoder, the user can obtain a text classifier, while combining an image encoder and a text decoder will enable the user to obtain an image captioning model. Each data type may have more than one encoder and decoder. For instance, text can be encoded with a convolutional neural network (CNN), a recurrent neural network (RNN), or other encoders. The user can then specify which one to use and its hyperparameters directly in the model definition file without having to write a single line of code. This versatile and flexible encoder-decoder architecture makes it easy for less experienced deep learning practitioners to train models for diverse machine learning tasks, such as text classification, object classification, image captioning, sequence tagging, regression, language modeling, machine translation, time series forecasting, and question answering. This opens up a variety of use cases that would typically be out of reach forinexperienced practitioners, and allows users experienced in one domain to approach new domains. At the moment, Ludwig contains encoders and decoders for binary values, float numbers, categories, discrete sequences, sets, bags, images, text, and time series, together with the capability to load some pre-trained models (for instance word embeddings), but we plan to expand the supported data types in future releases. In addition to its accessibility and flexible architecture, Ludwig also offers additional benefits for non-programmers. Ludwig incorporates a set of command line utilities for training, testing models, and obtaining predictions. Furthering its ease-of-use, the toolbox provides a programmatic API that allows users to train and use a model with just a couple lines of code. Additionally, it includes a suite of other tools for evaluating models, comparing their performance and predictions through visualizations and extracting both model weights and activations from them. Finally, the ability to train models on multiple GPUs locally and in a distributed fashion through the use of Horovod, an open source distributed training framework, makes it possible to iterate on models and obtain results quickly. To better understand how to use Ludwig for real-world applications, lets build a simple model with the toolbox. In this example, we create a model that predicts a books genre and price given its title, author, description, and cover. Our book dataset looks like the following:titleauthordescriptioncovergenrepriceDo Androids Dream of Electric Sheep? Philip K. DickBy 2021, the World War has killed millions, driving entire species into extinction and sending mankind off-planet. path-to-image/do-android-cover. jpgsci-fi9. 32War and PeaceLeo TolstoyWar and Peace broadly focuses on Napoleons invasion of Russia in 1812 and follows three of the most well-known characters in literaturepath-to-image/war-and-peace-cover. jpghistorical5. 42The Name of the RoseUmberto EcoIn 1327, Brother William of Baskerville is sent to investigate a wealthy Italian abbey whose monks are suspected of heresy. ..path-to-image/name-of-the-rose-cover. jpghistorical16. 99In order to learn a model that uses the content of the title, author, description, and cover columns as inputs to predict the values in the genre and price columns, the model definition YAML would be:input_features:  name: title type: text  name: author type: category  name: description type: text  name: cover type: imageoutput_features:  name: genre type: category  name: price type: numericaltraining: epochs: 10We start the training by typing the following command in our console:ludwig train data_csv path/to/file. csv model_definition_file model_definition. yamlWith this command, Ludwig performs a random split of the data in training, validation, and test sets, preprocess them, and builds four different encoders for the four inputs and one combiner and two decoders for the two output targets. Then, it trains the model on the training set until the accuracy on the validation set stops improving or the maximum number of ten epochs is reached. Training progress will be displayed in the console, but TensorBoard can also be used. Text features are encoded by default with a CNN encoder, but we could use, say, an RNN encoder that uses a a bidirectional LSTM with a state size of 200 for encoding the title instead. We would only need to change the title encoder definition to:name: titletype: textencoder: rnncell_type: lstmbidirectional: trueIf we wanted to change training parameters like number of epochs, learning rate, and batch size, we would change the model definition like this:input_features:  output_features:  training: epochs: 100 learning_rate: 0.001 batch_size: 64All parameters on how to perform the split and data preprocessing, the parameters of each encoder combiner and decoder have default values, but they are configurable. Refer to the user guide to discover the wide variety of model definitions and training parameters available, and take a look at our examples to see how Ludwig can be used for several different tasks. After training, Ludwig creates a result directory containing the trained model with its hyperparameters and summary statistics of the training process. We can visualize them using one of the several visualization options available with the visualize tool, for instance:ludwig visualize visualization learning_curves training_stats results/training_stats. jsonThis will display a graph that looks like the following, showing the loss and accuracy as functions of train epoch number:Figure 2: These learning curves show loss and accuracy over training epochs. Several visualizations are available. The visualization section in the user guideoffers more details. Users with new data who want their previously trained models to predict target output values can type the following command:ludwig predict data_csv path/to/data. csv model_path /path/to/modelIf a dataset contains ground truth information to compare with predictions, running this command returns model predictions and also some test performance statistics. These can be visualized via the visualize command (above), which can also be used to compare the performance and results prediction of different models. For instance:ludwig visualize visualization compare_performance test_stats path/to/test_stats_model_1. json path/to/test_stats_model_2. jsonwill return a bar plot comparing the models on different measures:Figure 3: This bar chart compares the performance of two models. There is also a handi experiment command that performs first training and then prediction without the need to use two separate command. Ludwig also provides a simple Python programmatic API that lets users train or load a model and use it to obtain predictions on new data:from ludwig import LudwigModel# train a modelmodel_definition = {}model = LudwigModel(model_definition)train_stats = model. train(training_dataframe)# or load a modelmodel = LudwigModel. load(model_path)# obtain predictionspredictions = model. predict(test_dataframe)model. close()This API enables using models trained with Ludwig inside existing code to build applications on top of them. More details on using a programmatic API with Ludwig are provided in the user guide and in the API documentation. We decided to open source Ludwig because we believe that it can be a useful tool for non-expert machine learning practitioners and experienced deep learning developers and researchers alike. The non-experts can quickly train and test deep learning models without having to write code. Experts can obtain strong baselines to compare their models against and have an experimentation setting that makes it easy to test new ideas and analyze models by performing standard data preprocessing and visualization. In future releases, we hope to add several new encoders for each data type, such as Transformer, ELMo, and BERT for text, and DenseNet and FractalNet for images. We also want to add additional data types like audio, point clouds, and graphs, while at the same time integrating more scalable solutions for managing big data sets, like Petastorm. Ludwig is built with extensibility principles in mind and, in order to facilitate contributions from the community, we provide a developer guide that showcases how simple it is to add additional data types as well as additional encoders and decoders for already existing ones. We hope you will enjoy using our tool as much as we enjoyed building it! If building the next generation of machine learning tools interests you, consider applying for a role with Uber AI! CommentsTweetShareShareVoteReddit+1\n",
      "Sentences: 96\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "Over the last decade, deep learning models have proven highly effective at performing a wide variety of machine learning tasks in vision, speech, and language. At Uber we are using these models for a variety of tasks, including customer support , object detection , improving maps , streamlining chat communications , forecasting , and preventing fraud .Many open source libraries, including TensorFlow , PyTorch , CNTK , MXNET , and Chainer , among others, have implemented the building blocks needed to build such models, allowing for faster and less error-prone development. This, in turn, has propelled the adoption of such models both by the machine learning research community and by industry practitioners, resulting in fast progress in both architecture design and industrial solutions. At Uber AI , we decided to avoid reinventing the wheel and to develop packages built on top of the strong foundations open source libraries provide. To this end, in 2017 we released Pyro , a deep probabilistic programming language built on PyTorch , and continued to improve it with the help of the open source community. Another major open source AI tool created by Uber is Horovod , a framework hosted by the LF Deep Learning Foundation that allows distributed training of deep learning models over multiple GPUs and several machines. Extending our commitment to making deep learning more accessible, we are releasing Ludwig , an open source, deep learning toolbox built on top of TensorFlow that allows users to train and test deep learning models without writing code. Ludwig is unique in its ability to help make deep learning easier to understand for non-experts and enable faster model improvement iteration cycles for experienced machine learning developers and researchers alike. By using Ludwig, experts and researchers can simplify the prototyping process and streamline data processing so that they can focus on developing deep learning architectures rather than data wrangling. We have been developing Ludwig internally at Uber over the past two years to streamline and simplify the use of deep learning models in applied projects, as they usually require comparisons among different architectures and fast iteration. We have witnessed its value to several of Ubers own projects, including our Customer Obsession Ticket Assistant (COTA) , information extraction from driver licenses, identification of points of interest during conversations between driver-partners and riders, food delivery time prediction, and much more. For this reason we decided to release it as open source, as we believe there is no other solution currently available with the same ease of use and flexibility. We originally designed Ludwig as a generic tool for simplifying the model development and comparison process when dealing with new applied machine learning problems. In order to do so, we drew inspiration from other machine learning software: from Weka  and MLlib , the idea of working directly with raw data and providing a certain number of pre-built models; from Caffe , the declarative nature of the definition file; and from scikit-learn , its simple programmatic API. This mix of influences makes it a pretty different tool from the usual deep learning libraries that provide tensor algebra primitives and few other utilities to code models, while at the same time making it more general than other specialized libraries like PyText , StanfordNLP , AllenNLP , and OpenCV .Ludwig provides a set of model architectures that can be combined together to create an end-to-end model for a given use case. As an analogy, if deep learning libraries provide the building blocks to make your building, Ludwig provides the buildings to make your city, and you can chose among the available buildings or add your own building to the set of available ones. The core design principles we baked into the toolbox are: No coding required : no coding skills are required to train a model and use it for obtaining predictions. Generality : a new data type-based approach to deep learning model design that makes the tool usable across many different use cases. Flexibility : experienced users have extensive control over model building and training, while newcomers will find it easy to use. Extensibility : easy to add new model architecture and new feature data types. Understandability : deep learning model internals are often considered black boxes, but we provide standard visualizations to understand their performance and compare their predictions. Ludwig allows its users to train a deep learning model by providing just a tabular file (like CSV) containing the data and a YAML configuration file that specifies which columns of the tabular file are input features and which are output target variables. The simplicity of the configuration file enables faster prototyping, potentially reducing hours of coding down to a few minutes. If more than one output target variable is specified, Ludwig will perform multi-task learning, learning to predict all the outputs simultaneously, a task that usually requires custom code. The model definition can contain additional information, in particular preprocessing information for each feature in the dataset , which encoder or decoder to use for each feature, architectural parameters for each encoder and decoder, and training parameters. Default values of preprocessing, training, and various model architecture parameters are chosen based on our experience or are adapted from the academic literature, allowing novices to easily train complex models. At the same time, the ability to set each of them individually in the model configuration file offers full flexibility to experts. Each model trained with Ludwig is saved and can be loaded at a later time to obtain predictions on new data. As an example, models can be loaded in a serving environment to provide predictions in software applications. Figure 1: Several input and output features may be specified in Ludwigs model description file, and their combination covers many machine learning tasks. The main new idea that Ludwig introduces is the notion of data type-specific encoders and decoders, which results in a highly modularized and extensible architecture : each type of data supported (text, images, categories, and so on) has a specific preprocessing function. In short, encoders map the raw data to tensors, and decoders map tensors to the raw data. With this design, the user has access to combiners (glue components of the architecture) that combine the tensors from all input encoders, process them, and return the tensors to be used for the output decoders. For instance, Ludwigs default concat combiner concatenates the outputs of different encoders, passes them through fully connected layers, and provides the final activation as input for output decoders. Other combiners are available for other use cases, and many more can be easily added by implementing a simple function interface. By composing these data type-specific components, users can make Ludwig train models on a wide variety of tasks. For example, by combining a text encoder and a category decoder, the user can obtain a text classifier, while combining an image encoder and a text decoder will enable the user to obtain an image captioning model. Each data type may have more than one encoder and decoder. For instance, text can be encoded with a convolutional neural network (CNN), a recurrent neural network (RNN), or other encoders. The user can then specify which one to use and its hyperparameters directly in the model definition file without having to write a single line of code. This versatile and flexible encoder-decoder architecture makes it easy for less experienced deep learning practitioners to train models for diverse machine learning tasks, such as text classification, object classification, image captioning, sequence tagging, regression, language modeling, machine translation, time series forecasting, and question answering. This opens up a variety of use cases that would typically be out of reach forinexperienced practitioners, and allows users experienced in one domain to approach new domains. At the moment, Ludwig contains encoders and decoders for binary values, float numbers, categories, discrete sequences, sets, bags, images, text, and time series, together with the capability to load some pre-trained models (for instance word embeddings), but we plan to expand the supported data types in future releases. In addition to its accessibility and flexible architecture, Ludwig also offers additional benefits for non-programmers. Ludwig incorporates a set of command line utilities for training, testing models, and obtaining predictions. Furthering its ease-of-use, the toolbox provides a programmatic API that allows users to train and use a model with just a couple lines of code. Additionally, it includes a suite of other tools for evaluating models, comparing their performance and predictions through visualizations and extracting both model weights and activations from them. Finally, the ability to train models on multiple GPUs locally and in a distributed fashion through the use of Horovod , an open source distributed training framework, makes it possible to iterate on models and obtain results quickly. Using LudwigTo better understand how to use Ludwig for real-world applications, lets build a simple model with the toolbox. In this example, we create a model that predicts a books genre and price given its title, author, description, and cover. Training the modelOur book dataset looks like the following:title author description cover genre price Do Androids Dream of Electric Sheep ? Philip K. Dick By 2021, the World War has killed millions, driving entire species into extinction and sending mankind off-planet. path-to-image/do-android-cover. jpg sci-fi 9.32 War and Peace Leo Tolstoy War and Peace broadly focuses on Napoleons invasion of Russia in 1812 and follows three of the most well-known characters in literature path-to-image/war-and-peace-cover. jpg historical 5.42 The Name of the Rose Umberto Eco In 1327, Brother William of Baskerville is sent to investigate a wealthy Italian abbey whose monks are suspected of heresy. .. path-to-image/name-of-the-rose-cover. jpg historical 16. 99      In order to learn a model that uses the content of the title, author, description, and cover columns as inputs to predict the values in the genre and price columns, the model definition YAML would be:input_features:  name: title type: text  name: author type: category  name: description type: text  name: cover type: image output_features:  name: genre type: category  name: price type: numerical training: epochs: 10We start the training by typing the following command in our console:ludwig train data_csv path/to/file. csv model_definition_file model_definition. yamlWith this command, Ludwig performs a random split of the data in training, validation, and test sets, preprocess them, and builds four different encoders for the four inputs and one combiner and two decoders for the two output targets. Then, it trains the model on the training set until the accuracy on the validation set stops improving or the maximum number of ten epochs is reached. Training progress will be displayed in the console, but TensorBoard can also be used. Text features are encoded by default with a CNN encoder, but we could use, say, an RNN encoder that uses a a bidirectional LSTM with a state size of 200 for encoding the title instead. We would only need to change the title encoder definition to:name: title type: text encoder: rnn cell_type: lstm bidirectional: trueIf we wanted to change training parameters like number of epochs, learning rate, and batch size, we would change the model definition like this:input_features:   output_features:   training: epochs: 100 learning_rate: 0.001 batch_size: 64All parameters on how to perform the split and data preprocessing, the parameters of each encoder combiner and decoder have default values, but they are configurable. Refer to the user guide to discover the wide variety of model definitions and training parameters available, and take a look at our examples to see how Ludwig can be used for several different tasks. Visualizing training resultsAfter training, Ludwig creates a result directory containing the trained model with its hyperparameters and summary statistics of the training process. We can visualize them using one of the several visualization options available with the visualize tool, for instance:ludwig visualize visualization learning_curves training_stats results/training_stats. jsonThis will display a graph that looks like the following, showing the loss and accuracy as functions of train epoch number: Figure 2: These learning curves show loss and accuracy over training epochs. Several visualizations are available. The visualization section in the user guide offers more details. Predicting results with trained modelsUsers with new data who want their previously trained models to predict target output values can type the following command:ludwig predict data_csv path/to/data. csv model_path /path/to/modelIf a dataset contains ground truth information to compare with predictions, running this command returns model predictions and also some test performance statistics. These can be visualized via the visualize command (above), which can also be used to compare the performance and results prediction of different models. For instance:ludwig visualize visualization compare_performance test_stats path/to/test_stats_model_1. json path/to/test_stats_model_2. jsonwill return a bar plot comparing the models on different measures: Figure 3: This bar chart compares the performance of two models. There is also a handi experiment command that performs first training and then prediction without the need to use two separate command. Using Ludwigs programmatic APILudwig also provides a simple Python programmatic API that lets users train or load a model and use it to obtain predictions on new data:from ludwig import LudwigModel# train a model model_definition = {} model = LudwigModel(model_definition) train_stats = model. train(training_dataframe) # or load a model model = LudwigModel. load(model_path)# obtain predictions predictions = model. predict(test_dataframe)model. close()This API enables using models trained with Ludwig inside existing code to build applications on top of them. More details on using a programmatic API with Ludwig are provided in the user guide and in the API documentation .We decided to open source Ludwig because we believe that it can be a useful tool for non-expert machine learning practitioners and experienced deep learning developers and researchers alike. The non-experts can quickly train and test deep learning models without having to write code. Experts can obtain strong baselines to compare their models against and have an experimentation setting that makes it easy to test new ideas and analyze models by performing standard data preprocessing and visualization. In future releases, we hope to add several new encoders for each data type, such as Transformer , ELM o , and BERT for text, and DenseNet and FractalNet for images. We also want to add additional data types like audio, point clouds, and graphs, while at the same time integrating more scalable solutions for managing big data sets, like Petastorm .Ludwig is built with extensibility principles in mind and, in order to facilitate contributions from the community, we provide a developer guide that showcases how simple it is to add additional data types as well as additional encoders and decoders for already existing ones. We hope you will enjoy using our tool as much as we enjoyed building it! If building the next generation of machine learning tools interests you, consider applying for a role with Uber AI!\n",
      "Sentences: 89\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "PrintOver the last decade, deep learning models have proven highly effective at performing a wide variety of machine learning tasks in vision, speech, and language. At Uber we are using these models for a variety of tasks, including customer support , object detection , improving maps , streamlining chat communications , forecasting , and preventing fraud .Many open source libraries, including TensorFlow , PyTorch , CNTK , MXNET , and Chainer , among others, have implemented the building blocks needed to build such models, allowing for faster and less error-prone development. This, in turn, has propelled the adoption of such models both by the machine learning research community and by industry practitioners, resulting in fast progress in both architecture design and industrial solutions. At Uber AI , we decided to avoid reinventing the wheel and to develop packages built on top of the strong foundations open source libraries provide. To this end, in 2017 we released Pyro , a deep probabilistic programming language built on PyTorch , and continued to improve it with the help of the open source community. Another major open source AI tool created by Uber is Horovod , a framework hosted by the LF Deep Learning Foundation that allows distributed training of deep learning models over multiple GPUs and several machines. Extending our commitment to making deep learning more accessible, we are releasing Ludwig , an open source, deep learning toolbox built on top of TensorFlow that allows users to train and test deep learning models without writing code. Ludwig is unique in its ability to help make deep learning easier to understand for non-experts and enable faster model improvement iteration cycles for experienced machine learning developers and researchers alike. By using Ludwig, experts and researchers can simplify the prototyping process and streamline data processing so that they can focus on developing deep learning architectures rather than data wrangling. LudwigWe have been developing Ludwig internally at Uber over the past two years to streamline and simplify the use of deep learning models in applied projects, as they usually require comparisons among different architectures and fast iteration. We have witnessed its value to several of Ubers own projects, including our Customer Obsession Ticket Assistant (COTA) , information extraction from driver licenses, identification of points of interest during conversations between driver-partners and riders, food delivery time prediction, and much more. For this reason we decided to release it as open source, as we believe there is no other solution currently available with the same ease of use and flexibility. We originally designed Ludwig as a generic tool for simplifying the model development and comparison process when dealing with new applied machine learning problems. In order to do so, we drew inspiration from other machine learning software: from Weka and MLlib , the idea of working directly with raw data and providing a certain number of pre-built models; from Caffe , the declarative nature of the definition file; and from scikit-learn , its simple programmatic API. This mix of influences makes it a pretty different tool from the usual deep learning libraries that provide tensor algebra primitives and few other utilities to code models, while at the same time making it more general than other specialized libraries like PyText , StanfordNLP , AllenNLP , and OpenCV .Ludwig provides a set of model architectures that can be combined together to create an end-to-end model for a given use case. As an analogy, if deep learning libraries provide the building blocks to make your building, Ludwig provides the buildings to make your city, and you can chose among the available buildings or add your own building to the set of available ones. The core design principles we baked into the toolbox are:No coding required: no coding skills are required to train a model and use it for obtaining predictions. Generality: a new data type-based approach to deep learning model design that makes the tool usable across many different use cases. Flexibility: experienced users have extensive control over model building and training, while newcomers will find it easy to use. Extensibility: easy to add new model architecture and new feature data types. Understandability: deep learning model internals are often considered black boxes, but we provide standard visualizations to understand their performance and compare their predictions. Ludwig allows its users to train a deep learning model by providing just a tabular file (like CSV) containing the data and a YAML configuration file that specifies which columns of the tabular file are input features and which are output target variables. The simplicity of the configuration file enables faster prototyping, potentially reducing hours of coding down to a few minutes. If more than one output target variable is specified, Ludwig will perform multi-task learning, learning to predict all the outputs simultaneously, a task that usually requires custom code. The model definition can contain additional information, in particular preprocessing information for each feature in the dataset, which encoder or decoder to use for each feature, architectural parameters for each encoder and decoder, and training parameters. Default values of preprocessing, training, and various model architecture parameters are chosen based on our experience or are adapted from the academic literature, allowing novices to easily train complex models. At the same time, the ability to set each of them individually in the model configuration file offers full flexibility to experts. Each model trained with Ludwig is saved and can be loaded at a later time to obtain predictions on new data. As an example, models can be loaded in a serving environment to provide predictions in software applications. Figure 1: Several input and output features may be specified in Ludwigs model description file, and their combination covers many machine learning tasks. The main new idea that Ludwig introduces is the notion of data type-specific encoders and decoders, which results in a highly modularized and extensible architecture: each type of data supported (text, images, categories, and so on) has a specific preprocessing function. In short, encoders map the raw data to tensors, and decoders map tensors to the raw data. With this design, the user has access to combiners (glue components of the architecture) that combine the tensors from all input encoders, process them, and return the tensors to be used for the output decoders. For instance, Ludwigs default concat combiner concatenates the outputs of different encoders, passes them through fully connected layers, and provides the final activation as input for output decoders. Other combiners are available for other use cases, and many more can be easily added by implementing a simple function interface. By composing these data type-specific components, users can make Ludwig train models on a wide variety of tasks. For example, by combining a text encoder and a category decoder, the user can obtain a text classifier, while combining an image encoder and a text decoder will enable the user to obtain an image captioning model. Each data type may have more than one encoder and decoder. For instance, text can be encoded with a convolutional neural network (CNN), a recurrent neural network (RNN), or other encoders. The user can then specify which one to use and its hyperparameters directly in the model definition file without having to write a single line of code. This versatile and flexible encoder-decoder architecture makes it easy for less experienced deep learning practitioners to train models for diverse machine learning tasks, such as text classification, object classification, image captioning, sequence tagging, regression, language modeling, machine translation, time series forecasting, and question answering. This opens up a variety of use cases that would typically be out of reach forinexperienced practitioners, and allows users experienced in one domain to approach new domains. At the moment, Ludwig contains encoders and decoders for binary values, float numbers, categories, discrete sequences, sets, bags, images, text, and time series, together with the capability to load some pre-trained models (for instance word embeddings), but we plan to expand the supported data types in future releases. In addition to its accessibility and flexible architecture, Ludwig also offers additional benefits for non-programmers. Ludwig incorporates a set of command line utilities for training, testing models, and obtaining predictions. Furthering its ease-of-use, the toolbox provides a programmatic API that allows users to train and use a model with just a couple lines of code. Additionally, it includes a suite of other tools for evaluating models, comparing their performance and predictions through visualizations and extracting both model weights and activations from them. Finally, the ability to train models on multiple GPUs locally and in a distributed fashion through the use of Horovod , an open source distributed training framework, makes it possible to iterate on models and obtain results quickly. Using LudwigTo better understand how to use Ludwig for real-world applications, lets build a simple model with the toolbox. In this example, we create a model that predicts a books genre and price given its title, author, description, and cover. Training the modelOur book dataset looks like the following:title\n",
      "Sentences: 51\n",
      "\n",
      "\n",
      "Article: https://www.jasonhickel.org/blog/2019/2/3/pinker-and-global-poverty\n",
      "NodeRank:\n",
      "Dear Steven,Im writing to respond to a letter you posted regarding claims I made in the Guardian about the global poverty narrative. Im addressing you directly because I think its preferable to engaging in back-channel debates, and because Id like to invite you to respond to what follows. This is an important question and it demands serious, honest engagement. The point of my piece was that the story of global poverty is more complex than you and Gates have been willing to acknowledge, and the data do not support your narrative about neoliberal globalization. Let me elaborate on my key points here, to clear up any confusion, while also addressing your specific comments. First, the long-term poverty graph (1820-present) developed by Max Roser and recently tweeted by Bill Gates is misleading and has little empirical legitimacy. There are a few reasons for this. Real data on poverty has only been collected since 1981, by the World Bank. It is widely accepted among those who research global poverty that any data prior to 1981 is simply too sketchy to be useful, and going back to as early as 1820 is more or less meaningless. The data for 1820-1970 comes from a source (Bourguignon and Morrisson 2002) that draws on the Maddison database on world GDP. That data was never intended to assess poverty, but rather the distribution of GDP  and that for only a limited range of countries. Data for the global South is particularly thin, and there is very little that exists for prior to 1900. The data is not robust enough to draw meaningful conclusions about what was happening to peoples livelihoods during the colonial period. It is important to recognise that the graph mixes two very different measures. The measure for 1820-1970 is based on estimates of GDP per capita, with only rough guesses about household share, and takes little if any account of the goods and resources that people may have acquired from their land, from trees, from forests, from rivers and the sea, and in the form of gifts from relatives. We might try to speculate about the share of GDP that the poorest people had, but thats very different from telling us anything very useful about poverty. By contrast, the World Banks measure for 1981ff is based on surveys that seek to assess household income and, wherever possible, consumption of all non-monetary goods. These two disparate measures cannot be united into a single long-term trend, and cannot be used to draw confident conclusions. Rosers graph might make for nice social media, but its not rooted in science. In fact, uniting the two methodologies is misleading in both directions. (1) By using GDP per capita from 1820-1970 it likely understates the resources that households had at their disposal in comparison to the representation of the later period, and (2) By including total consumption from 1981ff it likely overstates peoples income in comparison to the representation of the earlier period. The only way to construct a legitimate long-term graph would be to use a single consistent indicator. While data on GDP per capita alone is not regarded as a robust way of assessing poverty, it is at least available (if too patchy to be useful) for the whole period. But in such a graph the falloff in poverty since 1981 would not be nearly as steep, as it would not count non-monetary transactions. Alternatively, we could wait until someone devises a reasonable method for measuring poverty in terms of household consumption since 1820. But in the meantime, I think its wise to refrain from making claims about long-term poverty trends that lack empirical validity. You say: Hickels picture of the past is a romantic fairy tale, devoid of citations or evidence. On the contrary, as the above makes clear, it is the graph of the past on which you so glibly rely that is devoid of meaningful evidence. As to my actual claims about the past, my argument was straightforward. I simply pointed out that we cannot ignore the fact that the period 1820 to circa 1950 was one of violent dispossession across much of the global South. If you have read any colonial history, you will know colonizers had immense difficulty getting people to work on their mines and plantations. As it turns out, people tended to prefer their subsistence lifestyles, and wages were not high enough to induce them to leave. Colonizers had to coerce people into the labour market: imposing taxes, enclosing commons and constraining access to food, or just outright forcing people off their land. You ask for citations. Here are some you might try: Sven Beckerts Empire of Cotton, Ellen Woods The Origins of Capitalism: A Longer View, Mike Davis Late Victorian Holocausts, Adam Hochschilds King Leopolds Ghost, and of course Karl Polanyis The Great Transformation. The process of forcibly integrating colonized peoples into the capitalist labour system caused widespread dislocation (a history I cover in The Divide). Remember, this is the period of the Belgian labour system in the Congo, which so upended local economies that 10 million people died  half the population. This is the period of the Natives Land Act in South Africa, which dispossessed the countrys black population of 90% of the country. This is the period of the famines in India, where 30 million died needlessly as a result of policies the British imposed on Indian agriculture. This is the period of the Opium Wars in China and the unequal treaties that immiserated the population. And dont forget: all of this was conducted in the name of the free market. All of this violence, and much more, gets elided in your narrative and repackaged as a happy story of progress. And you say Im the one possessed of romantic fairy tales. The Maddison database on which you rely might tell us what the dispossessed gained in GDP per capita (eventually), but it does not tell us whether those gains offset their loss of lands, commons, supportive communities, stable local economies. And it tells us nothing about what global South economies might be like today had they been free to industrialize on their own terms (take the case of India, for instance). Let me be clear: this is not a critique of industrialization as such. It is a critique of how industrialization was carried out during the period in question. If people had willingly opted into the capitalist labour system, while retaining rights to their commons and while gaining a fair share of the yields they produced, we would have a very different story on our hands. So lets celebrate what industrialization has achieved  absolutely  but place it in proper context: colonization, violence, dispossession and all. All we gain from ignoring this history is ignorance. Now, to the present period. You say that the massive fall of global extreme poverty is simply a neutral fact of the data. But here again the data on this is more complex than you have ever acknowledged (I collaborated with Charles Kenny to review the basics here). The narrative that you and Gates peddle relies on a poverty line of $1.90 per day. You are aware, Im sure, that this line is not a neutral phenomenon, handed down by the gods or given in nature. It was invented by people, is used for particular ends, and is hotly contested both inside and outside of academia. Most scholars regard $1.90 as far too low to be meaningful, for reasons I have outlined in my work many times (see here and here). See Reddy and Lahotis withering critique of the $1.90 methodology here. Here are a few points to keep in mind. Using the $1.90 line shows that only 700 million people live in poverty. But note that the UNs FAO says that 815 million people do not have enough calories to sustain even minimal human activity. 1.5 billion are food insecure, and do not have enough calories to sustain normal human activity. And 2.1 billion suffer from malnutrition. How can there be fewer poor people than hungry and malnourished people? If $1.90 is inadequate to achieve basic nutrition and sustain normal human activity, then its too low  period. Its time for you and Gates to stop using it. Lifting people above this line doesnt mean lifting them out of poverty, extreme or otherwise. Remember: $1.90 is the equivalent of what that amount of money could buy in the US in 2011. The economist David Woodward once calculated that to live at this level (in an earlier base year) would be like 35 people trying to survive in Britain on a single minimum wage, with no benefits of any kind, no gifts, borrowing, scavenging, begging or savings to draw on (since these are all included as income in poverty calculations). That goes beyond any definition of extreme. It is patently absurd. It is an insult to humanity. In fact, even the World Bank has repeatedly stated that the line is too low to be used in any but the poorest countries, and should not be used to inform policy. In response to the Atkinson Report on Global Poverty, they created updated poverty lines for lower middle income ($3.20/day) and upper middle income ($5.50/day) countries. At those lines, some 2.4 billion people are in poverty today  more than three times higher than you would have people believe. But even these figures are not good enough. The USDA states that about $6.7/day is necessary for achieving basic nutrition. Peter Edwards argues that people need about $7.40 if they are to achieve normal human life expectancy. The New Economics Foundation concludes that around $8 is necessary to reduce infant mortality by a meaningful margin. Lant Pritchett and Charles Kenny have argued that since the poverty line is based on purchasing power in the US, then it should be linked to the US poverty line  so around $15/day. The literature on this issue is now vast and nuanced  I have only scratched the surface here  and yet you pretend it doesnt even exist. That is intellectually irresponsible, and an inadequate approach to scholarship. You say: The level at which one sets an arbitrary cutoff like the poverty line is irrelevant  the entire distribution has shifted, so the trend is the same wherever you set it. Not so fast. In fact, the story changes quite a bit - and you know it. If we use $7.40 per day, we see a decline in the proportion of people living in poverty, but its not nearly as dramatic as your rosy narrative would have it. In 1981 a staggering 71% lived in poverty. Today it hovers at 58% (for 2013, the most recent data). Suddenly your grand story of progress seems tepid, mediocre, and  in a world thats as fabulously rich as ours  completely obscene. There is nothing worth celebrating about a world where inequality is so extreme that 58% of people are in poverty, while a few dozen billionaires have more than all of their wealth combined. Thats proportions. Dont get me wrong: proportions are an important indicator  and we should pay attention to it. But absolute numbers are equally important. In fact, that is the metric that the worlds governments first agreed to target in the Rome Declaration in 1996, the precursor to the Millennial Development Goals. The goalposts were shifted to proportions in the following years, which created the impression of faster progress. But really now its a moot point: if the goal is to end poverty, what matters is absolute numbers. Certainly thats what matters from the perspective of poor people themselves. And if we look at absolute numbers, the trend changes completely. The poverty rate has worsened dramatically since 1981, from 3.2 billion to 4.2 billion, according to World Bank data. Six times higher than you would have people believe. Thats not progress in my book  thats a disgrace. It is a crushing indictment of our global economic system, which is clearly failing the majority of humanity. Your claims about global poverty intentionally skate around this fact. Again, that is not responsible scholarship. But whats really at stake here for you, as your letter reveals, is the free-market narrative that you have constructed. Your argument is that neoliberal capitalism is responsible for driving the most substantial gains against poverty. This claim is intellectually dishonest, and unsupported by facts. Heres why:The vast majority of gains against poverty have happened in one region: East Asia. As it happens, the economic success of China and the East Asian tigers  as scholars like Ha-Joon Chang and Robert Wade have long pointed out  is due not to the neoliberal markets that you espouse but rather state-led industrial policy, protectionism and regulation (the same measures that Western nations used to such great effect during their own period of industrial consolidation). They liberalized, to be sure  but they did so gradually and on their own terms. Not so for the rest of the global South. Indeed, these policy options were systematically denied to them, and destroyed where they already existed. From 1980 to 2000, the IMF and World Bank imposed brutal structural adjustment programs that did exactly the opposite: slashing tariffs, subsidies, social spending and capital controls while reversing land reforms and privatizing public assets  all in the face of massive public resistance. During this period, the number of people in poverty outside China increased by 1.3 billion. In fact, even the proportion of people living in poverty (to use your preferred method) increased, from 62% to 68%. (For detailed economic data and references to the relevant literature, see Chapter 5 of The Divide). In other words, the imposition of neoliberal capitalism from 1980 to 2000 made the poverty rate worse, not better. Since 2000, the most impressive gains against poverty (outside of East Asia) have come from Latin America, according to the World Bank, coinciding with a series of left-wing or social democratic governments that came to power across the continent. Whatever one might say about these governments (I have my own critiques), this doesnt sit very well with your neoliberal narrative. But there is something else that needs to be said here. You and Gates like to invoke the poverty numbers to make claims about the legitimacy of the existing global economic system. You say the system is working for the poor, so people should stop complaining about it. When it comes to assessing such a claim, its really neither absolute numbers nor proportions that matter. What matters, rather, is the extent of global poverty vis-à-vis our capacity to end it. As I have pointed out before, our capacity to end poverty (e.g., the cost of ending poverty as a proportion of the income of the non-poor) has increased many times faster than the proportional poverty rate has decreased (to use your preferred measure again). By this metric we are doing worse than ever before. Indeed, our civilization is regressing. Why? Because the vast majority of the yields of our global economy are being captured by the worlds rich. As I pointed out in the Guardian piece, only 5% of new income from global growth goes to the poorest 60% of humanity  people living on less than $7.40/day. You have neither acknowledged this as a problem nor attempted to defend it. Instead you just ignore it, I suppose because it undermines your claims about how well the economy is working for poor people. Heres how well its working: on our existing trajectory, according to research published in the World Economic Review, it will take more than 100 years to end poverty at $1.90/day, and over 200 years to end it at $7.4/day. Let that sink in. And to get there with the existing system  in other words, without a fairer distribution of income  we will have to grow the global economy to 175 times its present size. Even if such an outlandish feat were possible, it would drive climate change and ecological breakdown to the point of undermining any gains against poverty. It doesnt have to be this way, of course. We can end poverty right now simply by making the rules of our global economy fairer for the worlds majority (I describe how we can do this in The Divide, looking at everything from wages to debt to trade). But that is an approach that you and Gates seem desperate to avoid, in favour of a blustering defense of the status quo. You say, The drastic decline in extreme poverty is corroborated by measures of well-being other than income that are correlated with prosperity, such as longevity, child mortality, maternal mortality, literacy, basic education, undernourishment, consumption, etc. Yes, life expectancy, mortality and education have improved  this is fantastic news that we should celebrate! But, a few things:(1) You cant make an argument about poverty by pointing to something else entirely. Consumption is increasing, yes. But thats not whats at stake here. Whats at stake is whether consumption is increasing enough to raise people out of poverty. (2) Ill be the first to agree that income and consumption are not the only measures of well-being. But one reason they are absolutely crucial is because they allow us to assess inequality in the distribution of world resources. A higher life expectancy among the poor is no justification for condemning them to a tiny and ever-shrinking share of global income. That is not a morally defensible position. (3) In your work you have invoked gains in life expectancy and education as part of a narrative that seeks to justify neoliberal globalization. But here again thats intellectually dishonest. What contributes most to improvements in life expectancy is in fact simple public health interventions (sanitation, antibiotics, vaccines), and what matters for education is, well, public education. Indeed, the countries that have been most successful at this are those that have robust, free healthcare and education. Dont forget that the US has worse infant mortality than Cuba. (4) As for hunger, your claim here relies on a methodology used by the FAO after 2012 that has been widely criticized by scholars. The hunger-reduction narrative depends on a calorie line that  like your $1.90 poverty line  is too low to support normal human activity, ignores the impacts of food price crises, and tells us nothing about nutrient deficiencies. I cover this in detail in the second half of this paper. According to the FAOs earlier methodology, both the number and proportion of people in hunger was higher in 2009 than in 1995  another trend that you glibly ignore. In your concluding point, you descend to citing a piece by Ryan Bourne, not an academic who studies poverty but rather an employee of the Cato Institute, a right-wing think tank funded by the Koch Brothers. The piece is riddled with misleading claims which, when I pointed them out to him, he never corrected. I dont think we should consider this a valid source. You opened your letter by slandering me as a Marxist ideologue. I dont need to tell you that this doesnt count as an argument, and doesnt cover for the fact that you havent addressed any of my substantive claims. In any case, Im not quite sure what you mean. If by Marxist ideologue you mean someone who points out that the poverty data is more complex than your narrative allows, then, well, I suppose I am.\n",
      "Sentences: 165\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "Dear Steven,Im writing to respond to a letter you posted regarding claims I made in the Guardian about the global poverty narrative. Im addressing you directly because I think its preferable to engaging in back-channel debates, and because Id like to invite you to respond to what follows. This is an important question and it demands serious, honest engagement. The point of my piece was that the story of global poverty is more complex than you and Gates have been willing to acknowledge, and the data do not support your narrative about neoliberal globalization. Let me elaborate on my key points here, to clear up any confusion, while also addressing your specific comments. First, the long-term poverty graph (1820-present) developed by Max Roser and recently tweeted by Bill Gates is misleading and has little empirical legitimacy. There are a few reasons for this. Real data on poverty has only been collected since 1981, by the World Bank. It is widely accepted among those who research global poverty that any data prior to 1981 is simply too sketchy to be useful, and going back to as early as 1820 is more or less meaningless. The data for 1820-1970 comes from a source (Bourguignon and Morrisson 2002) that draws on the Maddison database on world GDP. That data was never intended to assess poverty, but rather the distribution of GDP  and that for only a limited range of countries. Data for the global South is particularly thin, and there is very little that exists for prior to 1900. The data is not robust enough to draw meaningful conclusions about what was happening to peoples livelihoods during the colonial period. It is important to recognise that the graph mixes two very different measures. The measure for 1820-1970 is based on estimates of GDP per capita, with only rough guesses about household share, and takes little if any account of the goods and resources that people may have acquired from their land, from trees, from forests, from rivers and the sea, and in the form of gifts from relatives. We might try to speculate about the share of GDP that the poorest people had, but thats very different from telling us anything very useful about poverty. By contrast, the World Banks measure for 1981ff is based on surveys that seek to assess household income and, wherever possible, consumption of all non-monetary goods. These two disparate measures cannot be united into a single long-term trend, and cannot be used to draw confident conclusions. Rosers graph might make for nice social media, but its not rooted in science. In fact, uniting the two methodologies is misleading in both directions. (1) By using GDP per capita from 1820-1970 it likely understates the resources that households had at their disposal in comparison to the representation of the later period, and (2) By including total consumption from 1981ff it likely overstates peoples income in comparison to the representation of the earlier period. The only way to construct a legitimate long-term graph would be to use a single consistent indicator. While data on GDP per capita alone is not regarded as a robust way of assessing poverty, it is at least available (if too patchy to be useful) for the whole period. But in such a graph the falloff in poverty since 1981 would not be nearly as steep, as it would not count non-monetary transactions. Alternatively, we could wait until someone devises a reasonable method for measuring poverty in terms of household consumption since 1820. But in the meantime, I think its wise to refrain from making claims about long-term poverty trends that lack empirical validity. You say: Hickels picture of the past is a romantic fairy tale, devoid of citations or evidence. On the contrary, as the above makes clear, it is the graph of the past on which you so glibly rely that is devoid of meaningful evidence. As to my actual claims about the past, my argument was straightforward. I simply pointed out that we cannot ignore the fact that the period 1820 to circa 1950 was one of violent dispossession across much of the global South. If you have read any colonial history, you will know colonizers had immense difficulty getting people to work on their mines and plantations. As it turns out, people tended to prefer their subsistence lifestyles, and wages were not high enough to induce them to leave. Colonizers had to coerce people into the labour market: imposing taxes, enclosing commons and constraining access to food, or just outright forcing people off their land. You ask for citations. Here are some you might try: Sven Beckerts Empire of Cotton , Ellen Woods The Origins of Capitalism: A Longer View , Mike Davis Late Victorian Holocausts , Adam Hochschilds King Leopolds Ghost , and of course Karl Polanyis The Great Transformation .The process of forcibly integrating colonized peoples into the capitalist labour system caused widespread dislocation (a history I cover in The Divide ). Remember, this is the period of the Belgian labour system in the Congo, which so upended local economies that 10 million people died  half the population. This is the period of the Natives Land Act in South Africa, which dispossessed the countrys black population of 90% of the country. This is the period of the famines in India, where 30 million died needlessly as a result of policies the British imposed on Indian agriculture. This is the period of the Opium Wars in China and the unequal treaties that immiserated the population. And dont forget: all of this was conducted in the name of the free market. All of this violence, and much more, gets elided in your narrative and repackaged as a happy story of progress. And you say Im the one possessed of romantic fairy tales. The Maddison database on which you rely might tell us what the dispossessed gained in GDP per capita (eventually), but it does not tell us whether those gains offset their loss of lands, commons, supportive communities, stable local economies. And it tells us nothing about what global South economies might be like today had they been free to industrialize on their own terms (take the case of India , for instance).Let me be clear: this is not a critique of industrialization as such . It is a critique of how industrialization was carried out during the period in question. If people had willingly opted into the capitalist labour system, while retaining rights to their commons and while gaining a fair share of the yields they produced, we would have a very different story on our hands. So lets celebrate what industrialization has achieved  absolutely  but place it in proper context: colonization, violence, dispossession and all. All we gain from ignoring this history is ignorance. Now, to the present period. You say that the massive fall of global extreme poverty is simply a neutral fact of the data. But here again the data on this is more complex than you have ever acknowledged (I collaborated with Charles Kenny to review the basics here ).The narrative that you and Gates peddle relies on a poverty line of $1.90 per day. You are aware, Im sure, that this line is not a neutral phenomenon, handed down by the gods or given in nature. It was invented by people, is used for particular ends, and is hotly contested both inside and outside of academia. Most scholars regard $1.90 as far too low to be meaningful, for reasons I have outlined in my work many times (see here and here ). See Reddy and Lahotis withering critique of the $1.90 methodology here .Here are a few points to keep in mind. Using the $1.90 line shows that only 700 million people live in poverty. But note that the UNs FAO says that 815 million people do not have enough calories to sustain even minimal human activity. 1.5 billion are food insecure, and do not have enough calories to sustain normal human activity. And 2.1 billion suffer from malnutrition. How can there be fewer poor people than hungry and malnourished people? If $1.90 is inadequate to achieve basic nutrition and sustain normal human activity, then its too low  period. Its time for you and Gates to stop using it. Lifting people above this line doesnt mean lifting them out of poverty, extreme or otherwise. Remember: $1.90 is the equivalent of what that amount of money could buy in the US in 2011. The economist David Woodward once calculated that to live at this level (in an earlier base year) would be like 35 people trying to survive in Britain on a single minimum wage, with no benefits of any kind, no gifts, borrowing, scavenging, begging or savings to draw on (since these are all included as income in poverty calculations). That goes beyond any definition of extreme. It is patently absurd. It is an insult to humanity. In fact, even the World Bank has repeatedly stated that the line is too low to be used in any but the poorest countries, and should not be used to inform policy . In response to the Atkinson Report on Global Poverty, they created updated poverty lines for lower middle income ($3.20/day) and upper middle income ($5.50/day) countries. At those lines, some 2.4 billion people are in poverty today  more than three times higher than you would have people believe. But even these figures are not good enough. The USDA states that about $6.7/day is necessary for achieving basic nutrition. Peter Edwards argues that people need about $7.40 if they are to achieve normal human life expectancy. The New Economics Foundation concludes that around $8 is necessary to reduce infant mortality by a meaningful margin. Lant Pritchett and Charles Kenny have argued that since the poverty line is based on purchasing power in the US, then it should be linked to the US poverty line  so around $15/day. The literature on this issue is now vast and nuanced  I have only scratched the surface here  and yet you pretend it doesnt even exist. That is intellectually irresponsible, and an inadequate approach to scholarship. You say: The level at which one sets an arbitrary cutoff like the poverty line is irrelevant  the entire distribution has shifted, so the trend is the same wherever you set it. Not so fast. In fact, the story changes quite a bit - and you know it. If we use $7.40 per day, we see a decline in the proportion of people living in poverty, but its not nearly as dramatic as your rosy narrative would have it. In 1981 a staggering 71% lived in poverty. Today it hovers at 58% (for 2013, the most recent data). Suddenly your grand story of progress seems tepid, mediocre, and  in a world thats as fabulously rich as ours  completely obscene. There is nothing worth celebrating about a world where inequality is so extreme that 58% of people are in poverty, while a few dozen billionaires have more than all of their wealth combined. Thats proportions. Dont get me wrong: proportions are an important indicator  and we should pay attention to it. But absolute numbers are equally important. In fact, that is the metric that the worlds governments first agreed to target in the Rome Declaration in 1996, the precursor to the Millennial Development Goals. The goalposts were shifted to proportions in the following years, which created the impression of faster progress. But really now its a moot point: if the goal is to end poverty, what matters is absolute numbers. Certainly thats what matters from the perspective of poor people themselves. And if we look at absolute numbers, the trend changes completely. The poverty rate has worsened dramatically since 1981, from 3.2 billion to 4.2 billion, according to World Bank data. Six times higher than you would have people believe. Thats not progress in my book  thats a disgrace. It is a crushing indictment of our global economic system, which is clearly failing the majority of humanity. Your claims about global poverty intentionally skate around this fact. Again, that is not responsible scholarship.\n",
      "Sentences: 100\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "Dear Steven,Im writing to respond to a letter you posted regarding claims I made in the Guardian about the global poverty narrative. Im addressing you directly because I think its preferable to engaging in back-channel debates, and because Id like to invite you to respond to what follows. This is an important question and it demands serious, honest engagement. The point of my piece was that the story of global poverty is more complex than you and Gates have been willing to acknowledge, and the data do not support your narrative about neoliberal globalization. Let me elaborate on my key points here, to clear up any confusion, while also addressing your specific comments. First, the long-term poverty graph (1820-present) developed by Max Roser and recently tweeted by Bill Gates is misleading and has little empirical legitimacy. There are a few reasons for this. Real data on poverty has only been collected since 1981, by the World Bank. It is widely accepted among those who research global poverty that any data prior to 1981 is simply too sketchy to be useful, and going back to as early as 1820 is more or less meaningless. The data for 1820-1970 comes from a source (Bourguignon and Morrisson 2002) that draws on the Maddison database on world GDP. That data was never intended to assess poverty, but rather the distribution of GDP  and that for only a limited range of countries. Data for the global South is particularly thin, and there is very little that exists for prior to 1900. The data is not robust enough to draw meaningful conclusions about what was happening to peoples livelihoods during the colonial period. It is important to recognise that the graph mixes two very different measures. The measure for 1820-1970 is based on estimates of GDP per capita, with only rough guesses about household share, and takes little if any account of the goods and resources that people may have acquired from their land, from trees, from forests, from rivers and the sea, and in the form of gifts from relatives. We might try to speculate about the share of GDP that the poorest people had, but thats very different from telling us anything very useful about poverty. By contrast, the World Banks measure for 1981ff is based on surveys that seek to assess household income and, wherever possible, consumption of all non-monetary goods. These two disparate measures cannot be united into a single long-term trend, and cannot be used to draw confident conclusions. Rosers graph might make for nice social media, but its not rooted in science. In fact, uniting the two methodologies is misleading in both directions. (1) By using GDP per capita from 1820-1970 it likely understates the resources that households had at their disposal in comparison to the representation of the later period, and (2) By including total consumption from 1981ff it likely overstates peoples income in comparison to the representation of the earlier period. The only way to construct a legitimate long-term graph would be to use a single consistent indicator. While data on GDP per capita alone is not regarded as a robust way of assessing poverty, it is at least available (if too patchy to be useful) for the whole period. But in such a graph the falloff in poverty since 1981 would not be nearly as steep, as it would not count non-monetary transactions. Alternatively, we could wait until someone devises a reasonable method for measuring poverty in terms of household consumption since 1820. But in the meantime, I think its wise to refrain from making claims about long-term poverty trends that lack empirical validity. You say: Hickels picture of the past is a romantic fairy tale, devoid of citations or evidence. On the contrary, as the above makes clear, it is the graph of the past on which you so glibly rely that is devoid of meaningful evidence. As to my actual claims about the past, my argument was straightforward. I simply pointed out that we cannot ignore the fact that the period 1820 to circa 1950 was one of violent dispossession across much of the global South. If you have read any colonial history, you will know colonizers had immense difficulty getting people to work on their mines and plantations. As it turns out, people tended to prefer their subsistence lifestyles, and wages were not high enough to induce them to leave. Colonizers had to coerce people into the labour market: imposing taxes, enclosing commons and constraining access to food, or just outright forcing people off their land. You ask for citations. Here are some you might try: Sven Beckerts Empire of Cotton, Ellen Woods The Origins of Capitalism: A Longer View, Mike Davis Late Victorian Holocausts, Adam Hochschilds King Leopolds Ghost, and of course Karl Polanyis The Great Transformation. The process of forcibly integrating colonized peoples into the capitalist labour system caused widespread dislocation (a history I cover in The Divide). Remember, this is the period of the Belgian labour system in the Congo, which so upended local economies that 10 million people died  half the population. This is the period of the Natives Land Act in South Africa, which dispossessed the countrys black population of 90% of the country. This is the period of the famines in India, where 30 million died needlessly as a result of policies the British imposed on Indian agriculture. This is the period of the Opium Wars in China and the unequal treaties that immiserated the population. And dont forget: all of this was conducted in the name of the free market. All of this violence, and much more, gets elided in your narrative and repackaged as a happy story of progress. And you say Im the one possessed of romantic fairy tales. The Maddison database on which you rely might tell us what the dispossessed gained in GDP per capita (eventually), but it does not tell us whether those gains offset their loss of lands, commons, supportive communities, stable local economies. And it tells us nothing about what global South economies might be like today had they been free to industrialize on their own terms (take the case of India , for instance).Let me be clear: this is not a critique of industrialization as such. It is a critique of how industrialization was carried out during the period in question. If people had willingly opted into the capitalist labour system, while retaining rights to their commons and while gaining a fair share of the yields they produced, we would have a very different story on our hands. So lets celebrate what industrialization has achieved  absolutely  but place it in proper context: colonization, violence, dispossession and all. All we gain from ignoring this history is ignorance. Now, to the present period. You say that the massive fall of global extreme poverty is simply a neutral fact of the data. But here again the data on this is more complex than you have ever acknowledged (I collaborated with Charles Kenny to review the basics here ).The narrative that you and Gates peddle relies on a poverty line of $1.90 per day. You are aware, Im sure, that this line is not a neutral phenomenon, handed down by the gods or given in nature. It was invented by people, is used for particular ends, and is hotly contested both inside and outside of academia. Most scholars regard $1.90 as far too low to be meaningful, for reasons I have outlined in my work many times (see here and here ). See Reddy and Lahotis withering critique of the $1.90 methodology here .Here are a few points to keep in mind. Using the $1.90 line shows that only 700 million people live in poverty. But note that the UNs FAO says that 815 million people do not have enough calories to sustain even minimal human activity. 1.5 billion are food insecure, and do not have enough calories to sustain normal human activity. And 2.1 billion suffer from malnutrition. How can there be fewer poor people than hungry and malnourished people? If $1.90 is inadequate to achieve basic nutrition and sustain normal human activity, then its too low  period. Its time for you and Gates to stop using it. Lifting people above this line doesnt mean lifting them out of poverty, extreme or otherwise. Remember: $1.90 is the equivalent of what that amount of money could buy in the US in 2011. The economist David Woodward once calculated that to live at this level (in an earlier base year) would be like 35 people trying to survive in Britain on a single minimum wage, with no benefits of any kind, no gifts, borrowing, scavenging, begging or savings to draw on (since these are all included as income in poverty calculations). That goes beyond any definition of extreme. It is patently absurd. It is an insult to humanity. In fact, even the World Bank has repeatedly stated that the line is too low to be used in any but the poorest countries, and should not be used to inform policy . In response to the Atkinson Report on Global Poverty, they created updated poverty lines for lower middle income ($3.20/day) and upper middle income ($5.50/day) countries. At those lines, some 2.4 billion people are in poverty today  more than three times higher than you would have people believe. But even these figures are not good enough. The USDA states that about $6.7/day is necessary for achieving basic nutrition. Peter Edwards argues that people need about $7.40 if they are to achieve normal human life expectancy. The New Economics Foundation concludes that around $8 is necessary to reduce infant mortality by a meaningful margin. Lant Pritchett and Charles Kenny have argued that since the poverty line is based on purchasing power in the US, then it should be linked to the US poverty line  so around $15/day. The literature on this issue is now vast and nuanced  I have only scratched the surface here  and yet you pretend it doesnt even exist. That is intellectually irresponsible, and an inadequate approach to scholarship. You say: The level at which one sets an arbitrary cutoff like the poverty line is irrelevant  the entire distribution has shifted, so the trend is the same wherever you set it. Not so fast. In fact, the story changes quite a bit - and you know it. If we use $7.40 per day, we see a decline in the proportion of people living in poverty, but its not nearly as dramatic as your rosy narrative would have it. In 1981 a staggering 71% lived in poverty. Today it hovers at 58% (for 2013, the most recent data). Suddenly your grand story of progress seems tepid, mediocre, and  in a world thats as fabulously rich as ours  completely obscene. There is nothing worth celebrating about a world where inequality is so extreme that 58% of people are in poverty, while a few dozen billionaires have more than all of their wealth combined. Thats proportions. Dont get me wrong: proportions are an important indicator  and we should pay attention to it. But absolute numbers are equally important. In fact, that is the metric that the worlds governments first agreed to target in the Rome Declaration in 1996, the precursor to the Millennial Development Goals. The goalposts were shifted to proportions in the following years, which created the impression of faster progress. But really now its a moot point: if the goal is to end poverty, what matters is absolute numbers. Certainly thats what matters from the perspective of poor people themselves. And if we look at absolute numbers, the trend changes completely. The poverty rate has worsened dramatically since 1981, from 3.2 billion to 4.2 billion, according to World Bank data. Six times higher than you would have people believe. Thats not progress in my book  thats a disgrace. It is a crushing indictment of our global economic system, which is clearly failing the majority of humanity. Your claims about global poverty intentionally skate around this fact. Again, that is not responsible scholarship. But whats really at stake here for you, as your letter reveals, is the free-market narrative that you have constructed. Your argument is that neoliberal capitalism is responsible for driving the most substantial gains against poverty. This claim is intellectually dishonest, and unsupported by facts. Heres why:The vast majority of gains against poverty have happened in one region: East Asia. As it happens, the economic success of China and the East Asian tigers  as scholars like Ha-Joon Chang and Robert Wade have long pointed out  is due not to the neoliberal markets that you espouse but rather state-led industrial policy, protectionism and regulation (the same measures that Western nations used to such great effect during their own period of industrial consolidation). They liberalized, to be sure  but they did so gradually and on their own terms. Not so for the rest of the global South. Indeed, these policy options were systematically denied to them, and destroyed where they already existed. From 1980 to 2000, the IMF and World Bank imposed brutal structural adjustment programs that did exactly the opposite: slashing tariffs, subsidies, social spending and capital controls while reversing land reforms and privatizing public assets  all in the face of massive public resistance. During this period, the number of people in poverty outside China increased by 1.3 billion. In fact, even the proportion of people living in poverty (to use your preferred method) increased, from 62% to 68%. (For detailed economic data and references to the relevant literature, see Chapter 5 of The Divide).In other words, the imposition of neoliberal capitalism from 1980 to 2000 made the poverty rate worse, not better. Since 2000, the most impressive gains against poverty (outside of East Asia) have come from Latin America, according to the World Bank, coinciding with a series of left-wing or social democratic governments that came to power across the continent. Whatever one might say about these governments (I have my own critiques), this doesnt sit very well with your neoliberal narrative. But there is something else that needs to be said here. You and Gates like to invoke the poverty numbers to make claims about the legitimacy of the existing global economic system. You say the system is working for the poor, so people should stop complaining about it. When it comes to assessing such a claim, its really neither absolute numbers nor proportions that matter. What matters, rather, is the extent of global poverty vis-à-vis our capacity to end it. As I have pointed out before , our capacity to end poverty (e.g., the cost of ending poverty as a proportion of the income of the non-poor) has increased many times faster than the proportional poverty rate has decreased (to use your preferred measure again). By this metric we are doing worse than ever before. Indeed, our civilization is regressing. Why? Because the vast majority of the yields of our global economy are being captured by the worlds rich. As I pointed out in the Guardian piece, only 5% of new income from global growth goes to the poorest 60% of humanity  people living on less than $7.40/day. You have neither acknowledged this as a problem nor attempted to defend it. Instead you just ignore it, I suppose because it undermines your claims about how well the economy is working for poor people. Heres how well its working: on our existing trajectory, according to research published in the World Economic Review , it will take more than 100 years to end poverty at $1.90/day, and over 200 years to end it at $7.4/day. Let that sink in. And to get there with the existing system  in other words, without a fairer distribution of income  we will have to grow the global economy to 175 times its present size. Even if such an outlandish feat were possible, it would drive climate change and ecological breakdown to the point of undermining any gains against poverty. It doesnt have to be this way, of course. We can end poverty right now simply by making the rules of our global economy fairer for the worlds majority (I describe how we can do this in The Divide , looking at everything from wages to debt to trade). But that is an approach that you and Gates seem desperate to avoid, in favour of a blustering defense of the status quo. You say, The drastic decline in extreme poverty is corroborated by measures of well-being other than income that are correlated with prosperity, such as longevity, child mortality, maternal mortality, literacy, basic education, undernourishment, consumption, etc. Yes, life expectancy, mortality and education have improved  this is fantastic news that we should celebrate! But, a few things:(1) You cant make an argument about poverty by pointing to something else entirely. Consumption is increasing, yes. But thats not whats at stake here. Whats at stake is whether consumption is increasing enough to raise people out of poverty. (2) Ill be the first to agree that income and consumption are not the only measures of well-being. But one reason they are absolutely crucial is because they allow us to assess inequality in the distribution of world resources. A higher life expectancy among the poor is no justification for condemning them to a tiny and ever-shrinking share of global income. That is not a morally defensible position. (3) In your work you have invoked gains in life expectancy and education as part of a narrative that seeks to justify neoliberal globalization. But here again thats intellectually dishonest. What contributes most to improvements in life expectancy is in fact simple public health interventions (sanitation, antibiotics, vaccines), and what matters for education is, well, public education. Indeed, the countries that have been most successful at this are those that have robust, free healthcare and education. Dont forget that the US has worse infant mortality than Cuba. (4) As for hunger, your claim here relies on a methodology used by the FAO after 2012 that has been widely criticized by scholars. The hunger-reduction narrative depends on a calorie line that  like your $1.90 poverty line  is too low to support normal human activity, ignores the impacts of food price crises, and tells us nothing about nutrient deficiencies. I cover this in detail in the second half of this paper . According to the FAOs earlier methodology, both the number and proportion of people in hunger was higher in 2009 than in 1995  another trend that you glibly ignore. In your concluding point, you descend to citing a piece by Ryan Bourne, not an academic who studies poverty but rather an employee of the Cato Institute, a right-wing think tank funded by the Koch Brothers. The piece is riddled with misleading claims which, when I pointed them out to him, he never corrected. I dont think we should consider this a valid source. You opened your letter by slandering me as a Marxist ideologue. I dont need to tell you that this doesnt count as an argument, and doesnt cover for the fact that you havent addressed any of my substantive claims. In any case, Im not quite sure what you mean. If by Marxist ideologue you mean someone who points out that the poverty data is more complex than your narrative allows, then, well, I suppose I am.\n",
      "Sentences: 161\n",
      "\n",
      "\n",
      "Article: https://www.nytimes.com/2019/02/11/health/artificial-intelligence-medical-diagnosis.html\n",
      "NodeRank:\n",
      "Each year, millions of Americans walk out of a doctors office with a misdiagnosis. Physicians try to be systematic when identifying illness and disease, but bias creeps in. Alternatives are overlooked. Now a group of researchers in the United States and China has tested a potential remedy for all-too-human frailties: artificial intelligence. In a paper published on Monday in Nature Medicine, the scientists reported that they had built a system that automatically diagnoses common childhood conditions  from influenza to meningitis  after processing the patients symptoms, history, lab results and other clinical data. The system was highly accurate, the researchers said, and one day may assist doctors in diagnosing complex or rare conditions. Drawing on the records of nearly 600,000 Chinese patients who had visited a pediatric hospital over an 18-month period, the vast collection of data used to train this new system highlights an advantage for China in the worldwide race toward artificial intelligence. Because its population is so large  and because its privacy norms put fewer restrictions on the sharing of digital data  it may be easier for Chinese companies and researchers to build and train the deep learning systems that are rapidly changing the trajectory of health care. On Monday, President Trump signed an executive order meant to spur the development of A.I. across government, academia and industry in the United States. As part of this American A.I. Initiative, the administration will encourage federal agencies and universities to share data that can drive the development of automated systems. Pooling health care data is a particularly difficult endeavor. Whereas researchers went to a single Chinese hospital for all the data they needed to develop their artificial-intelligence system, gathering such data from American facilities is rarely so straightforward. You have go to multiple places, said Dr. George Shih, associate professor of clinical radiology at Weill Cornell Medical Center and co-founder of MD. ai, a company that helps researchers label data for A.I. services. The equipment is never the same. You have to make sure the data is anonymized. Even if you get permission, it is a massive amount of work. After reshaping internet services, consumer devices and driverless cars in the early part of the decade, deep learning is moving rapidly into myriad areas of health care. Many organizations, including Google, are developing and testing systems that analyze electronic health records in an effort to flag medical conditions such as osteoporosis, diabetes, hypertension and heart failure. Similar technologies are being built to automatically detect signs of illness and disease in X-rays, M.R.I.s and eye scans. The new system relies on a neural network, a breed of artificial intelligence that is accelerating the development of everything from health care to driverless cars to military applications. A neural network can learn tasks largely on its own by analyzing vast amounts of data. Using the technology, Dr. Kang Zhang, chief of ophthalmic genetics at the University of California, San Diego, has built systems that can analyze eye scans for hemorrhages, lesions and other signs of diabetic blindness. Ideally, such systems would serve as a first line of defense, screening patients and pinpointing those who need further attention. Now Dr. Zhang and his colleagues have created a system that can diagnose an even wider range of conditions by recognizing patterns in text, not just in medical images. This may augment what doctors can do on their own, he said. In some situations, physicians cannot consider all the possibilities, he said. This system can spot-check and make sure the physician didnt miss anything. The experimental system analyzed the electronic medical records of nearly 600,000 patients at the Guangzhou Women and Childrens Medical Center in southern China, learning to associate common medical conditions with specific patient information gathered by doctors, nurses and other technicians. First, a group of trained physicians annotated the hospital records, adding labels that identified information related to certain medical conditions. The system then analyzed the labeled data. Then the neural network was given new information, including a patients symptoms as determined during a physical examination. Soon it was able to make connections on its own between written records and observed symptoms. When tested on unlabeled data, the software could rival the performance of experienced physicians. It was more than 90 percent accurate at diagnosing asthma; the accuracy of physicians in the study ranged from 80 to 94 percent. In diagnosing gastrointestinal disease, the system was 87 percent accurate, compared with the physicians accuracy of 82 to 90 percent. Able to recognize patterns in data that humans could never identify on their own, neural networks can be enormously powerful in the right situation. But even experts have difficulty understanding why such networks make particular decisions and how they teach themselves. As a result, extensive testing is needed to reassure both doctors and patients that these systems are reliable. Experts said extensive clinical trials are now needed for Dr. Zhangs system, given the difficulty of interpreting decisions made by neural networks. Medicine is a slow-moving field, said Ben Shickel, a researcher at the University of Florida who specializes in the use of deep learning for health care. No one is just going to deploy one of these techniques without rigorous testing that shows exactly what is going on. It could be years before deep-learning systems are deployed in emergency rooms and clinics. But some are closer to real-world use: Google is now running clinical trials of its eye-scan system at two hospitals in southern India. Deep-learning diagnostic tools are more likely to flourish in countries outside the United States, Dr. Zhang said. Automated screening systems may be particularly useful in places where doctors are scarce, including in India and China. The system built by Dr. Zhang and his colleagues benefited from the large scale of the data set gathered from the hospital in Guangzhou. Similar data sets from American hospitals are typically smaller, both because the average hospital is smaller and because regulations make it difficult to pool data from multiple facilities. Dr. Zhang said he and his colleagues were careful to protect patients privacy in the new study. But he acknowledged that researchers in China may have an advantage when it comes to collecting and analyzing this kind of data. The sheer size of the population  the sheer size of the data  is a big difference, he said. A version of this article appears in print on , on Page B1 of the New York edition with the headline: In China, Training A.I. To Diagnose Diseases. Order Reprints | Todays Paper | SubscribeDec. 4, 2015ImageDec. 26, 2017ImageOpinionDhruv KhullarJan. 31, 2019ImageMay 16, 2018ImageAdvertisement\n",
      "Sentences: 60\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "After reshaping internet services, consumer devices and driverless cars in the early part of the decade, deep learning is moving rapidly into myriad areas of health care. Many organizations, including Google , are developing and testing systems that analyze electronic health records in an effort to flag medical conditions such as osteoporosis, diabetes, hypertension and heart failure. Similar technologies are being built to automatically detect signs of illness and disease in X-rays, M.R.I.s and eye scans. The new system relies on a neural network , a breed of artificial intelligence that is accelerating the development of everything from health care to driverless cars to military applications . A neural network can learn tasks largely on its own by analyzing vast amounts of data. Using the technology, Dr. Kang Zhang, chief of ophthalmic genetics at the University of California, San Diego, has built systems that can analyze eye scans for hemorrhages, lesions and other signs of diabetic blindness. Ideally, such systems would serve as a first line of defense, screening patients and pinpointing those who need further attention. Now Dr. Zhang and his colleagues have created a system that can diagnose an even wider range of conditions by recognizing patterns in text, not just in medical images. This may augment what doctors can do on their own, he said. In some situations, physicians cannot consider all the possibilities, he said. This system can spot-check and make sure the physician didnt miss anything. The experimental system analyzed the electronic medical records of nearly 600,000 patients at the Guangzhou Women and Childrens Medical Center in southern China, learning to associate common medical conditions with specific patient information gathered by doctors, nurses and other technicians. A version of this article appears in print on , on Page B1 of the New York edition with the headline: In China, Training A.I. To Diagnose Diseases . Order Reprints | Todays Paper | Subscribe\n",
      "Sentences: 15\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "Health |A.I. Shows Promise as a Physician AssistantSupported byA. I. Shows Promise as a Physician AssistantImageDoctors competed against A.I. computers to recognize illnesses on magnetic resonance images of a human brain during a competition in Beijing last year. The human doctors lost. CreditCreditMark Schiefelbein/Associated PressBy Cade MetzFeb. 11, 2019Each year, millions of Americans walk out of a doctors office with a misdiagnosis . Physicians try to be systematic when identifying illness and disease, but bias creeps in. Alternatives are overlooked. Now a group of researchers in the United States and China has tested a potential remedy for all-too-human frailties: artificial intelligence. In a paper published on Monday in Nature Medicine, the scientists reported that they had built a system that automatically diagnoses common childhood conditions  from influenza to meningitis  after processing the patients symptoms, history, lab results and other clinical data. The system was highly accurate, the researchers said, and one day may assist doctors in diagnosing complex or rare conditions. Drawing on the records of nearly 600,000 Chinese patients who had visited a pediatric hospital over an 18-month period, the vast collection of data used to train this new system highlights an advantage for China in the worldwide race toward artificial intelligence. Because its population is so large  and because its privacy norms put fewer restrictions on the sharing of digital data  it may be easier for Chinese companies and researchers to build and train the deep learning systems that are rapidly changing the trajectory of health care. On Monday, President Trump signed an executive order meant to spur the development of A.I. across government, academia and industry in the United States. As part of this American A.I. Initiative, the administration will encourage federal agencies and universities to share data that can drive the development of automated systems. Pooling health care data is a particularly difficult endeavor. Whereas researchers went to a single Chinese hospital for all the data they needed to develop their artificial-intelligence system, gathering such data from American facilities is rarely so straightforward. You have go to multiple places, said Dr. George Shih, associate professor of clinical radiology at Weill Cornell Medical Center and co-founder of MD. ai, a company that helps researchers label data for A.I. services. The equipment is never the same. You have to make sure the data is anonymized. Even if you get permission, it is a massive amount of work. After reshaping internet services, consumer devices and driverless cars in the early part of the decade, deep learning is moving rapidly into myriad areas of health care. Many organizations, including Google , are developing and testing systems that analyze electronic health records in an effort to flag medical conditions such as osteoporosis, diabetes, hypertension and heart failure. Similar technologies are being built to automatically detect signs of illness and disease in X-rays, M.R.I.s and eye scans. The new system relies on a neural network , a breed of artificial intelligence that is accelerating the development of everything from health care to driverless cars to military applications . A neural network can learn tasks largely on its own by analyzing vast amounts of data. Using the technology, Dr. Kang Zhang, chief of ophthalmic genetics at the University of California, San Diego, has built systems that can analyze eye scans for hemorrhages, lesions and other signs of diabetic blindness. Ideally, such systems would serve as a first line of defense, screening patients and pinpointing those who need further attention. Now Dr. Zhang and his colleagues have created a system that can diagnose an even wider range of conditions by recognizing patterns in text, not just in medical images. This may augment what doctors can do on their own, he said. In some situations, physicians cannot consider all the possibilities, he said. This system can spot-check and make sure the physician didnt miss anything. The experimental system analyzed the electronic medical records of nearly 600,000 patients at the Guangzhou Women and Childrens Medical Center in southern China, learning to associate common medical conditions with specific patient information gathered by doctors, nurses and other technicians. First, a group of trained physicians annotated the hospital records, adding labels that identified information related to certain medical conditions. The system then analyzed the labeled data. Then the neural network was given new information, including a patients symptoms as determined during a physical examination. Soon it was able to make connections on its own between written records and observed symptoms. When tested on unlabeled data, the software could rival the performance of experienced physicians. It was more than 90 percent accurate at diagnosing asthma; the accuracy of physicians in the study ranged from 80 to 94 percent. In diagnosing gastrointestinal disease, the system was 87 percent accurate, compared with the physicians accuracy of 82 to 90 percent. Able to recognize patterns in data that humans could never identify on their own, neural networks can be enormously powerful in the right situation. But even experts have difficulty understanding why such networks make particular decisions and how they teach themselves. As a result, extensive testing is needed to reassure both doctors and patients that these systems are reliable. Experts said extensive clinical trials are now needed for Dr. Zhangs system, given the difficulty of interpreting decisions made by neural networks. Medicine is a slow-moving field, said Ben Shickel, a researcher at the University of Florida who specializes in the use of deep learning for health care. No one is just going to deploy one of these techniques without rigorous testing that shows exactly what is going on. It could be years before deep-learning systems are deployed in emergency rooms and clinics. But some are closer to real-world use: Google is now running clinical trials of its eye-scan system at two hospitals in southern India. Deep-learning diagnostic tools are more likely to flourish in countries outside the United States, Dr. Zhang said. Automated screening systems may be particularly useful in places where doctors are scarce, including in India and China. The system built by Dr. Zhang and his colleagues benefited from the large scale of the data set gathered from the hospital in Guangzhou. Similar data sets from American hospitals are typically smaller, both because the average hospital is smaller and because regulations make it difficult to pool data from multiple facilities. Dr. Zhang said he and his colleagues were careful to protect patients privacy in the new study. But he acknowledged that researchers in China may have an advantage when it comes to collecting and analyzing this kind of data. The sheer size of the population  the sheer size of the data  is a big difference, he said. A version of this article appears in print on , on Page B1 of the New York edition with the headline: In China, Training A.I. To Diagnose Diseases. Order Reprints | Todays Paper | SubscribeRelated Coverage\n",
      "Sentences: 64\n",
      "\n",
      "\n",
      "Article: https://www.cnbc.com/2019/02/12/google-facebook-apple-news-should-be-regulated-uk-government-report.html\n",
      "NodeRank:\n",
      "Independent ProgrammingSubscribe to CNBC PROLicensing & ReprintsJoin the CNBC PanelAdvertise With UsClosed CaptioningDigital ProductsTerms of ServicePrivacy PolicyNews ReleasesInternshipsCorrectionsAbout CNBCAdChoicesSite MapPodcastsContactCareersHelp\n",
      "Sentences: 1\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "Google and Facebook should be regulated for news content, UK government report saysA U.K. government report published Tuesday said a regulator should ensure tech firms like Facebook, Google and Apple are taking steps to help users identify trustworthy, reliable news on their platforms. The report also said social media companies should agree to \"code of conduct\" to govern their commercial agreements with publishers. The U.K. report's findings could add weight to the case for further regulation of tech companies in Britain and across Europe. Elizabeth Schulze | @eschulze9 Published 5 Hours Ago CNBC. com\n",
      "Sentences: 5\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "Chesnot | Getty ImagesFacebook founder and CEO Mark Zuckerberg. Online platforms including Google , Facebook and Apple should be regulated in how they distribute news content, according to a new U.K. government report. The report, published Tuesday , said a state regulator should ensure tech firms are taking steps to help users identify trustworthy, reliable news on their platforms. It said the regulator would require companies like Facebook and Google to build on initiatives they have already established to weed out fake content. \"This task is too important to leave entirely to the judgment of commercial entities,\" the report said. Online sites like Facebook, Twitter and YouTube have been under fire for allowing fake content to spread on their platforms. The companies have been investing in security measures to eliminate false accounts and misinformation , but the U.K. government report said these efforts should be enforced by a government agency. It also said they should sign a \"code of conduct\" to govern their commercial agreements with publishers. \"The experience of the last decade has shown that it is perfectly possible for social media platforms to be immensely profitable while simultaneously carrying a large quantity of fake news,\" it said. British Prime Minster Theresa May commissioned the report in 2018 to investigate the \"sustainability of the production and distribution of high-quality journalism.\" The independent review included contributions from publishers, advertisers, journalists, academics and industry groups. Rising tide of regulationThe U.K. report's findings could add weight to the case for further regulation of tech companies in Britain and across Europe. The recommendations in the report are non-binding and will now be considered by the U.K. government. The report called on the U.K.'s competition authority to investigate the online advertising industry to \"ensure fair competition.\" Google and Facebook accounted for an estimated 54 percent of online advertising revenue in the U.K. in 2017. \"The government must take steps to ensure the position of Google and Facebook does not do undue harm to publishers,\" the report said. Last week Germany's antitrust watchdog ruled Facebook had abused its market dominance in how it collects and merges user data. The authority said Facebook cannot combine data from separate apps like Instagram and WhatsApp without users' consent. Facebook said it is appealing the decision. Legal experts say antitrust authorities in Europe may be well-placed to lead the charge against tech companies in the region. \"Competition agencies are often more experienced and better resourced than data protection agencies and hence in a better position to successfully build the case against a big company like Facebook,\" said Anu Bradford, a professor and director of the European Legal Studies Center at Columbia Law School, in an email to CNBC last week. The European Commission, the executive arm of the EU, is currently investigating Google for antitrust violations in its advertising business. The Commission has already levied two record fines on the company for abusing antitrust rules with its Android devices and its comparison shopping service. show chapters\n",
      "Sentences: 24\n",
      "\n",
      "\n",
      "Article: https://techcrunch.com/2019/02/11/amazon-is-buying-home-mesh-router-startup-eero/\n",
      "NodeRank:\n",
      "Amazon is about to expand its smart home offerings in a big way. The company just announced its intention to acquire Bay Area-based home mesh router startup, Eero. Its a pretty clear fit for the online retailer as it pushes to make Alexa a feature in the connected home. The move also makes sense for five-year-old Eero, which, in spite of being early to the home mesh router game and pulling in some high-profile investors, has struggled. This time last year, the company laid off 30 employees  roughly one-fifth of its work force. Amazons certainly got the deep pockets, and the addition of Alexa to routers from Huawei and Netgear last year demonstrate that this category can be a viable one. It makes sense, as these coverage-extending mesh routers, like Echo Dots, are designed to be plugged into every room of the home. Amazon has been picking up a number of high-profile home automation startups in recent years, including Ring and Blink, as it looks to launch its own in-house Alexa smart home ecosystem. In many cases, Amazon has opted to retain the startups branding, which could bode well for the future of the Eero name  though the company admittedly doesnt have the same sort of recognition as Ring. We are incredibly impressed with the eero team and how quickly they invented a WiFi solution that makes connected devices just work, Amazon SVP Dave Limp said in a press release. We have a shared vision that the smart home experience can get even easier, and were committed to continue innovating on behalf of customers. The deal is still waiting for all of the standard regulatory approval. Details of the acquisition have yet to be disclosed.\n",
      "Sentences: 13\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "Amazon is about to expand its smart home offerings in a big way. The company just announced its intention to acquire Bay Area-based home mesh router startup, Eero. Its a pretty clear fit for the online retailer as it pushes to make Alexa a feature in the connected home. The move also makes sense for five-year-old Eero, which, in spite of being early to the home mesh router game and pulling in some high-profile investors, has struggled. This time last year, the company laid off 30 employees  roughly one-fifth of its work force. Amazons certainly got the deep pockets, and the addition of Alexa to routers from Huawei and Netgear last year demonstrate that this category can be a viable one. It makes sense, as these coverage-extending mesh routers, like Echo Dots, are designed to be plugged into every room of the home. Amazon has been picking up a number of high-profile home automation startups in recent years, including Ring and Blink, as it looks to launch its own in-house Alexa smart home ecosystem. In many cases, Amazon has opted to retain the startups branding, which could bode well for the future of the Eero name  though the company admittedly doesnt have the same sort of recognition as Ring. We are incredibly impressed with the eero team and how quickly they invented a WiFi solution that makes connected devices just work, Amazon SVP Dave Limp said in a press release. We have a shared vision that the smart home experience can get even easier, and were committed to continue innovating on behalf of customers. The deal is still waiting for all of the standard regulatory approval. Details of the acquisition have yet to be disclosed.\n",
      "Sentences: 13\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "Brian Heater @bheater / 1 dayAmazon is about to expand its smart home offerings in a big way. The company just announced its intention to acquire Bay Area-based home mesh router startup, Eero. Its a pretty clear fit for the online retailer as it pushes to make Alexa a feature in the connected home. The move also makes sense for five-year-old Eero, which, in spite of being early to the home mesh router game and pulling in some high-profile investors, has struggled. This time last year, the company laid off 30 employees  roughly one-fifth of its work force. Amazons certainly got the deep pockets, and the addition of Alexa to routers from Huawei and Netgear last year demonstrate that this category can be a viable one. It makes sense, as these coverage-extending mesh routers, like Echo Dots, are designed to be plugged into every room of the home. Amazon has been picking up a number of high-profile home automation startups in recent years, including Ring and Blink, as it looks to launch its own in-house Alexa smart home ecosystem. In many cases, Amazon has opted to retain the startups branding, which could bode well for the future of the Eero name  though the company admittedly doesnt have the same sort of recognition as Ring. We are incredibly impressed with the eero team and how quickly they invented a WiFi solution that makes connected devices just work, Amazon SVP Dave Limp said in a press release. We have a shared vision that the smart home experience can get even easier, and were committed to continue innovating on behalf of customers. The deal is still waiting for all of the standard regulatory approval. Details of the acquisition have yet to be disclosed.\n",
      "Sentences: 13\n",
      "\n",
      "\n",
      "Article: http://www.greatdisasters.co.uk/the-de-havilland-comet/\n",
      "NodeRank:\n",
      "Published February 1, 2019Although engineering is all around us, all of the time, most of us dont tend to give it much thought. The processes by which everyday items are designed dont usually matter too much to the end user. They do matter a lot, though, when they have an impact on safety. And when the product in question is a commercial jet airliner, supposed to carry people around the world in comfort and luxury, a flaw in the design can cost lives. The first de Havilland Comet prototype. Source: Wikimedia CommonsCommercial aviation is arguably one of the wonders of the modern age. It has helped to shrink the world, putting the entire globe within relatively easy reach of the average person. Although it existed as far back as the First World War  the first commercial air passenger paid $400 for a journey made in January 1914  it was in the wake of the Second World War that it expanded and, if youll excuse the pun, really took off. Many of the first commercial aircraft were originally designed for military use, and only later converted for passengers, but as air travel became more popular it was clear that aviation manufacturers needed to cater specifically to civilian usage. They needed to equip their aircraft with onboard comforts like lavatories, and for airline companies to make flight affordable, they needed to expand capacity. This led to the development of commercial jet airliners. They could be larger, faster, and more comfortable than their propeller-driven predecessors. It was the beginning of the Jet Age. De Havilland were a British aviation manufacturer intent on riding the crest of that wave. In 1949, they proudly unveiled the worlds first commercial jet airliner, the de Havilland Comet. It was introduced to passenger services in May of 1952. With four turbojet engines in the wings, the Comet promised a smoother flight experience than propeller aircraft could provide. Its pressurised cabin  something modern air travellers take for granted  was also a new development which meant that the Comet could now fly above rough weather, instead of taking a rough trip through it or a long one around it. And the large square windows gave passengers spectacular views. Advertisements for the plane in the 1950s made it sound like a dream. Eight miles a minute, eight miles high, in the smooth clear sky. You scarcely hear the Comets four mighty Rolls Royce jet engines You delight in the Comets luxury and BOACs world-famed personal service You arrive gloriously fresh, barely realising youve travelled at all! British Pathé footage of the de Havilland Comet in 1952However, de Havilland would soon discover that having the first jet airliner didnt necessarily mean it was the best. On the 2nd of May, 1953, BOAC Flight 783 took off from what was then called Dum Dum Airport in Calcutta. (Its now Netaji Subhas Chandra Bose international Airport in Kolkata. )The flight had started in Singapore, and was heading on towards London, via Delhi. On board this particular Comet there were six crew members and 37 passengers, mostly British but also including three Americans, two Burmese passengers and one Filipino. Just six minutes after taking off, Flight 783 was seen coming down in flames near the village of Jagalgori. There were no survivors. An investigation quickly followed. It was noted that the weather had been poor when Flight 783 came down; thunder storms were reported in the area. However, the investigation reported that weather conditions at take-off had been well within the allowed criteria, and the pilot was aware of the risk of storms. It further stated, The captain was not only well qualified but had considerable experience of weather conditions on this route. He was therefore fully competent to judge the weather forecast en route and the warning given, and make up his mind whether to take off or not. Witnesses had reported that the storm was unusually severe, though, and as the Comet was still climbing out they wouldnt have been able to fly over it. The evidence at this point, just one month after the incident, was somewhat inconclusive. We have no evidence before us to indicate sabotage or a stroke of lightning,nor faulty workmanship or defective material. There being thus no direct evidence as to the cause of the failure of the Comet to get safely through the storm, we have to infer it from the the state of the wreckage, which was distributed over a large area. There is no doubt that, as an expert witness, Mr Lett, has stated, the aircraft suffered a complete structural failure in the air and thereafter the aircraft was on fire in the air. What, specifically, had caused that failure, nobody knew for sure. De Havilland and BOAC made a joint statement after the report was published. The report is agreed in that the aircraft met severe gusts in thunderstorms. The force of the gusts has not been possible to determine but it is probable that they were of an unusually high order. It is not possible until the detailed examination, now underway, of the aircraft wreckage has been completed at the Royal Aircraft Establishment to determine the sequence of structural failure. Until such a sequence is determined it is only possible to theorise on the cause of the accident. The very considerable flying experience including many flights in turbulent conditions over the last three years by BOAC and de Havillands with Comet aircraft does not suggest that overcontrol or loss of control by the pilot was likely. The investigation recommended that de Havilland consider if any modification to the structure of the Comet is necessary. Of course, without knowing what the problem was, they were in the dark as to what they should modify. A de Havilland Comet at Entebbe Airport in 1952. Source: Wikimedia CommonsThey didnt really have time, either. Six months after the publication of that report, there was another catastrophe. It was the 10th of January, 1954, and another de Havilland Comet was making its way from Singapore to London. Designated Yoke Peter, it was the third Comet ever built, and on this trip it made it safely as far as Rome, where BOAC engineers made a routine inspection of the aircraft. They didnt see anything wrong with it, and Flight 781 took off as planned for the final leg of its flight. There were 6 crew and 29 passengers on board. Ten of the passengers were children. About twenty minutes into the flight, the captain of the Comet, 31 year old Alan Gibson, was on the radio speaking to another BOAC pilot in the area about the weather. In the middle of a sentence, Gibson was abruptly cut off. Near Elba Island, off the Italian coast, fishermen saw wreckage falling from the sky into the sea. They rushed to the scene, hoping to find survivors, but there were none. Investigators faced a challenge. In 1954, planes didnt carry flight data or cockpit voice recorders  the so-called black boxes which todays investigators rely on. Instead, like the investigators of Flight 783, they had to figure out what had happened from the physical evidence. But the wreckage of the plane had fallen into the sea. A pioneering search and recovery operation ensued. It involved Royal Navy ships, a civilian salvage vessel and novel use of underwater TV cameras  something which had never been done before. Eventually, enough wreckage would be raised and taken to the Royal Aircraft Establishment for the investigators to examine it for evidence. They already knew it had been a sudden event, of which Captain Gibson had no warning  otherwise he would have said something to the pilot he was on the radio with. The bodies of the passengers and crew had also revealed some details of the crash. The pathologist who examined them found a pattern of injuries. Many had limbs which had been broken or damaged after they died, but in the majority the cause of death appeared to be a combination of fractured skulls and ruptured lungs. The lung damage pointed to a problem older planes hadnt had  sudden depressurisation of the cabin, which caused the air inside to expand rapidly. Investigators simulated the crash, using a model fuselage with dummies seated inside and increasing the pressure inside until it exploded. This showed that the injuries had been caused when the passengers were thrown out of their seats and into the ceiling by the forces involved. But what had caused the explosion? The possibility of a deliberate bombing was considered, but discounted. The engines then came under suspicion. Based on the possibility of an explosion in the engines turbines, the entire Comet fleet was grounded until modifications to reinforce the turbine ring could be made. The New York Times reported this action on the 12th of January 1954  just two days after the crash. The British Overseas Airways Corporation temporarily withdrew from service tonight all de Havilland Comet jet airliners as an aftermath of the crash of a Comet yesterday near the island of Elba. This suspension of jet service by Britains one big overseas airline was followed promptly by similar action by the two French airlines that use the Comet  Air France and Union Aero Maritime des Transports. The B. O. A. C. described its action as a measure of prudence to enable a minute and unhurried technical examination of every aircraft in the Comet fleet to be carried out at maintenance headquarters at London airport. It was emphasized that the step did not constitute a grounding. It was taken after consultation with Alan T. Lennox-Boyd, Transport Minister, and had his concurrence  but was not Government-ordered. However, keeping the Comet out of service would be costly, so BOAC was keen to get the modifications made to the turbine rings and get it back into the skies as soon as possible. By the end of March, their Chairman was speaking reassuringly to the public on TV. We obviously wouldnt be flying the Comet with passengers if we werent satisfied conditions were suitable. Comet flights resumed on the 23rd of March 1954. A little over two weeks later, on the 8th of April, a BOAC charter flight operating as South African Airways Flight 201 left Ciampino Airport in Rome, bound for Cairo with a crew of 7 and 14 passengers. The same team of engineers who had checked Flight 781 inspected this Comet, designated Yoke Yoke, too. They had found some minor faults, and the plane had been held up for 25 hours as a result, but they were satisfied that it was ready to fly. The weather that evening was overcast but otherwise good. As the plane climbed into the sky, the Captain made routine radio reports of his location. And then, silence. Once again, radio contact with a de Havilland Comet had been lost. The New York Times later carried the bad news:Britain today weighed the cost of a stunning blow to her proudest pioneer industry  jet civil aviation  as the crash of another Comet airliner was confirmed. Twenty-one persons, including three Americans, were believed to have died when the plane was lost in the Mediterranean. The discovery of at least six bodies and bits of wreckage floating in the sea about 70 miles (110 km) south of Naples put a pall on the last hopes for the British Overseas Airways Corporation craft, missing since 6:57 oclock last night. The loss of this plane made it clear that the investigators working on Flight 781 had been wrong about the turbine ring; the modifications they had made to the fleet had done nothing to stop this plane, and all its passengers, from falling out of the sky. The investigation was back to square one. Once again, the Comet fleet was grounded as air-sea operations began to search for wreckage. However, aside from pieces that had been found floating, there was little hope for retrieval. The depth of the Mediterranean Sea at that location, much deeper than where Flight 781 had come down, made it impossible. With so little evidence available from the latest crash, the investigators had to return to the wreckage of Yoke Peter, the Flight 781 plane, to solve both incidents. Diagram showing recovered pieces of the Yoke Peter fuselage. Source: Wikimedia CommonsFortunately, that search operation was meeting with more success; by the end of August 1954 they had some 70% of the plane, and from the parts brought to the surface the investigators could draw more conclusions. In the tail section, they found shreds of carpet from the cabin. The rear fuselage also bore the imprint of a coin thrown from somebodys pocket, and traces of paint that were found to come from the passengers seats. All of this pointed, again, to an in-flight breakup. With most of the wreckage of Yoke Peter now raised and returned to the Royal Aircraft Establishment, the investigators were able to piece it back together, like a jigsaw puzzle, and trace the cracks in the aircraft back to the point where they had begun; a point in the roof of the cabin. They could now see that, after that first rupture had opened, a window had smashed into the elevators  vital control structures on the wings. The rear fuselage had torn away, the outer wing structure fell, and finally the cockpit broke away. The wreckage had caught fire from the fuel as it fell. But the investigators still needed to establish why that first crack had appeared. They had their suspicions, but needed proof. Today, they would be able to turn to a computer simulation, able to recreate the flight in virtual space, but in the fifties that simply wasnt an option. Instead, they embarked upon a remarkable practical experiment. BOAC donated the fuselage of another plane, Yoke Uncle, to the investigation, and a huge tank was constructed which would be able to contain the whole fuselage. By filling the tank with water, and then pumping even more water into the interior of the plane, they could recreate the effects of repeated pressurisation and depressurisation, as if the plane was taking flight after flight without ever leaving the ground. It wasnt going to be a quick test. Even running twenty-four hours a day and seven days a week, it was thought it could take months to reach a conclusion. Before beginning the experiment, Yoke Uncle had taken 1,230 flights. The tank experiment continued until it had added another 1,830  so the fuselage had experienced the equivalent of 3,060 flights  and then, the fuselage of Yoke Uncle broke, just as Yoke Yoke and Yoke Peter had done. Because of the water in the tank, the depressurisation was contained, instead of exploding with the force of a 500lb bomb as it had done in the air with the real flights. This meant that the investigators could clearly and easily see where the failure had occurred. The starting point was discovered at the corner of one of the windows. Upon closely examining that first crack, they found discolouration which came from algae in the water. This proved that the crack had endured several flights before it had actually spread to a catastrophic extent. Key piece of skin recovered from Yoke Peter. This is where the failure occurred. Source: Wikimedia CommonsThe cause was metal fatigue. Each time the Comet took off, the pressurisation in the cabin made the metal expand a little. Each time it landed, and depressurised, the metal contracted again. Over the course of many flights, this constant back and forth weakened the structure, just like bending a wire paperclip back and forth until it breaks. Obviously metal fatigue itself wasnt a new phenomenon. The engineers designing the Comet knew about it, but thought that they had taken it into account. What had they missed? The investigators returned to Yoke Uncle and the water tank. According to the report, no attempt had been made to measure the stress in the material of the skin at points where it might be expected to be higher than average. One reason for this omission was that the number of places coming within this descriptions is large, and it would have taken a long time to install the necessary strain gauges and other associated equipment. But it now seemed highly probable that the stress near the corners of the windows was higher than had been believed by the designers. Those strain gauges were now employed, and it was found that the general level of the stress in the skin in these regions was significantly higher than had been previously believed. The experimental plane, Yoke Uncle, had failed after 3,060 simulated flights. However, there were additional stresses involved in a real flight that werent replicated by the water tank experiment, such as the vibrations of the engines, so investigators believed that the plane would have failed sooner in real flight conditions. Yoke Peter, the Flight 781 plane, had made 1,290 flights at the time of its crash, and Yoke Yoke, the South African Airways flight, had made 900 flights. The report said, Sir Arnold Hall said in evidence that in the light of the experiment on Yoke Uncle, and of the measurements and calculation of stress he considered that the cabin of Yoke Peter had reached a point in its life when it could be said to be in danger of failure from fatigue, and that the cabin of Yoke Yoke would similarly be in danger. Dr Walker said that he did not regard the picture presented by the three failures (on the assumption that these were all due to the same fundamental cause) as surprising, since the three results taken together are consistent with general experience of the strength under repeated loading of a number of nominally identical structures, in which the stress level is high. Based on these conclusions, it was realised that designers had underestimated the safe loading strength requirements of pressurised airliner cabins. The standards were revised accordingly. The Comet itself went through extensive redesign. Most importantly, those large square windows were replaced with the round windows were all so familiar with today. Just by removing the sharp corners, the stress on the metal was reduced dramatically. But it was too late for de Havilland; they had lost their lead. Airlines who had been keen to buy the Comet before had now cancelled their orders, and even after the plane was redesigned and relaunched, sales didnt recover. Other manufacturers had taken notice of the Comets issues  and the solutions  and had been able to implement those lessons in their designs. American designs, like the Boeing 707 and the Douglas DC-8, had taken over from the Comet as the leading choices. In the end, the Comets first strength was also its biggest weakness; these tragedies occurred because it was the first commercial jet airliner. Metal fatigue, whilst a known phenomenon, wasnt thoroughly understood, and the designers simply hadnt known what effect the repeated stresses of pressurisation and depressurisation would have. Other manufacturers admitted, at least in private, that if de Havilland hadnt made those mistakes, one of them would have. Mistakes were made, and the lessons were learned at the expense of 99 lives, but every passenger who has travelled in a jet airliner since then has been safer because of it. Id like to say a special thank you to Patreon supporters like LouLi, Skeleheron, Mish Liddle and WillFriedrichs, and to all of you for listening and reading. Supporting the Great Disasters podcast on Patreon can give you access to exclusive content, including special mini-episodes, and helps the show keep going.\n",
      "Sentences: 163\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "Published February 1, 2019Although engineering is all around us, all of the time, most of us dont tend to give it much thought. The processes by which everyday items are designed dont usually matter too much to the end user. They do matter a lot, though, when they have an impact on safety. And when the product in question is a commercial jet airliner, supposed to carry people around the world in comfort and luxury, a flaw in the design can cost lives. The first de Havilland Comet prototype. Source: Wikimedia CommonsCommercial aviation is arguably one of the wonders of the modern age. It has helped to shrink the world, putting the entire globe within relatively easy reach of the average person. Although it existed as far back as the First World War  the first commercial air passenger paid $400 for a journey made in January 1914  it was in the wake of the Second World War that it expanded and, if youll excuse the pun, really took off. Many of the first commercial aircraft were originally designed for military use, and only later converted for passengers, but as air travel became more popular it was clear that aviation manufacturers needed to cater specifically to civilian usage. They needed to equip their aircraft with onboard comforts like lavatories, and for airline companies to make flight affordable, they needed to expand capacity. This led to the development of commercial jet airliners. They could be larger, faster, and more comfortable than their propeller-driven predecessors. It was the beginning of the Jet Age. De Havilland were a British aviation manufacturer intent on riding the crest of that wave. In 1949, they proudly unveiled the worlds first commercial jet airliner, the de Havilland Comet. It was introduced to passenger services in May of 1952. With four turbojet engines in the wings, the Comet promised a smoother flight experience than propeller aircraft could provide. Its pressurised cabin  something modern air travellers take for granted  was also a new development which meant that the Comet could now fly above rough weather, instead of taking a rough trip through it or a long one around it. And the large square windows gave passengers spectacular views. Advertisements for the plane in the 1950s made it sound like a dream. Eight miles a minute, eight miles high, in the smooth clear sky. You scarcely hear the Comets four mighty Rolls Royce jet engines You delight in the Comets luxury and BOACs world-famed personal service You arrive gloriously fresh, barely realising youve travelled at all! British Pathé footage of the de Havilland Comet in 1952However, de Havilland would soon discover that having the first jet airliner didnt necessarily mean it was the best. On the 2nd of May, 1953, BOAC Flight 783 took off from what was then called Dum Dum Airport in Calcutta. (Its now Netaji Subhas Chandra Bose international Airport in Kolkata. )The flight had started in Singapore, and was heading on towards London, via Delhi. On board this particular Comet there were six crew members and 37 passengers, mostly British but also including three Americans, two Burmese passengers and one Filipino. Just six minutes after taking off, Flight 783 was seen coming down in flames near the village of Jagalgori. There were no survivors. An investigation quickly followed. It was noted that the weather had been poor when Flight 783 came down; thunder storms were reported in the area. However, the investigation reported that weather conditions at take-off had been well within the allowed criteria, and the pilot was aware of the risk of storms. It further stated, The captain was not only well qualified but had considerable experience of weather conditions on this route. He was therefore fully competent to judge the weather forecast en route and the warning given, and make up his mind whether to take off or not. Witnesses had reported that the storm was unusually severe, though, and as the Comet was still climbing out they wouldnt have been able to fly over it. The evidence at this point, just one month after the incident, was somewhat inconclusive. We have no evidence before us to indicate sabotage or a stroke of lightning,nor faulty workmanship or defective material. There being thus no direct evidence as to the cause of the failure of the Comet to get safely through the storm, we have to infer it from the the state of the wreckage, which was distributed over a large area. There is no doubt that, as an expert witness, Mr Lett, has stated, the aircraft suffered a complete structural failure in the air and thereafter the aircraft was on fire in the air. What, specifically, had caused that failure, nobody knew for sure. De Havilland and BOAC made a joint statement after the report was published. The report is agreed in that the aircraft met severe gusts in thunderstorms. The force of the gusts has not been possible to determine but it is probable that they were of an unusually high order. It is not possible until the detailed examination, now underway, of the aircraft wreckage has been completed at the Royal Aircraft Establishment to determine the sequence of structural failure. Until such a sequence is determined it is only possible to theorise on the cause of the accident. The very considerable flying experience including many flights in turbulent conditions over the last three years by BOAC and de Havillands with Comet aircraft does not suggest that overcontrol or loss of control by the pilot was likely. The investigation recommended that de Havilland consider if any modification to the structure of the Comet is necessary. Of course, without knowing what the problem was, they were in the dark as to what they should modify. A de Havilland Comet at Entebbe Airport in 1952. Source: Wikimedia CommonsThey didnt really have time, either. Six months after the publication of that report, there was another catastrophe. It was the 10th of January, 1954, and another de Havilland Comet was making its way from Singapore to London. Designated Yoke Peter, it was the third Comet ever built, and on this trip it made it safely as far as Rome, where BOAC engineers made a routine inspection of the aircraft. They didnt see anything wrong with it, and Flight 781 took off as planned for the final leg of its flight. There were 6 crew and 29 passengers on board. Ten of the passengers were children. About twenty minutes into the flight, the captain of the Comet, 31 year old Alan Gibson, was on the radio speaking to another BOAC pilot in the area about the weather. In the middle of a sentence, Gibson was abruptly cut off. Near Elba Island, off the Italian coast, fishermen saw wreckage falling from the sky into the sea. They rushed to the scene, hoping to find survivors, but there were none. Investigators faced a challenge. In 1954, planes didnt carry flight data or cockpit voice recorders  the so-called black boxes which todays investigators rely on. Instead, like the investigators of Flight 783, they had to figure out what had happened from the physical evidence. But the wreckage of the plane had fallen into the sea. A pioneering search and recovery operation ensued. It involved Royal Navy ships, a civilian salvage vessel and novel use of underwater TV cameras  something which had never been done before. Eventually, enough wreckage would be raised and taken to the Royal Aircraft Establishment for the investigators to examine it for evidence. They already knew it had been a sudden event, of which Captain Gibson had no warning  otherwise he would have said something to the pilot he was on the radio with. The bodies of the passengers and crew had also revealed some details of the crash. The pathologist who examined them found a pattern of injuries. Many had limbs which had been broken or damaged after they died, but in the majority the cause of death appeared to be a combination of fractured skulls and ruptured lungs. The lung damage pointed to a problem older planes hadnt had  sudden depressurisation of the cabin, which caused the air inside to expand rapidly. Investigators simulated the crash, using a model fuselage with dummies seated inside and increasing the pressure inside until it exploded. This showed that the injuries had been caused when the passengers were thrown out of their seats and into the ceiling by the forces involved. But what had caused the explosion? The possibility of a deliberate bombing was considered, but discounted. The engines then came under suspicion. Based on the possibility of an explosion in the engines turbines, the entire Comet fleet was grounded until modifications to reinforce the turbine ring could be made. The New York Times reported this action on the 12th of January 1954  just two days after the crash. The British Overseas Airways Corporation temporarily withdrew from service tonight all de Havilland Comet jet airliners as an aftermath of the crash of a Comet yesterday near the island of Elba. This suspension of jet service by Britains one big overseas airline was followed promptly by similar action by the two French airlines that use the Comet  Air France and Union Aero Maritime des Transports. The B. O. A. C. described its action as a measure of prudence to enable a minute and unhurried technical examination of every aircraft in the Comet fleet to be carried out at maintenance headquarters at London airport. It was emphasized that the step did not constitute a grounding. It was taken after consultation with Alan T. Lennox-Boyd, Transport Minister, and had his concurrence  but was not Government-ordered. However, keeping the Comet out of service would be costly, so BOAC was keen to get the modifications made to the turbine rings and get it back into the skies as soon as possible. By the end of March, their Chairman was speaking reassuringly to the public on TV. We obviously wouldnt be flying the Comet with passengers if we werent satisfied conditions were suitable. Comet flights resumed on the 23rd of March 1954. A little over two weeks later, on the 8th of April, a BOAC charter flight operating as South African Airways Flight 201 left Ciampino Airport in Rome, bound for Cairo with a crew of 7 and 14 passengers. The same team of engineers who had checked Flight 781 inspected this Comet, designated Yoke Yoke, too. They had found some minor faults, and the plane had been held up for 25 hours as a result, but they were satisfied that it was ready to fly. The weather that evening was overcast but otherwise good. As the plane climbed into the sky, the Captain made routine radio reports of his location. And then, silence. Once again, radio contact with a de Havilland Comet had been lost. The New York Times later carried the bad news:Britain today weighed the cost of a stunning blow to her proudest pioneer industry  jet civil aviation  as the crash of another Comet airliner was confirmed. Twenty-one persons, including three Americans, were believed to have died when the plane was lost in the Mediterranean. The discovery of at least six bodies and bits of wreckage floating in the sea about 70 miles (110 km) south of Naples put a pall on the last hopes for the British Overseas Airways Corporation craft, missing since 6:57 oclock last night. The loss of this plane made it clear that the investigators working on Flight 781 had been wrong about the turbine ring; the modifications they had made to the fleet had done nothing to stop this plane, and all its passengers, from falling out of the sky. The investigation was back to square one. Once again, the Comet fleet was grounded as air-sea operations began to search for wreckage. However, aside from pieces that had been found floating, there was little hope for retrieval. The depth of the Mediterranean Sea at that location, much deeper than where Flight 781 had come down, made it impossible. With so little evidence available from the latest crash, the investigators had to return to the wreckage of Yoke Peter, the Flight 781 plane, to solve both incidents. Diagram showing recovered pieces of the Yoke Peter fuselage. Source: Wikimedia CommonsFortunately, that search operation was meeting with more success; by the end of August 1954 they had some 70% of the plane, and from the parts brought to the surface the investigators could draw more conclusions. In the tail section, they found shreds of carpet from the cabin. The rear fuselage also bore the imprint of a coin thrown from somebodys pocket, and traces of paint that were found to come from the passengers seats. All of this pointed, again, to an in-flight breakup. With most of the wreckage of Yoke Peter now raised and returned to the Royal Aircraft Establishment, the investigators were able to piece it back together, like a jigsaw puzzle, and trace the cracks in the aircraft back to the point where they had begun; a point in the roof of the cabin. They could now see that, after that first rupture had opened, a window had smashed into the elevators  vital control structures on the wings. The rear fuselage had torn away, the outer wing structure fell, and finally the cockpit broke away. The wreckage had caught fire from the fuel as it fell. But the investigators still needed to establish why that first crack had appeared. They had their suspicions, but needed proof. Today, they would be able to turn to a computer simulation, able to recreate the flight in virtual space, but in the fifties that simply wasnt an option. Instead, they embarked upon a remarkable practical experiment. BOAC donated the fuselage of another plane, Yoke Uncle, to the investigation, and a huge tank was constructed which would be able to contain the whole fuselage. By filling the tank with water, and then pumping even more water into the interior of the plane, they could recreate the effects of repeated pressurisation and depressurisation, as if the plane was taking flight after flight without ever leaving the ground. It wasnt going to be a quick test. Even running twenty-four hours a day and seven days a week, it was thought it could take months to reach a conclusion. Before beginning the experiment, Yoke Uncle had taken 1,230 flights. The tank experiment continued until it had added another 1,830  so the fuselage had experienced the equivalent of 3,060 flights  and then, the fuselage of Yoke Uncle broke, just as Yoke Yoke and Yoke Peter had done. Because of the water in the tank, the depressurisation was contained, instead of exploding with the force of a 500lb bomb as it had done in the air with the real flights. This meant that the investigators could clearly and easily see where the failure had occurred. The starting point was discovered at the corner of one of the windows. Upon closely examining that first crack, they found discolouration which came from algae in the water. This proved that the crack had endured several flights before it had actually spread to a catastrophic extent. Key piece of skin recovered from Yoke Peter. This is where the failure occurred. Source: Wikimedia CommonsThe cause was metal fatigue. Each time the Comet took off, the pressurisation in the cabin made the metal expand a little. Each time it landed, and depressurised, the metal contracted again. Over the course of many flights, this constant back and forth weakened the structure, just like bending a wire paperclip back and forth until it breaks. Obviously metal fatigue itself wasnt a new phenomenon. The engineers designing the Comet knew about it, but thought that they had taken it into account. What had they missed? The investigators returned to Yoke Uncle and the water tank. According to the report, no attempt had been made to measure the stress in the material of the skin at points where it might be expected to be higher than average. One reason for this omission was that the number of places coming within this descriptions is large, and it would have taken a long time to install the necessary strain gauges and other associated equipment. But it now seemed highly probable that the stress near the corners of the windows was higher than had been believed by the designers. Those strain gauges were now employed, and it was found that the general level of the stress in the skin in these regions was significantly higher than had been previously believed. The experimental plane, Yoke Uncle, had failed after 3,060 simulated flights. However, there were additional stresses involved in a real flight that werent replicated by the water tank experiment, such as the vibrations of the engines, so investigators believed that the plane would have failed sooner in real flight conditions. Yoke Peter, the Flight 781 plane, had made 1,290 flights at the time of its crash, and Yoke Yoke, the South African Airways flight, had made 900 flights. The report said, Sir Arnold Hall said in evidence that in the light of the experiment on Yoke Uncle, and of the measurements and calculation of stress he considered that the cabin of Yoke Peter had reached a point in its life when it could be said to be in danger of failure from fatigue, and that the cabin of Yoke Yoke would similarly be in danger. Dr Walker said that he did not regard the picture presented by the three failures (on the assumption that these were all due to the same fundamental cause) as surprising, since the three results taken together are consistent with general experience of the strength under repeated loading of a number of nominally identical structures, in which the stress level is high. Based on these conclusions, it was realised that designers had underestimated the safe loading strength requirements of pressurised airliner cabins. The standards were revised accordingly. The Comet itself went through extensive redesign. Most importantly, those large square windows were replaced with the round windows were all so familiar with today. Just by removing the sharp corners, the stress on the metal was reduced dramatically. But it was too late for de Havilland; they had lost their lead. Airlines who had been keen to buy the Comet before had now cancelled their orders, and even after the plane was redesigned and relaunched, sales didnt recover. Other manufacturers had taken notice of the Comets issues  and the solutions  and had been able to implement those lessons in their designs. American designs, like the Boeing 707 and the Douglas DC-8, had taken over from the Comet as the leading choices. In the end, the Comets first strength was also its biggest weakness; these tragedies occurred because it was the first commercial jet airliner. Metal fatigue, whilst a known phenomenon, wasnt thoroughly understood, and the designers simply hadnt known what effect the repeated stresses of pressurisation and depressurisation would have. Other manufacturers admitted, at least in private, that if de Havilland hadnt made those mistakes, one of them would have. Mistakes were made, and the lessons were learned at the expense of 99 lives, but every passenger who has travelled in a jet airliner since then has been safer because of it. Thanks this episode go to:Id like to say a special thank you to Patreon supporters like LouLi, Skeleheron , Mish Liddle and WillFriedrichs , and to all of you for listening and reading. Supporting the Great Disasters podcast on Patreon can give you access to exclusive content, including special mini-episodes, and helps the show keep going.\n",
      "Sentences: 163\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "The de Havilland CometPublished February 1, 2019Although engineering is all around us, all of the time, most of us dont tend to give it much thought. The processes by which everyday items are designed dont usually matter too much to the end user. They do matter a lot, though, when they have an impact on safety. And when the product in question is a commercial jet airliner, supposed to carry people around the world in comfort and luxury, a flaw in the design can cost lives. The first de Havilland Comet prototype. Source: Wikimedia CommonsCommercial aviation is arguably one of the wonders of the modern age. It has helped to shrink the world, putting the entire globe within relatively easy reach of the average person. Although it existed as far back as the First World War  the first commercial air passenger paid $400 for a journey made in January 1914  it was in the wake of the Second World War that it expanded and, if youll excuse the pun, really took off. Many of the first commercial aircraft were originally designed for military use, and only later converted for passengers, but as air travel became more popular it was clear that aviation manufacturers needed to cater specifically to civilian usage. They needed to equip their aircraft with onboard comforts like lavatories, and for airline companies to make flight affordable, they needed to expand capacity. This led to the development of commercial jet airliners. They could be larger, faster, and more comfortable than their propeller-driven predecessors. It was the beginning of the Jet Age. De Havilland were a British aviation manufacturer intent on riding the crest of that wave. In 1949, they proudly unveiled the worlds first commercial jet airliner, the de Havilland Comet. It was introduced to passenger services in May of 1952. With four turbojet engines in the wings, the Comet promised a smoother flight experience than propeller aircraft could provide. Its pressurised cabin  something modern air travellers take for granted  was also a new development which meant that the Comet could now fly above rough weather, instead of taking a rough trip through it or a long one around it. And the large square windows gave passengers spectacular views. Advertisements for the plane in the 1950s made it sound like a dream. Eight miles a minute, eight miles high, in the smooth clear sky. You scarcely hear the Comets four mighty Rolls Royce jet engines You delight in the Comets luxury and BOACs world-famed personal service You arrive gloriously fresh, barely realising youve travelled at all! British Pathé footage of the de Havilland Comet in 1952However, de Havilland would soon discover that having the first jet airliner didnt necessarily mean it was the best. On the 2nd of May, 1953, BOAC Flight 783 took off from what was then called Dum Dum Airport in Calcutta. (Its now Netaji Subhas Chandra Bose international Airport in Kolkata. )The flight had started in Singapore, and was heading on towards London, via Delhi. On board this particular Comet there were six crew members and 37 passengers, mostly British but also including three Americans, two Burmese passengers and one Filipino. Just six minutes after taking off, Flight 783 was seen coming down in flames near the village of Jagalgori. There were no survivors. An investigation quickly followed. It was noted that the weather had been poor when Flight 783 came down; thunder storms were reported in the area. However, the investigation reported that weather conditions at take-off had been well within the allowed criteria, and the pilot was aware of the risk of storms. It further stated, The captain was not only well qualified but had considerable experience of weather conditions on this route. He was therefore fully competent to judge the weather forecast en route and the warning given, and make up his mind whether to take off or not. Witnesses had reported that the storm was unusually severe, though, and as the Comet was still climbing out they wouldnt have been able to fly over it. The evidence at this point, just one month after the incident, was somewhat inconclusive. We have no evidence before us to indicate sabotage or a stroke of lightning,nor faulty workmanship or defective material. There being thus no direct evidence as to the cause of the failure of the Comet to get safely through the storm, we have to infer it from the the state of the wreckage, which was distributed over a large area. There is no doubt that, as an expert witness, Mr Lett, has stated, the aircraft suffered a complete structural failure in the air and thereafter the aircraft was on fire in the air. What, specifically, had caused that failure, nobody knew for sure. De Havilland and BOAC made a joint statement after the report was published. The report is agreed in that the aircraft met severe gusts in thunderstorms. The force of the gusts has not been possible to determine but it is probable that they were of an unusually high order. It is not possible until the detailed examination, now underway, of the aircraft wreckage has been completed at the Royal Aircraft Establishment to determine the sequence of structural failure. Until such a sequence is determined it is only possible to theorise on the cause of the accident. The very considerable flying experience including many flights in turbulent conditions over the last three years by BOAC and de Havillands with Comet aircraft does not suggest that overcontrol or loss of control by the pilot was likely. The investigation recommended that de Havilland consider if any modification to the structure of the Comet is necessary. Of course, without knowing what the problem was, they were in the dark as to what they should modify. A de Havilland Comet at Entebbe Airport in 1952. Source: Wikimedia CommonsThey didnt really have time, either. Six months after the publication of that report, there was another catastrophe. It was the 10th of January, 1954, and another de Havilland Comet was making its way from Singapore to London. Designated Yoke Peter, it was the third Comet ever built, and on this trip it made it safely as far as Rome, where BOAC engineers made a routine inspection of the aircraft. They didnt see anything wrong with it, and Flight 781 took off as planned for the final leg of its flight. There were 6 crew and 29 passengers on board. Ten of the passengers were children. About twenty minutes into the flight, the captain of the Comet, 31 year old Alan Gibson, was on the radio speaking to another BOAC pilot in the area about the weather. In the middle of a sentence, Gibson was abruptly cut off. Near Elba Island, off the Italian coast, fishermen saw wreckage falling from the sky into the sea. They rushed to the scene, hoping to find survivors, but there were none. Investigators faced a challenge. In 1954, planes didnt carry flight data or cockpit voice recorders  the so-called black boxes which todays investigators rely on. Instead, like the investigators of Flight 783, they had to figure out what had happened from the physical evidence. But the wreckage of the plane had fallen into the sea. A pioneering search and recovery operation ensued. It involved Royal Navy ships, a civilian salvage vessel and novel use of underwater TV cameras  something which had never been done before. Eventually, enough wreckage would be raised and taken to the Royal Aircraft Establishment for the investigators to examine it for evidence. They already knew it had been a sudden event, of which Captain Gibson had no warning  otherwise he would have said something to the pilot he was on the radio with. The bodies of the passengers and crew had also revealed some details of the crash. The pathologist who examined them found a pattern of injuries. Many had limbs which had been broken or damaged after they died, but in the majority the cause of death appeared to be a combination of fractured skulls and ruptured lungs. The lung damage pointed to a problem older planes hadnt had  sudden depressurisation of the cabin, which caused the air inside to expand rapidly. Investigators simulated the crash, using a model fuselage with dummies seated inside and increasing the pressure inside until it exploded. This showed that the injuries had been caused when the passengers were thrown out of their seats and into the ceiling by the forces involved. But what had caused the explosion? The possibility of a deliberate bombing was considered, but discounted. The engines then came under suspicion. Based on the possibility of an explosion in the engines turbines, the entire Comet fleet was grounded until modifications to reinforce the turbine ring could be made. The New York Times reported this action on the 12th of January 1954  just two days after the crash. The British Overseas Airways Corporation temporarily withdrew from service tonight all de Havilland Comet jet airliners as an aftermath of the crash of a Comet yesterday near the island of Elba. This suspension of jet service by Britains one big overseas airline was followed promptly by similar action by the two French airlines that use the Comet  Air France and Union Aero Maritime des Transports. The B. O. A. C. described its action as a measure of prudence to enable a minute and unhurried technical examination of every aircraft in the Comet fleet to be carried out at maintenance headquarters at London airport. It was emphasized that the step did not constitute a grounding. It was taken after consultation with Alan T. Lennox-Boyd, Transport Minister, and had his concurrence  but was not Government-ordered. However, keeping the Comet out of service would be costly, so BOAC was keen to get the modifications made to the turbine rings and get it back into the skies as soon as possible. By the end of March, their Chairman was speaking reassuringly to the public on TV. We obviously wouldnt be flying the Comet with passengers if we werent satisfied conditions were suitable. Comet flights resumed on the 23rd of March 1954. A little over two weeks later, on the 8th of April, a BOAC charter flight operating as South African Airways Flight 201 left Ciampino Airport in Rome, bound for Cairo with a crew of 7 and 14 passengers. The same team of engineers who had checked Flight 781 inspected this Comet, designated Yoke Yoke, too. They had found some minor faults, and the plane had been held up for 25 hours as a result, but they were satisfied that it was ready to fly. The weather that evening was overcast but otherwise good. As the plane climbed into the sky, the Captain made routine radio reports of his location. And then, silence. Once again, radio contact with a de Havilland Comet had been lost. The New York Times later carried the bad news:Britain today weighed the cost of a stunning blow to her proudest pioneer industry  jet civil aviation  as the crash of another Comet airliner was confirmed. Twenty-one persons, including three Americans, were believed to have died when the plane was lost in the Mediterranean. The discovery of at least six bodies and bits of wreckage floating in the sea about 70 miles (110 km) south of Naples put a pall on the last hopes for the British Overseas Airways Corporation craft, missing since 6:57 oclock last night. The loss of this plane made it clear that the investigators working on Flight 781 had been wrong about the turbine ring; the modifications they had made to the fleet had done nothing to stop this plane, and all its passengers, from falling out of the sky. The investigation was back to square one. Once again, the Comet fleet was grounded as air-sea operations began to search for wreckage. However, aside from pieces that had been found floating, there was little hope for retrieval. The depth of the Mediterranean Sea at that location, much deeper than where Flight 781 had come down, made it impossible. With so little evidence available from the latest crash, the investigators had to return to the wreckage of Yoke Peter, the Flight 781 plane, to solve both incidents. Diagram showing recovered pieces of the Yoke Peter fuselage. Source: Wikimedia CommonsFortunately, that search operation was meeting with more success; by the end of August 1954 they had some 70% of the plane, and from the parts brought to the surface the investigators could draw more conclusions. In the tail section, they found shreds of carpet from the cabin. The rear fuselage also bore the imprint of a coin thrown from somebodys pocket, and traces of paint that were found to come from the passengers seats. All of this pointed, again, to an in-flight breakup. With most of the wreckage of Yoke Peter now raised and returned to the Royal Aircraft Establishment, the investigators were able to piece it back together, like a jigsaw puzzle, and trace the cracks in the aircraft back to the point where they had begun; a point in the roof of the cabin. They could now see that, after that first rupture had opened, a window had smashed into the elevators  vital control structures on the wings. The rear fuselage had torn away, the outer wing structure fell, and finally the cockpit broke away. The wreckage had caught fire from the fuel as it fell. But the investigators still needed to establish why that first crack had appeared. They had their suspicions, but needed proof. Today, they would be able to turn to a computer simulation, able to recreate the flight in virtual space, but in the fifties that simply wasnt an option. Instead, they embarked upon a remarkable practical experiment. BOAC donated the fuselage of another plane, Yoke Uncle, to the investigation, and a huge tank was constructed which would be able to contain the whole fuselage. By filling the tank with water, and then pumping even more water into the interior of the plane, they could recreate the effects of repeated pressurisation and depressurisation, as if the plane was taking flight after flight without ever leaving the ground. It wasnt going to be a quick test. Even running twenty-four hours a day and seven days a week, it was thought it could take months to reach a conclusion. Before beginning the experiment, Yoke Uncle had taken 1,230 flights. The tank experiment continued until it had added another 1,830  so the fuselage had experienced the equivalent of 3,060 flights  and then, the fuselage of Yoke Uncle broke, just as Yoke Yoke and Yoke Peter had done. Because of the water in the tank, the depressurisation was contained, instead of exploding with the force of a 500lb bomb as it had done in the air with the real flights. This meant that the investigators could clearly and easily see where the failure had occurred. The starting point was discovered at the corner of one of the windows. Upon closely examining that first crack, they found discolouration which came from algae in the water. This proved that the crack had endured several flights before it had actually spread to a catastrophic extent. Key piece of skin recovered from Yoke Peter. This is where the failure occurred. Source: Wikimedia CommonsThe cause was metal fatigue. Each time the Comet took off, the pressurisation in the cabin made the metal expand a little. Each time it landed, and depressurised, the metal contracted again. Over the course of many flights, this constant back and forth weakened the structure, just like bending a wire paperclip back and forth until it breaks. Obviously metal fatigue itself wasnt a new phenomenon. The engineers designing the Comet knew about it, but thought that they had taken it into account. What had they missed? The investigators returned to Yoke Uncle and the water tank. According to the report, no attempt had been made to measure the stress in the material of the skin at points where it might be expected to be higher than average. One reason for this omission was that the number of places coming within this descriptions is large, and it would have taken a long time to install the necessary strain gauges and other associated equipment. But it now seemed highly probable that the stress near the corners of the windows was higher than had been believed by the designers. Those strain gauges were now employed, and it was found that the general level of the stress in the skin in these regions was significantly higher than had been previously believed. The experimental plane, Yoke Uncle, had failed after 3,060 simulated flights. However, there were additional stresses involved in a real flight that werent replicated by the water tank experiment, such as the vibrations of the engines, so investigators believed that the plane would have failed sooner in real flight conditions. Yoke Peter, the Flight 781 plane, had made 1,290 flights at the time of its crash, and Yoke Yoke, the South African Airways flight, had made 900 flights. The report said, Sir Arnold Hall said in evidence that in the light of the experiment on Yoke Uncle, and of the measurements and calculation of stress he considered that the cabin of Yoke Peter had reached a point in its life when it could be said to be in danger of failure from fatigue, and that the cabin of Yoke Yoke would similarly be in danger. Dr Walker said that he did not regard the picture presented by the three failures (on the assumption that these were all due to the same fundamental cause) as surprising, since the three results taken together are consistent with general experience of the strength under repeated loading of a number of nominally identical structures, in which the stress level is high. Based on these conclusions, it was realised that designers had underestimated the safe loading strength requirements of pressurised airliner cabins. The standards were revised accordingly. The Comet itself went through extensive redesign. Most importantly, those large square windows were replaced with the round windows were all so familiar with today. Just by removing the sharp corners, the stress on the metal was reduced dramatically. But it was too late for de Havilland; they had lost their lead. Airlines who had been keen to buy the Comet before had now cancelled their orders, and even after the plane was redesigned and relaunched, sales didnt recover. Other manufacturers had taken notice of the Comets issues  and the solutions  and had been able to implement those lessons in their designs. American designs, like the Boeing 707 and the Douglas DC-8, had taken over from the Comet as the leading choices. In the end, the Comets first strength was also its biggest weakness; these tragedies occurred because it was the first commercial jet airliner. Metal fatigue, whilst a known phenomenon, wasnt thoroughly understood, and the designers simply hadnt known what effect the repeated stresses of pressurisation and depressurisation would have. Other manufacturers admitted, at least in private, that if de Havilland hadnt made those mistakes, one of them would have. Mistakes were made, and the lessons were learned at the expense of 99 lives, but every passenger who has travelled in a jet airliner since then has been safer because of it. Thanks this episode go to:Id like to say a special thank you to Patreon supporters like LouLi, Skeleheron, Mish Liddle and WillFriedrichs, and to all of you for listening and reading. Supporting the Great Disasters podcast on Patreon can give you access to exclusive content, including special mini-episodes, and helps the show keep going. No Newer Posts Return to BlogBe First to Comment\n",
      "Sentences: 164\n",
      "\n",
      "\n",
      "Article: https://techcrunch.com/2019/02/11/google-docs-gets-an-api-for-task-automation/\n",
      "NodeRank:\n",
      "Google  today announced the general availability of a new API for Google Docs that will allow developers to automate many of the tasks that users typically do manually in the companys online office suite. The API has been in developer preview since last Aprils Google Cloud Next 2018 and is now available to all developers. As Google notes, the REST API was designed to help developers build workflow automation services for their users, build content management services and create documents in bulk. Using the API, developers can also set up processes that manipulate documents after the fact to update them, and the API also features the ability to insert, delete, move, merge and format text, insert inline images and work with lists, among other things. The canonical use case here isinvoicing, where you need to regularly create similar documents with ever-changing order numbers and line items based on information from third-party systems (or maybe even just a Google Sheet). Google also notes that the APIs import/export abilities allow you to use Docs for internal content management systems. Some of the companies that built solutions based on the new API during the preview period include Zapier, Netflix, Mailchimp and Final Draft. Zapier integrated the Docs API into its own workflow automation tool to help its users create offer letters based on a template, for example, while Netflix used it to build an internal tool that helps its engineers gather data and automate its documentation workflow.\n",
      "Sentences: 8\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "Google today announced the general availability of a new API for Google Docs that will allow developers to automate many of the tasks that users typically do manually in the companys online office suite. The API has been in developer preview since last Aprils Google Cloud Next 2018 and is now available to all developers. As Google notes, the REST API was designed to help developers build workflow automation services for their users, build content management services and create documents in bulk. Using the API, developers can also set up processes that manipulate documents after the fact to update them, and the API also features the ability to insert, delete, move , merge and format text, insert inline images and work with lists , among other things. The canonical use case here isinvoicing, where you need to regularly create similar documents with ever-changing order numbers and line items based on information from third-party systems (or maybe even just a Google Sheet). Google also notes that the APIs import/export abilities allow you to use Docs for internal content management systems. Some of the companies that built solutions based on the new API during the preview period include Zapier, Netflix, Mailchimp and Final Draft. Zapier integrated the Docs API into its own workflow automation tool to help its users create offer letters based on a template, for example, while Netflix used it to build an internal tool that helps its engineers gather data and automate its documentation workflow.\n",
      "Sentences: 8\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "Frederic Lardinois @fredericl / 1 dayGoogle today announced the general availability of a new API for Google Docs that will allow developers to automate many of the tasks that users typically do manually in the companys online office suite. The API has been in developer preview since last Aprils Google Cloud Next 2018 and is now available to all developers. As Google notes, the REST API was designed to help developers build workflow automation services for their users, build content management services and create documents in bulk. Using the API, developers can also set up processes that manipulate documents after the fact to update them, and the API also features the ability to insert, delete, move , merge and format text, insert inline images and work with lists , among other things. The canonical use case here isÂinvoicing, where you need to regularly create similar documents with ever-changing order numbers and line items based on information from third-party systems (or maybe even just a Google Sheet). Google also notes that the APIs import/export abilities allow you to use Docs for internal content management systems. Some of the companies that built solutions based on the new API during the preview period include Zapier, Netflix, Mailchimp and Final Draft. Zapier integrated the Docs API into its own workflow automation tool to help its users create offer letters based on a template, for example, while Netflix used it to build an internal tool that helps its engineers gather data and automate its documentation workflow.\n",
      "Sentences: 8\n",
      "\n",
      "\n",
      "Article: https://ephemeralnewyork.wordpress.com/2019/02/11/the-bobbed-hair-bandit-on-the-run-in-brooklyn/\n",
      "NodeRank:\n",
      "The writing on the wall of an East SidetenementUpper Manhattan once resembled a countrytown Like other working-class girls in 1920s Brooklyn, Celia Cooney had big dreams. Celia (at right and below) was a 20-year-old newlywed who toiled in a laundry. She and her husband, Ed, shared a furnished room on Madison Street in a neighborhood then called Bedford, todays Bedford-Stuyvesant. Celia and Ed were very much in love. But like many young couples, they had a hard time saving money. Ed didnt make much as a welder, and Celia enjoyed nice things, like the sealskin fur coat Ed bought for her. So when Celia found out she was pregnant, the Cooneys decided they needed to shore up their finances. How? By committing armed robbery. Thats the genesis of the Bobbed-Haired Bandit, as Celia was dubbed by the press. Together the couple (below, in their wedding photo) would stage holdups of Brooklyn groceries and drugstores and become Roaring Twenties tabloid icons. Their first robbery was at a Roulstons, a grocery chain in Park Slope. On the evening of January 5, the two drove to the store on Seventh Avenue and Seventh Street. Wearing her fur coat, Celia went in first and asked for a dozen eggs, according to the 2005 book, The Bobbed-Hair Bandit, by Stephen Duncombe and Andrew Mattson. As the clerk readied her purchase, Ed entered the store. Celia pulled an automatic out of her pocket, pointed it at the clerk, and yelled, Stick em up, quick! just as the bad guys in the detective stories and pulpy novels she devoured would say. Ed then whipped out a gun in each hand and cleaned out the cash register. The two took off with more than $600. The next day, the brazen heist made by a slight, five-foot woman and her male partner ended up in the Brooklyn Eagle, with the headline Woman With a Gun. Celia and Ed went on to commit several more robberies. The newspapers giddily wrote up each hit, making much of Celias bobbed haira daring style popular with flappers and other women who saw themselves as modern and liberated. Ed was dubbed her tall male companion. After the first robbery, the couple immediately rented a two-story frame house at 1099 Pacific Street. They bought pricey furniture, and Celia made her husband a special dinner of porterhouse steak, states The Bobbed-Hair Bandit. But they quickly spent their lootand had to commit more robberies to keep up their new higher-end lifestyle. With so much tabloid exposure, the police were under pressure to capture the girl robber. That led cops to arrest and charge a 23-year-old bobbed-haired Brooklyn actress named Helen Quigley for the crimes. Angry that the police had arrested an innocent young woman, Celia left a note for them after she and Ed robbed a Clinton Hill drugstore. The note was addressed to the dirty fish-peddling bums and ordered them to let Helen Quigley gowhich eventually the police did. Celia and Eds stick-up spree finally ended in early spring, after a warehouse worker at the National Biscuit Company on Pacific Street was wounded during a holdup. Panicked, the couple fled, leaving behind $8,000 in an open safe, wrote the New York Times in 2015. A warehouse employee recognized Ed from the neighborhood, and the couple was soon identified. By then, they had taken off for Florida, where Celia gave birth to her daughter on April 12, who sadly died days later. After the couple was arrested and brought back to New York (above, mobbed by crowds at Penn Station), they pleaded guilty and landed 10 to 20 years in prison. Paroled after seven years, the couple went on to have two sons. (Finally free and reunited with their lawyer, above. )Ed died of tuberculosis in 1936. As for Celia, she reportedly was a dutiful and selfless mother, working to support her boys, one of whom became a deacon in the Roman Catholic Church, continued the Times. It was not until a few years before she died in 1992 that her middle-aged sons learned about the Bobbed Hair Bandit. [Top photo: Wikipedia; second image: Newspapers. com blog, Fishwrap; third image: Brooklyn Eagle; fourth image: Library of Congress; fifth image: Buffalo Commercial; sixth image: author collection; seventh image: New York Daily News; eighth image: Getty Images]TwitterFacebookRedditTumblrPinterestEmailLinkedInGoogleLike Loading... Tags: Bobbed-Hair Bandit, Brooklyn 1920s, Brooklyn Bobbed-Hair Bandit, Brooklyn criminals, Celia and Ed Cooney Brooklyn, Celia Cooney Bobbed-Hair Bandit, Famous Crimes of BrooklynThis entry was posted on February 11, 2019 at 4:40 am and is filed under Brooklyn, Disasters and crimes. You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own site. snufflegrinbooks Says:February 11, 2019 at 7:26 am | Reply Thanks, Ephemeral, very interesting. A very Bonnie & Clyde story. Were there other instances of male/female, husband/wife gangsters, does anyone know ? Shayne Davidson Says:February 11, 2019 at 1:55 pm | Reply There were plenty of them: https://capturedandexposed. com/2019/02/06/the-felonious-housewife/ keenanpatrick424 Says:February 11, 2019 at 11:15 am | Reply I have never understood the appeal and celebration of armed robbers especially the subset of criminal couples (Bonnie and Clyde,The Honeymoon Killers)). Is Ephemeral N.Y.wistful for this hideous behavior? Are you pininig for a reprise of this greedy and violent anti social behavior? In this era of identity politics and political correctness would a transgender bipolar couple of people of color terrorizing and shooting at working people and wounding innocent bystanders satisfy your voyeuristic perversion? Mike Says:February 11, 2019 at 1:08 pm | Reply Jeez. Relax. Take a deep breath keenanpatrick424 Says:February 11, 2019 at 8:51 pm Mike- How relaxed would you be if during a robbery you were on the floor blind folded ,hog-tied a knee in your back and a 45 jammed against the back of your skull. Instead of romanticizing violent gun wielding punks, have some empathy for the victms. ENY How about a little known N.Y.C. story about a couple of con artists who took off a bank or a Wall St. firm instead of a couple who violently preyed on their own kind. ephemeralnewyork Says:February 11, 2019 at 4:02 pm | Reply Like all posts on ENY, this one simply tells a little-known story from New Yorks pasttheres no pining or agenda behind it. Tommy Dulski Says:February 11, 2019 at 11:59 am | Reply Great little story, I had never heard of this couple before. Mykola (Mick) Dementiuk Says:February 11, 2019 at 1:25 pm | Reply In the early 1960s me and my girlfriend stole from stores, mostly in Newark NJ, nothing big just small pocket items. It ended when I was nabbed pilfering a package of sandwich meat I stuffed in my pocket, looked mighty tasty at the time, and showed my career of being a stickup-man quickly came to a close. Anyway, nice story. Shayne Davidson Says:February 11, 2019 at 1:58 pm | Reply Crime is part of history. In this case, the woman reformed and led an honest life after she was released from prison. Interesting story! VirginiaB Says:February 11, 2019 at 3:27 pm | Reply What a storyand great illustrations. While they were certainly criminals, Id hardly call them Bonnie and Clyde. Id love to know more about her turn-around after prison. Is there a book about this? Ive read some great books based on your blog. Thanks! ephemeralnewyork Says:February 11, 2019 at 4:04 pm | Reply Yes, youll want to check out The Bobbed-Hair Bandit by Stephen Duncombe and Andrew Mattson. Its a very well-done deep dive into Celia and Ed and 1920sBrooklyn https://books. google. com/books? id=uy2aqWeHaZUC&printsec=frontcover&dq=the+bobbed+hair+bandit&hl=en&sa=X&ved=0ahUKEwiC0Iunh7TgAhWtdd8KHbsQBWQQ6AEIKjAA#v=onepage&q=the%20bobbed%20hair%20bandit&f=false VirginiaB Says:February 11, 2019 at 4:15 pm Thanks so muchI have already requested it from my library. I learn so much from your blog. ephemeralnewyork Says:February 11, 2019 at 4:22 pm | Reply Glad to hear it! Its a fascinating story I loved researching. Enter your comment here... Fill in your details below or click an icon to log in:     Email (required) (Address never made public)Name (required)WebsiteYou are commenting using your WordPress. com account. (LogOut/Change)You are commenting using your Google+ account. (LogOut/Change)You are commenting using your Twitter account. (LogOut/Change)You are commenting using your Facebook account. (LogOut/Change)CancelConnecting to %s Notify me of new comments via email. Notify me of new posts via email. This site uses Akismet to reduce spam. Learn how your comment data is processed.\n",
      "Sentences: 91\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "Like other working-class girls in 1920s Brooklyn, Celia Cooney had big dreams. Celia (at right and below) was a 20-year-old newlywed who toiled in a laundry. She and her husband, Ed, shared a furnished room on Madison Street in a neighborhood then called Bedford, todays Bedford-Stuyvesant. Celia and Ed were very much in love. But like many young couples, they had a hard time saving money. Ed didnt make much as a welder, and Celia enjoyed nice things, like the sealskin fur coat Ed bought for her. So when Celia found out she was pregnant, the Cooneys decided they needed to shore up their finances. How? By committing armed robbery. Thats the genesis of the Bobbed-Haired Bandit, as Celia was dubbed by the press. Together the couple (below, in their wedding photo) would stage holdups of Brooklyn groceries and drugstores and become Roaring Twenties tabloid icons. Their first robbery was at a Roulstons , a grocery chain in Park Slope. On the evening of January 5, the two drove to the store on Seventh Avenue and Seventh Street. Wearing her fur coat, Celia went in first and asked for a dozen eggs, according to the 2005 book, The Bobbed-Hair Bandit , by Stephen Duncombe and Andrew Mattson. As the clerk readied her purchase, Ed entered the store. Celia pulled an automatic out of her pocket, pointed it at the clerk, and yelled, Stick em up, quick! just as the bad guys in the detective stories and pulpy novels she devoured would say. Ed then whipped out a gun in each hand and cleaned out the cash register. The two took off with more than $600. The next day, the brazen heist made by a slight, five-foot woman and her male partner ended up in the Brooklyn Eagle , with the headline Woman With a Gun. Celia and Ed went on to commit several more robberies. The newspapers giddily wrote up each hit, making much of Celias bobbed haira daring style popular with flappers and other women who saw themselves as modern and liberated. Ed was dubbed her tall male companion. After the first robbery, the couple immediately rented a two-story frame house at 1099 Pacific Street. They bought pricey furniture, and Celia made her husband a special dinner of porterhouse steak, states The Bobbed-Hair Bandit .But they quickly spent their lootand had to commit more robberies to keep up their new higher-end lifestyle. With so much tabloid exposure, the police were under pressure to capture the girl robber. That led cops to arrest and charge a 23-year-old bobbed-haired Brooklyn actress named Helen Quigley for the crimes. Angry that the police had arrested an innocent young woman, Celia left a note for them after she and Ed robbed a Clinton Hill drugstore. The note was addressed to the dirty fish-peddling bums and ordered them to let Helen Quigley gowhich eventually the police did. Celia and Eds stick-up spree finally ended in early spring, after a warehouse worker at the National Biscuit Company on Pacific Street was wounded during a holdup. Panicked, the couple fled, leaving behind $8,000 in an open safe, wrote the New York Times in 2015. A warehouse employee recognized Ed from the neighborhood, and the couple was soon identified. By then, they had taken off for Florida, where Celia gave birth to her daughter on April 12, who sadly died days later. After the couple was arrested and brought back to New York (above, mobbed by crowds at Penn Station), they pleaded guilty and landed 10 to 20 years in prison. Paroled after seven years, the couple went on to have two sons. (Finally free and reunited with their lawyer, above. )Ed died of tuberculosis in 1936. As for Celia, she reportedly was a dutiful and selfless mother, working to support her boys, one of whom became a deacon in the Roman Catholic Church, continued the Times .It was not until a few years before she died in 1992 that her middle-aged sons learned about the Bobbed Hair Bandit. [Top photo: Wikipedia; second image: Newspapers. com blog, Fishwrap ; third image: Brooklyn Eagle ; fourth image: Library of Congress; fifth image: Buffalo Commercial ; sixth image: author collection; seventh image: New York Daily News ; eighth image: Getty Images]This entry was posted on February 11, 2019 at 4:40 am and is filed under Brooklyn , Disasters and crimes . You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response , or trackback from your own site.\n",
      "Sentences: 42\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "The bobbed-hair bandit on the run inBrooklynLike other working-class girls in 1920s Brooklyn, Celia Cooney had big dreams. Celia (at right and below) was a 20-year-old newlywed who toiled in a laundry. She and her husband, Ed, shared a furnished room on Madison Street in a neighborhood then called Bedford, todays Bedford-Stuyvesant. Celia and Ed were very much in love. But like many young couples, they had a hard time saving money. Ed didnt make much as a welder, and Celia enjoyed nice things, like the sealskin fur coat Ed bought for her. So when Celia found out she was pregnant, the Cooneys decided they needed to shore up their finances. How? By committing armed robbery. Thats the genesis of the Bobbed-Haired Bandit, as Celia was dubbed by the press. Together the couple (below, in their wedding photo) would stage holdups of Brooklyn groceries and drugstores and become Roaring Twenties tabloid icons. Their first robbery was at a Roulstons , a grocery chain in Park Slope. On the evening of January 5, the two drove to the store on Seventh Avenue and Seventh Street. Wearing her fur coat, Celia went in first and asked for a dozen eggs, according to the 2005 book, The Bobbed-Hair Bandit , by Stephen Duncombe and Andrew Mattson. As the clerk readied her purchase, Ed entered the store. Celia pulled an automatic out of her pocket, pointed it at the clerk, and yelled, Stick em up, quick! just as the bad guys in the detective stories and pulpy novels she devoured would say. Ed then whipped out a gun in each hand and cleaned out the cash register. The two took off with more than $600. The next day, the brazen heist made by a slight, five-foot woman and her male partner ended up in the Brooklyn Eagle, with the headline Woman With a Gun. Celia and Ed went on to commit several more robberies. The newspapers giddily wrote up each hit, making much of Celias bobbed haira daring style popular with flappers and other women who saw themselves as modern and liberated. Ed was dubbed her tall male companion. After the first robbery, the couple immediately rented a two-story frame house at 1099 Pacific Street. They bought pricey furniture, and Celia made her husband a special dinner of porterhouse steak, states The Bobbed-Hair Bandit .But they quickly spent their lootand had to commit more robberies to keep up their new higher-end lifestyle. With so much tabloid exposure, the police were under pressure to capture the girl robber. That led cops to arrest and charge a 23-year-old bobbed-haired Brooklyn actress named Helen Quigley for the crimes. Angry that the police had arrested an innocent young woman, Celia left a note for them after she and Ed robbed a Clinton Hill drugstore. The note was addressed to the dirty fish-peddling bums and ordered them to let Helen Quigley gowhich eventually the police did. Celia and Eds stick-up spree finally ended in early spring, after a warehouse worker at the National Biscuit Company on Pacific Street was wounded during a holdup. Panicked, the couple fled, leaving behind $8,000 in an open safe, wrote the New York Times in 2015. A warehouse employee recognized Ed from the neighborhood, and the couple was soon identified. By then, they had taken off for Florida, where Celia gave birth to her daughter on April 12, who sadly died days later. After the couple was arrested and brought back to New York (above, mobbed by crowds at Penn Station), they pleaded guilty and landed 10 to 20 years in prison. Paroled after seven years, the couple went on to have two sons. (Finally free and reunited with their lawyer, above. )Ed died of tuberculosis in 1936. As for Celia, she reportedly was a dutiful and selfless mother, working to support her boys, one of whom became a deacon in the Roman Catholic Church, continued the Times. It was not until a few years before she died in 1992 that her middle-aged sons learned about the Bobbed Hair Bandit. [Top photo: Wikipedia; second image: Newspapers. com blog, Fishwrap ; third image: Brooklyn Eagle; fourth image: Library of Congress; fifth image: Buffalo Commercial; sixth image: author collection; seventh image: New York Daily News; eighth image: Getty Images]Share this:\n",
      "Sentences: 41\n",
      "\n",
      "\n",
      "Article: https://www.cbc.ca/news/technology/mars-one-bankrupt-1.5014522\n",
      "NodeRank:\n",
      "Mars One, a Dutch company that planned to send humans on a one-way trip to Mars and start the first human colony on the Red Planet, has been declared bankrupt. CBC News  Posted: Feb 11, 2019 2:25 PM ET | Last Updated: February 12Mars One began accepting applications in 2013 for a mission to establish a permanent settlement on Mars. It planned to launch a total of 24 people in groups of four every two years starting in 2024. (Bryan Versteeg/Canadian Press) commentsMars One, a Dutch company that planned to send humans on a one-way trip to Mars and start the first human colony on the Red Planet, hasbeen declared bankrupt. The declaration was made in a Swiss court on Jan. 15and posted on Reddit this past weekend officially dissolving Mars One Ventures AG. \"We are currently working with the administrator and an investor to find a solution moving forward, though at this moment that is all we can share,\" the company said in an email. The bankruptcy affects the company's commercial arm, Mars One Ventures AG, but does not affect the non-profit Mars One Foundation, the email said. A press release notes the company is \"currently working on a solution with an investor. \"Mars One began accepting applications in 2013 for a mission to establish a permanent settlement on Mars. It planned to launch a total of 24 people in groups of four every two years starting in 2024. The company claimed it could do so using existing technology. It estimated launching the first four people would cost $6 billion US, and said it planned to raise the money through broadcasting rights and sponsorships. Special ReportGetting a crew to Mars: Here's how NASA is tackling the mind-bending to-do listAudioCan Mars One really get off the ground? More than 200,000 people from 100 countries applied, including more than 8,000 Canadians. The company announced a shortlist of 100 people in 2015, including six Canadians. However, experts questioned the plan and pointed out that it had some potentially deadly flaws, and some criticsopenly questioned whether it wasa scam. \"The fact is the money is not there, the technology is not there,\" Elmo Keep, a journalist who has been investigating the company, told CBC Radio'sqthis past November. CBC's Journalistic Standards and Practices|About CBC NewsReport Typo or Error|Send FeedbackSpecial Report Getting a crew to Mars: Here's how NASA is tackling the mind-bending to-do listAudio Can Mars One really get off the ground? Mars One plan has potentially deadly flaws, scientists say Calgary artist Bryan Versteeg behind Mars One imagesAudio Why some Canadians want to die on MarsFind more popular storiesTo encourage thoughtful and respectful conversations, first and last names will appear with each submission to CBC/Radio-Canada's online communities (except in children and youth-oriented communities). Pseudonyms will no longer be permitted. By submitting a comment, you accept that CBC has the right to reproduce and publish that comment in whole or in part, in any manner CBC chooses. Please note that CBC does not endorse the opinions expressed in comments. Comments on this story are moderated according to our Submission Guidelines. Comments are welcome while open. We reserve the right to close comments at any time. More Stories from us\n",
      "Sentences: 26\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "Mars One, which offered 1-way trips to Mars, declared bankruptMars One, a Dutch company that planned to send humans on a one-way trip to Mars and start the first human colony on the Red Planet, has been declared bankrupt. Social Sharing200,000 people applied from around the world, and 6 Canadians made 100-person shortlistCBC News  Posted: Feb 11, 2019 2:25 PM ET | Last Updated: February 12Mars One began accepting applications in 2013 for a mission to establish a permanent settlement on Mars. It planned to launch a total of 24 people in groups of four every two years starting in 2024. (Bryan Versteeg/Canadian Press)Mars One, a Dutch company that planned to send humans on a one-way trip to Mars and start the first human colony on the Red Planet, hasbeen declared bankrupt. The declaration was made in a Swiss court on Jan. 15 and posted on Reddit this past weekend officially dissolving Mars One Ventures AG. \"We are currently working with the administrator and an investor to find a solution moving forward, though at this moment that is all we can share,\" the company said in an email. The bankruptcy affects the company's commercial arm, Mars One Ventures AG, but does not affect the non-profit Mars One Foundation, the email said. A press release notes the company is \"currently working on a solution with an investor. \"Mars One began accepting applications in 2013 for a mission to establish a permanent settlement on Mars. It planned to launch a total of 24 people in groups of four every two years starting in 2024. The company claimed it could do so using existing technology. It estimated launching the first four people would cost $6 billion US, and said it planned to raise the money through broadcasting rights and sponsorships. More than 200,000 people from 100 countries applied, including more than 8,000 Canadians. The company announced a shortlist of 100 people in 2015, including six Canadians .However, experts questioned the plan and pointed out that it had some potentially deadly flaws , and some critics openly questioned whether it wasa scam . \"The fact is the money is not there, the technology is not there,\" Elmo Keep, a journalist who has been investigating the company, told CBC Radio'sqthis past November .To encourage thoughtful and respectful conversations, first and last names will appear with each submission to CBC/Radio-Canada's online communities (except in children and youth-oriented communities). Pseudonyms will no longer be permitted. By submitting a comment, you accept that CBC has the right to reproduce and publish that comment in whole or in part, in any manner CBC chooses. Please note that CBC does not endorse the opinions expressed in comments. Comments on this story are moderated according to our Submission Guidelines . Comments are welcome while open. We reserve the right to close comments at any time.\n",
      "Sentences: 21\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "Mars One, which offered 1-way trips to Mars, declared bankruptSocial SharingMars One, which offered 1-way trips to Mars, declared bankruptMars One, a Dutch company that planned to send humans on a one-way trip to Mars and start the first human colony on the Red Planet, has been declared bankrupt. Social Sharing200,000 people applied from around the world, and 6 Canadians made 100-person shortlistCBC News Posted: Feb 11, 2019 2:25 PM ET | Last Updated: February 12Mars One began accepting applications in 2013 for a mission to establish a permanent settlement on Mars. It planned to launch a total of 24 people in groups of four every two years starting in 2024. (Bryan Versteeg/Canadian Press)\n",
      "Sentences: 4\n",
      "\n",
      "\n",
      "Article: https://medium.com/@shnatsel/how-rusts-standard-library-was-vulnerable-for-years-and-nobody-noticed-aebf0503c3d6\n",
      "NodeRank:\n",
      "Sergey \"Shnatsel\" DavidoffBlockedUnblockFollowFollowingAug 17, 2018Rust is a new systems programming language that prides itself on memory safety and speed. The gist of it is that if you write code in Rust, it goes as fast as C or C++, but you will not get mysterious intermittent crashes in production or horrific security vulnerabilities, unlike in the latter two. That is, until you explicitly opt in to that kind of thing. Uh oh. You see, Rust provides safe abstractions that let you do useful stuff without having to deal with the complexities of memory layouts and other low-level arcana. But dealing with those things is necessary to run code on modern hardware, so something has to deal with it. In memory-safe languages like Python or Go this is usually handled by the language runtimeand Rust is no exception. In Rust, the nutty-gritty of hazardous memory accesses is handled by the standard library. It implements the basic building blocks such as vectors that expose a safe interface to the outside, but perform potentially unsafe operations internally. To do that, they explicitly opt in to potentially unsafe operations (read: barely reproducible crashes, security vulnerabilities) by annotating a block with unsafe, like this: unsafe { Dragons::hatch(); }However, Rust is different from languages like Python or Go in that it lets you use unsafe outside the standard library. On one hand, this means that you can write a library in Rust and call into it from other languages, e.g. Python. Language bindings are unsafe by design, so the ability to write such code in Rust is a major advantage over other memory-safe languages such as Go. On the other hand, this opens the floodgates for judicious use of unsafe. In fact, a couple of months ago a promising library caught some flak for engaging in precisely this sort of thing. So when I was trying to gauge whether Rust actually delivers on its promise of memory safety, thats where I started. Ive messed with popular Rust libraries over the course of a month and then described my findings in Auditing popular Rust crates: how a one-line unsafe has nearly ruined everything. The TL;DR version of it is that Rust crates do sometimes use unsafe when its not absolutely necessary, and bugs that lead to denial of service are abundant, but after poking six different crates I have failed to get an actual exploit. Clearly, I had to kick it up a notch. There is a highly effective technique for discovering vulnerabilities that I havent applied to Rust yet. It beats everything else by a long shot, and can be used only by the bad guys who want to break stuff, not the good guys who fix it. Its searching the bug tracker. You see, most people writing code in C or C++ are not actually security-minded. They just want their code to work and go fast. When they encounter a bug that makes the program output garbage or crash, they simply fix it and go investigate the next bug. What else is there to do? Well, turns out in C and C++ many of those bugs are caused by mistakes in memory management. Its those kinds of bugs that present remote code execution vulnerabilities, and that safe Rust is designed to prevent. The proper way to handle them is to file them into a database called Common Vulnerabilities and Exposures (CVE for short) so that people who care about security are alerted to it and ship fixes to users. In practice such bugs are silently fixed in the next release at best, or remain open for years at worst, until either someone discovers them independently or the bug is caught powering some kind of malware in the wild. This leaves a lot of security vulnerabilities in plain sight on the public bug tracker, neatly documented, just waiting for someone to come along and weaponize them. I particularly like an example of such a bug in libjpeg that was discovered in 2003 but not recognized as a security issue. The patch to fix it ended up in limbo until 2013, at which point it was incorporated into an update so obscure that nobody received it anyway. The fix did not even get a changelog entry. It was independently discovered later in 2013 by Michal Zalewski, author of afl-fuzz, and after 10 years since the vulnerability was discovered the fix has at last shipped. That is, anyone who has bothered to just scroll through the bug tracker could steal cookies and passwords out of your web browser by simply loading an image and a bit of JavaScript for 10 years. Touché. The worst part is, bugs that are already fixed are not eligible for bug bounties. So the Bugtracker Search technique will not get you bug bounty money; it will, however, get you real exploits for production systems. This is why its unrivaled if you want to break stuff, and useless if you want to fix it and not go broke in the process. Also, getting maintainers to take your this is a security vulnerability comments seriously can be problematic, and actually exploiting the bug to prove it can be a lot of work, which further discourages pro bono applications of this technique. Actually applying the Bugtracker Search to Rust code was even easier than I expected. Turns out GitHub lets you search all the projects written in a certain language, so Ive just typed unsound in search query, selected Rust as language and off we go! Bugs, bugs everywhere! I did not have much time to spare at the moment, so typing crash instead of unsound in the search box is left as an exercise for the reader. Also, Ive only searched for open bugs in recently updated projects and ignored the standard library (those guys gotta be responsible, right? ).This got me my first Rust zero-day exploit! It was discovered two months before Ive found it through github search and comes with its own blogpost, albeit focusing on performance. After I pointed out that it is a security vulnerability, the crate maintainer has fixed it within two hours. And then has backported the fix to every affected series even though the crate is still in 0.x.x versions. Kudos! Still, actually exploiting this bug in practice is tricky. It would be a good candidate for exploit chaining, but its hard to use by itself. Okay, that was not ultimate enough. Time to kick it up another notch. At this point were looking for something that is straightforward to exploit (something like buffer overflow with data an attacker can control) and has not been recognized as a security vulnerability yet. It doesnt matter if the bug is fixed in the latest version of the code: people lack incentives to update to the latest version as long as whatever theyre using works for them, and have a very clear incentive to not upgrade because whatever theyre using is known to work well, while the latest update is not. So even if there is an update that fixed the issue, a lot of people will not actually install it, because there is no reason tounless it is marked as a security update. I was contemplating my course of action when Ive accidentally stumbled upon a reddit thread discussing the history of vulnerabilities in the Rust standard library, which pointed out this gem:seg fault pushing on either side of a VecDequehttps://github. com/rust-lang/rust/issues/44800This is a buffer overflow bug in the standard librarys implementation of a double-ended queue. The data written out of bounds is controlled by the attacker. This makes it a good candidate for a remote code execution exploit. The bug affects Rust versions 1.3 to 1.21 inclusive. It is causing a crash that is relatively easy to observe, yet it has gone unnoticed for two years. In the release that fixed it it did not even get a changelog entry. No CVE was filed about this vulnerability. As a result, Debian Stable still ships vulnerable Rust versions for some architectures. I expect many enterprise users to have vulnerable versions as well. As usual, bad guys win. I did not expect to find something like this in the standard library because Rust has a very well thought out and responsible security policy (other projects, take note! ), and the Rust security team consists of people who regularly work on the compiler and standard library. The fix should not have gone unnoticed. I have contacted the Rust security team about the issue, asking them to make an announcement and file a CVE. The reply was:Hey Sergey,This was fixed way back in September; we dont support old Rusts. As such, its not really eligible for this kind of thing at this point, as far as our current policies goes. Ill bring it up at a future core team meeting, just in case. And then, shortly:<snip>We talked about this Wednesday evening.- We do want to change our policy hereThe current policy is that we only support the latest RustThe general feeling is if its important enough for a point release, its important enough for a CVEThis specific patch does seem like it should have gotten more attention at the timeThis stuff also obviously ties into LTS stuff as well- We dont have the time or inclination to work on updating this policy until after the [2018] edition shipsWed rather take the time to get it right, but dont have the time right nowOkay, I have to admit that this sounds reasonable. They have subsequently reaffirmed that they have no intention to file a CVE for this issue, so I went ahead and applied for one myself via http://iwantacve. org/. This is supposed to involve a confirmation by email, and I am yet to hear back. I have no clue how long this will take. Update: this issue has been assigned CVE-2018-1000657. This exposes a bigger issue with the standard library: insufficient verification. If this bugwhich is relatively easy to observe! has gone unnoticed for two years, surely something like it is still lurking in the depths of the standard library? This problem is not unique to Rust. For example, Erlangthat funky language that people use to program systems with 99,9999999% uptime (no, thats not an exaggeration)has repeatedly shipped with a broken implementation of Map data structure in its standard library. There is a fascinating series of four articles detailing a systemic approach used to discover those issues. To actually deliver on the safety guarantees, Rust standard library needs dramatically better testing and verification procedures. Some of its primitives were mathematically proven to be correct as part of RustBelt project, but that did not extend to implementations of data structures. One way to do that would be to use the same approach as was used for verifying the Map structure in Erlangbuilding a model of the behavior of the structure in question and automatically generating tests based on it, then verifying that the outputs of the model and the implementation match for certain automatically generated inputs. Rust already has the tooling for that in the form of QuickCheck and proptest. Another way to verify the implementations is to use a symbolic execution framework such as KLEE or SAW. They work by analyzing the code and figuring out all possible program states for all possible execution paths. This lets you either generate inputs that trigger faulty behavior or make sure that certain behavior is impossible. Sadly, neither of those tools supports recent versions of Rust. Alas, both of those approaches are time-consuming and would require coordinated effort. Its not something one can do for the entire standard library over a couple of weekendsotherwise Id be opening a pull request for Rust standard library by now instead of writing this article. Oh, and before you bring out the pitchforks and denounce Rust for all eternity: for reference, Python runtime gets at about 5 remote code execution vulnerabilities per year. And thats just the already discovered ones that got a CVE! How many were silently fixed or still lurk in the depths of Python runtime? Only the bad guys know. I have once reported a buffer overflow in a popular C library that is used in one of the major web browsers. It was the textbook example of a security vulnerability, and could be triggered simply by opening a webpage. I was told that the bug was silently fixed in a subsequent release that nobody has upgraded to yet. When I asked the maintainers to file a CVE, they said that if they filed one for every such bug they fixed theyd never get any actual work done. Oh, and the worst thing? The vulnerability Ive reported in that library was found by a fully automated tool in less than a day. All I did to discover the vulnerability was basically point and click. Imagine how many more exploitable bugs a dedicated security expert could discover! This was when I have actually understood and internalized that everything isbroken. The horrifying thing for me is that I still use that web browser. Its not like I have any alternativesevery practical web browser relies on a huge mess of C code. And it is evident that humans are unable to write secure C code, unless they swear off dynamic memory allocation altogether. This is why Im so hopeful about Rust. It is the only language in existence that could really, truly, completely and utterly supplant C and C++ while providing memory safety. There is a mathematical proof of correctness for a practical subset of safe Rust and even some inherently unsafe standard library primitives, and ongoing work on expanding it to cover even more of the language. So we know that safe Rust actually works. The really hard theoretical problems are solved. But the inherently unsafe parts of the implementation, such as the language runtime, could use more attention.\n",
      "Sentences: 120\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "How Rusts standard library was vulnerable for years and nobodynoticedRust is a new systems programming language that prides itself on memory safety and speed. The gist of it is that if you write code in Rust, it goes as fast as C or C++, but you will not get mysterious intermittent crashes in production or horrific security vulnerabilities, unlike in the latter two. That is, until you explicitly opt in to that kind of thing. Uh oh. Wait, what? You see, Rust provides safe abstractions that let you do useful stuff without having to deal with the complexities of memory layouts and other low-level arcana. But dealing with those things is necessary to run code on modern hardware, so something has to deal with it. In memory-safe languages like Python or Go this is usually handled by the language runtimeand Rust is no exception. In Rust, the nutty-gritty of hazardous memory accesses is handled by the standard library. It implements the basic building blocks such as vectors that expose a safe interface to the outside, but perform potentially unsafe operations internally. To do that, they explicitly opt in to potentially unsafe operations (read: barely reproducible crashes, security vulnerabilities) by annotating a block with unsafe , like this: unsafe { Dragons::hatch(); }However, Rust is different from languages like Python or Go in that it lets you use unsafe outside the standard library. On one hand, this means that you can write a library in Rust and call into it from other languages, e.g. Python. Language bindings are unsafe by design, so the ability to write such code in Rust is a major advantage over other memory-safe languages such as Go. On the other hand, this opens the floodgates for judicious use of unsafe . In fact, a couple of months ago a promising library caught some flak for engaging in precisely this sort of thing . So when I was trying to gauge whether Rust actually delivers on its promise of memory safety, thats where I started. Ive messed with popular Rust libraries over the course of a month and then described my findings in Auditing popular Rust crates: how a one-line unsafe has nearly ruined everything . The TL;DR version of it is that Rust crates do sometimes use unsafe when its not absolutely necessary, and bugs that lead to denial of service are abundant, but after poking six different crates I have failed to get an actual exploit. Clearly, I had to kick it up a notch. Ultimate Power, Bad GuysOnlyThere is a highly effective technique for discovering vulnerabilities that I havent applied to Rust yet. It beats everything else by a long shot, and can be used only by the bad guys who want to break stuff, not the good guys who fix it. Its searching the bug tracker. You see, most people writing code in C or C++ are not actually security-minded. They just want their code to work and go fast. When they encounter a bug that makes the program output garbage or crash, they simply fix it and go investigate the next bug. What else is there to do? Well, turns out in C and C++ many of those bugs are caused by mistakes in memory management. Its those kinds of bugs that present remote code execution vulnerabilities, and that safe Rust is designed to prevent. The proper way to handle them is to file them into a database called Common Vulnerabilities and Exposures (CVE for short) so that people who care about security are alerted to it and ship fixes to users. In practice such bugs are silently fixed in the next release at best, or remain open for years at worst, until either someone discovers them independently or the bug is caught powering some kind of malware in the wild. This leaves a lot of security vulnerabilities in plain sight on the public bug tracker, neatly documented, just waiting for someone to come along and weaponize them. I particularly like an example of such a bug in libjpeg that was discovered in 2003 but not recognized as a security issue. The patch to fix it ended up in limbo until 2013, at which point it was incorporated into an update so obscure that nobody received it anyway. The fix did not even get a changelog entry. It was independently discovered later in 2013 by Michal Zalewski, author of afl-fuzz , and after 10 years since the vulnerability was discovered the fix has at last shipped. That is, anyone who has bothered to just scroll through the bug tracker could steal cookies and passwords out of your web browser by simply loading an image and a bit of JavaScript for 10 years. Touché. The worst part is, bugs that are already fixed are not eligible for bug bounties. So the Bugtracker Search technique will not get you bug bounty money; it will, however, get you real exploits for production systems. This is why its unrivaled if you want to break stuff, and useless if you want to fix it and not go broke in the process. Also, getting maintainers to take your this is a security vulnerability comments seriously can be problematic, and actually exploiting the bug to prove it can be a lot of work, which further discourages pro bono applications of this technique. Into thewoodsActually applying the Bugtracker Search to Rust code was even easier than I expected. Turns out GitHub lets you search all the projects written in a certain language, so Ive just typed unsound in search query, selected Rust as language and off we go ! Bugs, bugs everywhere! I did not have much time to spare at the moment, so typing crash instead of unsound in the search box is left as an exercise for the reader. Also, Ive only searched for open bugs in recently updated projects and ignored the standard library (those guys gotta be responsible, right? ).This got me my first Rust zero-day exploit ! It was discovered two months before Ive found it through github search and comes with its own blogpost, albeit focusing on performance. After I pointed out that it is a security vulnerability, the crate maintainer has fixed it within two hours. And then has backported the fix to every affected series even though the crate is still in 0.x.x versions. Kudos! Still, actually exploiting this bug in practice is tricky. It would be a good candidate for exploit chaining, but its hard to use by itself. Okay, that was not ultimate enough. Time to kick it up another notch. In the belly of thebeastAt this point were looking for something that is straightforward to exploit (something like buffer overflow with data an attacker can control) and has not been recognized as a security vulnerability yet. It doesnt matter if the bug is fixed in the latest version of the code: people lack incentives to update to the latest version as long as whatever theyre using works for them, and have a very clear incentive to not upgrade because whatever theyre using is known to work well, while the latest update is not. So even if there is an update that fixed the issue, a lot of people will not actually install it, because there is no reason to unless it is marked as a security update. I was contemplating my course of action when Ive accidentally stumbled upon a reddit thread discussing the history of vulnerabilities in the Rust standard library, which pointed out this gem: seg fault pushing on either side of a VecDeque https://github. com/rust-lang/rust/issues/44800This is a buffer overflow bug in the standard librarys implementation of a double-ended queue . The data written out of bounds is controlled by the attacker. This makes it a good candidate for a remote code execution exploit. The bug affects Rust versions 1.3 to 1.21 inclusive. It is causing a crash that is relatively easy to observe, yet it has gone unnoticed for two years. In the release that fixed it it did not even get a changelog entry. No CVE was filed about this vulnerability. As a result, Debian Stable still ships vulnerable Rust versions for some architectures. I expect many enterprise users to have vulnerable versions as well. As usual, bad guys win. Whooops! I did not expect to find something like this in the standard library because Rust has a very well thought out and responsible security policy (other projects, take note! ), and the Rust security team consists of people who regularly work on the compiler and standard library. The fix should not have gone unnoticed. I have contacted the Rust security team about the issue, asking them to make an announcement and file a CVE. The reply was: Hey Sergey, This was fixed way back in September; we dont support old Rusts. As such, its not really eligible for this kind of thing at this point, as far as our current policies goes. Ill bring it up at a future core team meeting, just in case. And then, shortly: <snip> We talked about this Wednesday evening. - We do want to change our policy here The current policy is that we only support the latest Rust The general feeling is if its important enough for a point release, its important enough for a CVE This specific patch does seem like it should have gotten more attention at the time This stuff also obviously ties into LTS stuff as well - We dont have the time or inclination to work on updating this policy until after the [2018] edition ships Wed rather take the time to get it right, but dont have the time right nowOkay, I have to admit that this sounds reasonable. They have subsequently reaffirmed that they have no intention to file a CVE for this issue, so I went ahead and applied for one myself via http://iwantacve. org/ . This is supposed to involve a confirmation by email, and I am yet to hear back. I have no clue how long this will take. Update: this issue has been assigned CVE-2018-1000657 .Bugs, bugs everywhere! This exposes a bigger issue with the standard library: insufficient verification. If this bugwhich is relatively easy to observe! has gone unnoticed for two years, surely something like it is still lurking in the depths of the standard library? This problem is not unique to Rust. For example, Erlangthat funky language that people use to program systems with 99,9999999% uptime ( no, thats not an exaggeration )has repeatedly shipped with a broken implementation of Map data structure in its standard library. There is a fascinating series of four articles detailing a systemic approach used to discover those issues. To actually deliver on the safety guarantees, Rust standard library needs dramatically better testing and verification procedures. Some of its primitives were mathematically proven to be correct as part of RustBelt project, but that did not extend to implementations of data structures. One way to do that would be to use the same approach as was used for verifying the Map structure in Erlangbuilding a model of the behavior of the structure in question and automatically generating tests based on it, then verifying that the outputs of the model and the implementation match for certain automatically generated inputs. Rust already has the tooling for that in the form of QuickCheck and proptest .Another way to verify the implementations is to use a symbolic execution framework such as KLEE or SAW . They work by analyzing the code and figuring out all possible program states for all possible execution paths. This lets you either generate inputs that trigger faulty behavior or make sure that certain behavior is impossible. Sadly, neither of those tools supports recent versions of Rust. Alas, both of those approaches are time-consuming and would require coordinated effort. Its not something one can do for the entire standard library over a couple of weekendsotherwise Id be opening a pull request for Rust standard library by now instead of writing this article. Oh, and before you bring out the pitchforks and denounce Rust for all eternity: for reference, Python runtime gets at about 5 remote code execution vulnerabilities per year . And thats just the already discovered ones that got a CVE! How many were silently fixed or still lurk in the depths of Python runtime? Only the bad guys know. Everything isbrokenI have once reported a buffer overflow in a popular C library that is used in one of the major web browsers. It was the textbook example of a security vulnerability, and could be triggered simply by opening a webpage. I was told that the bug was silently fixed in a subsequent release that nobody has upgraded to yet. When I asked the maintainers to file a CVE, they said that if they filed one for every such bug they fixed theyd never get any actual work done. Oh, and the worst thing? The vulnerability Ive reported in that library was found by a fully automated tool in less than a day. All I did to discover the vulnerability was basically point and click. Imagine how many more exploitable bugs a dedicated security expert could discover! This was when I have actually understood and internalized that everything isbroken .The horrifying thing for me is that I still use that web browser. Its not like I have any alternativesevery practical web browser relies on a huge mess of C code. And it is evident that humans are unable to write secure C code, unless they swear off dynamic memory allocation altogether .This is why Im so hopeful about Rust. It is the only language in existence that could really, truly, completely and utterly supplant C and C++ while providing memory safety. There is a mathematical proof of correctness for a practical subset of safe Rust and even some inherently unsafe standard library primitives, and ongoing work on expanding it to cover even more of the language. So we know that safe Rust actually works. The really hard theoretical problems are solved. But the inherently unsafe parts of the implementation, such as the language runtime, could use more attention.\n",
      "Sentences: 120\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "FollowFollowingAug 17, 2018Rust is a new systems programming language that prides itself on memory safety and speed. The gist of it is that if you write code in Rust, it goes as fast as C or C++, but you will not get mysterious intermittent crashes in production or horrific security vulnerabilities, unlike in the latter two. That is, until you explicitly opt in to that kind of thing. Uh oh. Wait, what? You see, Rust provides safe abstractions that let you do useful stuff without having to deal with the complexities of memory layouts and other low-level arcana. But dealing with those things is necessary to run code on modern hardware, so something has to deal with it. In memory-safe languages like Python or Go this is usually handled by the language runtime  and Rust is no exception. In Rust, the nutty-gritty of hazardous memory accesses is handled by the standard library. It implements the basic building blocks such as vectors that expose a safe interface to the outside, but perform potentially unsafe operations internally. To do that, they explicitly opt in to potentially unsafe operations (read: barely reproducible crashes, security vulnerabilities) by annotating a block with unsafe, like this: unsafe { Dragons::hatch(); }However, Rust is different from languages like Python or Go in that it lets you use unsafe outside the standard library. On one hand, this means that you can write a library in Rust and call into it from other languages, e.g. Python. Language bindings are unsafe by design, so the ability to write such code in Rust is a major advantage over other memory-safe languages such as Go. On the other hand, this opens the floodgates for judicious use of unsafe. In fact, a couple of months ago a promising library caught some flak for engaging in precisely this sort of thing . So when I was trying to gauge whether Rust actually delivers on its promise of memory safety, thats where I started. Ive messed with popular Rust libraries over the course of a month and then described my findings in Auditing popular Rust crates: how a one-line unsafe has nearly ruined everything . The TL;DR version of it is that Rust crates do sometimes use unsafe when its not absolutely necessary, and bugs that lead to denial of service are abundant, but after poking six different crates I have failed to get an actual exploit. Clearly, I had to kick it up a notch. Ultimate Power, Bad GuysOnlyThere is a highly effective technique for discovering vulnerabilities that I havent applied to Rust yet. It beats everything else by a long shot, and can be used only by the bad guys who want to break stuff, not the good guys who fix it. Its searching the bug tracker. You see, most people writing code in C or C++ are not actually security-minded. They just want their code to work and go fast. When they encounter a bug that makes the program output garbage or crash, they simply fix it and go investigate the next bug. What else is there to do? Well, turns out in C and C++ many of those bugs are caused by mistakes in memory management. Its those kinds of bugs that present remote code execution vulnerabilities, and that safe Rust is designed to prevent. The proper way to handle them is to file them into a database called Common Vulnerabilities and Exposures (CVE for short) so that people who care about security are alerted to it and ship fixes to users. In practice such bugs are silently fixed in the next release at best, or remain open for years at worst, until either someone discovers them independently or the bug is caught powering some kind of malware in the wild. This leaves a lot of security vulnerabilities in plain sight on the public bug tracker, neatly documented, just waiting for someone to come along and weaponize them. I particularly like an example of such a bug in libjpeg that was discovered in 2003 but not recognized as a security issue. The patch to fix it ended up in limbo until 2013, at which point it was incorporated into an update so obscure that nobody received it anyway. The fix did not even get a changelog entry. It was independently discovered later in 2013 by Michal Zalewski, author of afl-fuzz , and after 10 years since the vulnerability was discovered the fix has at last shipped. That is, anyone who has bothered to just scroll through the bug tracker could steal cookies and passwords out of your web browser by simply loading an image and a bit of JavaScript for 10 years. Touché. The worst part is, bugs that are already fixed are not eligible for bug bounties. So the Bugtracker Search technique will not get you bug bounty money; it will, however, get you real exploits for production systems. This is why its unrivaled if you want to break stuff, and useless if you want to fix it and not go broke in the process. Also, getting maintainers to take your this is a security vulnerability comments seriously can be problematic, and actually exploiting the bug to prove it can be a lot of work, which further discourages pro bono applications of this technique. Into thewoodsActually applying the Bugtracker Search to Rust code was even easier than I expected. Turns out GitHub lets you search all the projects written in a certain language, so Ive just typed unsound in search query, selected Rust as language and off we go ! Bugs, bugs everywhere! I did not have much time to spare at the moment, so typing crash instead of unsound in the search box is left as an exercise for the reader. Also, Ive only searched for open bugs in recently updated projects and ignored the standard library (those guys gotta be responsible, right? ).This got me my first Rust zero-day exploit ! It was discovered two months before Ive found it through github search and comes with its own blogpost, albeit focusing on performance. After I pointed out that it is a security vulnerability, the crate maintainer has fixed it within two hours. And then has backported the fix to every affected series even though the crate is still in 0.x.x versions. Kudos! Still, actually exploiting this bug in practice is tricky. It would be a good candidate for exploit chaining, but its hard to use by itself. Okay, that was not ultimate enough. Time to kick it up another notch. In the belly of thebeastAt this point were looking for something that is straightforward to exploit (something like buffer overflow with data an attacker can control) and has not been recognized as a security vulnerability yet. It doesnt matter if the bug is fixed in the latest version of the code: people lack incentives to update to the latest version as long as whatever theyre using works for them, and have a very clear incentive to not upgrade because whatever theyre using is known to work well, while the latest update is not. So even if there is an update that fixed the issue, a lot of people will not actually install it, because there is no reason to  unless it is marked as a security update. I was contemplating my course of action when Ive accidentally stumbled upon a reddit thread discussing the history of vulnerabilities in the Rust standard library, which pointed out this gem:seg fault pushing on either side of a VecDequehttps://github. com/rust-lang/rust/issues/44800This is a buffer overflow bug in the standard librarys implementation of a double-ended queue . The data written out of bounds is controlled by the attacker. This makes it a good candidate for a remote code execution exploit. The bug affects Rust versions 1.3 to 1.21 inclusive. It is causing a crash that is relatively easy to observe, yet it has gone unnoticed for two years. In the release that fixed it it did not even get a changelog entry. No CVE was filed about this vulnerability. As a result, Debian Stable still ships vulnerable Rust versions for some architectures. I expect many enterprise users to have vulnerable versions as well. As usual, bad guys win. Whooops! I did not expect to find something like this in the standard library because Rust has a very well thought out and responsible security policy (other projects, take note! ), and the Rust security team consists of people who regularly work on the compiler and standard library. The fix should not have gone unnoticed. I have contacted the Rust security team about the issue, asking them to make an announcement and file a CVE. The reply was:Hey Sergey,This was fixed way back in September; we dont support old Rusts. As such, its not really eligible for this kind of thing at this point, as far as our current policies goes. Ill bring it up at a future core team meeting, just in case. And then, shortly:We talked about this Wednesday evening.- We do want to change our policy here The current policy is that we only support the latest Rust The general feeling is if its important enough for a point release, its important enough for a CVE This specific patch does seem like it should have gotten more attention at the time This stuff also obviously ties into LTS stuff as well- We dont have the time or inclination to work on updating this policy until after the [2018] edition ships Wed rather take the time to get it right, but dont have the time right nowOkay, I have to admit that this sounds reasonable. They have subsequently reaffirmed that they have no intention to file a CVE for this issue, so I went ahead and applied for one myself via http://iwantacve. org/ . This is supposed to involve a confirmation by email, and I am yet to hear back. I have no clue how long this will take. Update: this issue has been assigned CVE-2018-1000657 .Bugs, bugs everywhere! This exposes a bigger issue with the standard library: insufficient verification. If this bug  which is relatively easy to observe! has gone unnoticed for two years, surely something like it is still lurking in the depths of the standard library? This problem is not unique to Rust. For example, Erlang  that funky language that people use to program systems with 99,9999999% uptime ( no, thats not an exaggeration )  has repeatedly shipped with a broken implementation of Map data structure in its standard library. There is a fascinating series of four articles detailing a systemic approach used to discover those issues. To actually deliver on the safety guarantees, Rust standard library needs dramatically better testing and verification procedures. Some of its primitives were mathematically proven to be correct as part of RustBelt project, but that did not extend to implementations of data structures. One way to do that would be to use the same approach as was used for verifying the Map structure in Erlang  building a model of the behavior of the structure in question and automatically generating tests based on it, then verifying that the outputs of the model and the implementation match for certain automatically generated inputs. Rust already has the tooling for that in the form of QuickCheck and proptest .Another way to verify the implementations is to use a symbolic execution framework such as KLEE or SAW . They work by analyzing the code and figuring out all possible program states for all possible execution paths. This lets you either generate inputs that trigger faulty behavior or make sure that certain behavior is impossible. Sadly, neither of those tools supports recent versions of Rust. Alas, both of those approaches are time-consuming and would require coordinated effort. Its not something one can do for the entire standard library over a couple of weekends  otherwise Id be opening a pull request for Rust standard library by now instead of writing this article. Oh, and before you bring out the pitchforks and denounce Rust for all eternity: for reference, Python runtime gets at about 5 remote code execution vulnerabilities per year . And thats just the already discovered ones that got a CVE! How many were silently fixed or still lurk in the depths of Python runtime? Only the bad guys know. Everything isbrokenI have once reported a buffer overflow in a popular C library that is used in one of the major web browsers. It was the textbook example of a security vulnerability, and could be triggered simply by opening a webpage. I was told that the bug was silently fixed in a subsequent release that nobody has upgraded to yet. When I asked the maintainers to file a CVE, they said that if they filed one for every such bug they fixed theyd never get any actual work done. Oh, and the worst thing? The vulnerability Ive reported in that library was found by a fully automated tool in less than a day. All I did to discover the vulnerability was basically point and click. Imagine how many more exploitable bugs a dedicated security expert could discover! This was when I have actually understood and internalized that everything isbroken .The horrifying thing for me is that I still use that web browser. Its not like I have any alternatives  every practical web browser relies on a huge mess of C code. And it is evident that humans are unable to write secure C code, unless they swear off dynamic memory allocation altogether .This is why Im so hopeful about Rust. It is the only language in existence that could really, truly, completely and utterly supplant C and C++ while providing memory safety. There is a mathematical proof of correctness for a practical subset of safe Rust and even some inherently unsafe standard library primitives, and ongoing work on expanding it to cover even more of the language. So we know that safe Rust actually works. The really hard theoretical problems are solved. But the inherently unsafe parts of the implementation, such as the language runtime, could use more attention. Update: Brian Troutwine has kicked off a project to validate Rust standard library primitives using QuickCheck! Check out bughunt-rust on GitHub , and join the hunt!\n",
      "Sentences: 121\n",
      "\n",
      "\n",
      "Article: https://opensource.zalando.com/blog/2019/02/Open-Source-Harassment-Policy/\n",
      "NodeRank:\n",
      "All Posts2019FebruaryDefining a company policy to handle harassment in open sourceJanuaryNakadi Goes to FOSDEM. December Review -  Patroni, Machine Learning Meetup and more. 2018DecemberNovember Review - Maintainer training, new releases and more. NovemberZalando Postgres Operator: One Year LaterOctober Review - Hacktoberfest, new releases and more. Connexion 2.0 ReleaseSeptemberZalando Strengthens Its InnerSource StrategyAugustZalando Open Source Team welcomes the new InnerSource ManagerFebruaryZalando @ FOSDEMJanuaryThe faces behind the FASHION-MNIST2017OctoberA Plea For Small Pull RequestsAugustInnerSource Do's and Don'ts out of Dortmund\n",
      "Sentences: 5\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "When you as a Zalando employee engage in open source communities as part of your work, you will interact with the wider open source communities outside Zalando - this is generally a good experience and collaborating with many different types of developers with different backgrounds is generally a positive input to your personal development. However, there is also a small risk of encountering negative or even abusive behavior from community members when you act as an open source contributor or maintainer. As an employer encouraging open source participation, we have decided to devise a policy for how we as a company can support our employees in case of harassment. Statistics on harassment in open sourceAn extensive survey by Github in 2017 showed that nearly one out of five have experienced negative behavior personally and 50% have witnessed it between other people - fortunately outright harassment is much less likely with 14% witnessing it and 3% experiencing it personally. Witnessing and experiencing behavior such as name-calling, stereotyping and outright harassment can have a big negative impact on peoples desire to be part of open source communities, especially for women or ethnic or sexual minorities who are already underrepresented in the open source world (3% female, 16% ethnic minority, 7% sexual minority).So, the open source community see an underrepresentation of minorities and those who do participate have a risk of encountering hostile behavior. Is the risk of harassment big? No - generally speaking the risk is low, but the impact of potential harassment is very real. As an industry we must prioritize the topic of diversity in open source, abusive behavior should not be tolerated, and in the case of it happening, companies should be ready to support their employees in dealing with it. Supporting employee participationAs an employer, Zalando encourages its employees to take active part in open source development. Developers are granted time to maintain the projects we release and to contribute upstream to projects which are of strategic importance to Zalando. We as a company therefore have an obligation to ensure that we support employees who engage in open source on Zalandos behalf. Support isnt just about granting time and resources for open source development, support is also understanding the potential risk employees face doing open source and to be ready to offer legal and HR guidance and understanding to employees in the event of harassment. It is with this mindset that we have put together a formal policy for dealing with harassment in open source for our maintainers and contributors, a policy which employees can use to determine where inside Zalando they can find help to deal with such behavior and also to clarify what they can expect from Legal and HR. The policyWe have divided the policy into 2 parts: proactive and reactive measures. First of all: proactively we recommend that employees only engage with projects who have a code of conduct in place, we also enforce that all new projects released by Zalando have a code of conduct in place as part of the boilerplate files we provide. As part of our internal mandatory training for open source maintainers and during on-boarding of new employees we also make our expectations very clear: in case of behavior in breach of the code of conduct, it is expected you enforce the code or ask the open source team for help on how to act. Secondly, if an employee do need guidance we reactively provide the following options: P&O (Zalando HR) can guide you on how to react to abusive behavior and help you determine if legal action is required. Talk to your lead if you need assistance, or reach out directly to the open source team. The open source team will assist you in reporting the abuse to the responsible platform owner (such as Github) Zalando legal will provide legal guidance in case such is required If it is established there is a need to report the incident to law enforcement, P&O (Zalando HR) and Zalando legal will assist you in collecting evidence and file a reportA small step forwardWhile policies will never solve the root cause, it is a step in the right direction. We believe in equal opportunity and access to the world of open source. We believe open source is important, not just to tech companies, but to society as a whole and we must all do what we can to ensure that the communities building the software that we all rely on is inclusive and safe for everyone to be part of.\n",
      "Sentences: 21\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "Defining a company policy to handle harassment in open sourcePer Ploug, 04 February 2019When you as a Zalando employee engage in open source communities as part of your work, you will interact with the wider open source communities outside Zalando - this is generally a good experience and collaborating with many different types of developers with different backgrounds is generally a positive input to your personal development. However, there is also a small risk of encountering negative or even abusive behavior from community members when you act as an open source contributor or maintainer. As an employer encouraging open source participation, we have decided to devise a policy for how we as a company can support our employees in case of harassment. Statistics on harassment in open sourceAn extensive survey by Github in 2017 showed that nearly one out of five have experienced negative behavior personally and 50% have witnessed it between other people - fortunately outright harassment is much less likely with 14% witnessing it and 3% experiencing it personally. Witnessing and experiencing behavior such as name-calling, stereotyping and outright harassment can have a big negative impact on peoples desire to be part of open source communities, especially for women or ethnic or sexual minorities who are already underrepresented in the open source world (3% female, 16% ethnic minority, 7% sexual minority).So, the open source community see an underrepresentation of minorities and those who do participate have a risk of encountering hostile behavior. Is the risk of harassment big? No - generally speaking the risk is low, but the impact of potential harassment is very real. As an industry we must prioritize the topic of diversity in open source, abusive behavior should not be tolerated, and in the case of it happening, companies should be ready to support their employees in dealing with it. Supporting employee participationAs an employer, Zalando encourages its employees to take active part in open source development. Developers are granted time to maintain the projects we release and to contribute upstream to projects which are of strategic importance to Zalando. We as a company therefore have an obligation to ensure that we support employees who engage in open source on Zalandos behalf. Support isnât just about granting time and resources for open source development, support is also understanding the potential risk employees face doing open source and to be ready to offer legal and HR guidance and understanding to employees in the  event of harassment. It is with this mindset that we have put together a formal policy for dealing with harassment in open source for our maintainers and contributors, a policy which employees can use to determine where inside Zalando they can find help to deal with such behavior and also to clarify what they can expect from Legal and HR. The policyWe have divided the policy into 2 parts: proactive and reactive measures. First of all: proactively we recommend that employees only engage with projects who have a code of conduct in place, we also enforce that all new projects released by Zalando have a code of conduct in place as part of the boilerplate files we provide. As part of our internal mandatory training for open source maintainers and during on-boarding of new employees we also make our expectations very clear: in case of behavior in breach of the code of conduct, it is expected you enforce the code or ask the open source team for help on how to act. Secondly, if an employee do need guidance we reactively provide the following options:P&O (Zalando HR) can guide you on how to react to abusive behavior and help you determine if legal action is required. Talk to your lead if you need assistance, or reach out directly to the open source team. The open source team will assist you in reporting the abuse to the responsible platform owner (such as Github)Zalando legal will provide legal guidance in case such is requiredIf it is established there is a need to report the incident to law enforcement, P&O (Zalando HR) and Zalando legal will assist you in collecting evidence and file a reportA small step forwardWhile policies will never solve the root cause, it is a step in the right direction. We believe in equal opportunity and access to the world of open source. We believe open source is important, not just to tech companies, but to society as a whole and we must all do what we can to ensure that the communities building the software that we all rely on is inclusive and safe for everyone to be part of. Share\n",
      "Sentences: 22\n",
      "\n",
      "\n",
      "Article: https://blog.parse.ly/post/7689/analyst-demystify-traffic-google-sends-publishers/\n",
      "NodeRank:\n",
      "Featured  Featured  by Kelsey Arendt September 12, 2018I really love it when one of my go-to news sources sneaks its way into my periphery with a new way of telling a storya new podcast or app, or really any rich new content format. As an Android user, Ive been noticing Googles finding all sorts of ways to sneak in more recommended content, and not just in search results. I started investigating Googles referring urls. Those recommended content elements I was noticing on my phone? Theyre all from the same referring url: Google Quick Searchbox, or GQSB. So what exactly is GQSB and how does its growth compare to Google News? Whats the deal with googleapis since my last analysis? And most importantly, how can publishers identify and gain exposure on these products? (Spoiler alert: Prepare for another discussion about AMP. )As Google optimizes its content-based products, I wanted to understand whats driving traffic and what you should do to get on board. Heres where my analysis took me. View related pages:From a user perspective, sometimes it felt obvious why I was being served certain contentI searched for something, read it, and scrolling up ever-so-slightly on the page, was served very relevant related articles. I noticed that sometimes the recommended content was from the same source, and sometimes I got a variety. I also noticed that this didnt ever happen in the Chrome app; it was in Googles weird little webview, the purgatorial browser between search and site. Check out your feed: Then Google suggested Check out your feed, based on past search queries. Left of home feed: which got me thinking about my beloved left-of-home feedso perfectly curated and relevant, as if Google has known me for years. So much content! So many ways to get it! How can I track this tapestry of Google traffic? Sometimes querying Googles referring urls feels a bit like talking to an oracle: never explicit, sifting through circular gibberish. Imagine my surprise when I found that the three content recommendation types that caught my eye all send the same referrer: android-app://com. google. android. googlequicksearchbox (or some variation thereof).This referring URL also made a brief appearance in 2016, and then quickly disappeared as the traffic returned to direct. But this time, these arent from the search box! For the sake of this article, Im going to call it by its old nickname, GQSB. GQSB was previously understood to be only organic search from the search bar widget on mobile devices, but now its a blend of search box and recommended content. That includes, but maybe is not limited to:View Related Pages. This appears in a Google webviewnot Chromeon both AMP and non-AMP pages. Check out your feed at the bottom of search inputs. This takes you to interest-based content cards. Left-of-home swipe cards. (Note: These cards arent related to the googleapis referrer, and weve clarified that in this post. )It would be easy to lump all of this together as search traffic because a lot of it is still a click from the actual search box. But its indistinguishable from the url sent by what is clearly recommended, not-searched-for, content. That is: Im not actively searching for a particular topic; Im being recommended content based on my past search history, or at least based on what Im currently reading. And heres why we should care about it: GQSB recs have dwarfed all our other Google content recs. (And that spike in May is all AMP. Color me shocked. )So if GQSB is the referring domain sending the most traffic, whats going on with Google News and googleapis? While GQSB recommends content, its not an aggregator in the same way that Google News and googleapis are. So while GQSB is sending more traffic, Google News takes the title of fastest growing Google content recommender thanks to a change made in May. Up until then, Google recommended content across three main products:Google News (news. google. com)Google Play Newsstand (play. google. com)in-app Chrome recommendations (googleapis. com)In May 2018, Google merged their Play Newsstand app with Google News, creating a single, streamlined news aggregator for desktop, mobile, and app. It makes sense that Google should merge these two practically identical products. In mid-2017, Google News was quietly losing traffic. Meanwhile, Google Newsstand was slowly but surely inching upwards. Since merging, Google News is the fastest growing Google content recommender, growing at a faster rate than googleapis. Today, Google News is the largest and fastest growing Google news aggregator, with 60% mobile traffic. Oh, and about that mobile traffic: it is 85% AMP, and growing. In May 2018, Google News traffic jumped up 25. 7% from the previous month. Over the summer months, Google News average month-over-month percentage growth has remained in the double-digits. Meanwhile, googleapis has stalled to single-digit 2.4% growth. Merging Newsstand brought a dormant Google News back to life. Needless to say, May to August 2018 has been an interesting period for Google content recommendations. It may be hasty to say whether the rate of change will continue, but worth watching. Not only does de-duplicating similar products just make sense, but merging products indicates Googles investment in growing the Google News mobile offering. So lets unpack the growth that happened post-merger. While the uptick in Google News mobile referrers (after merging a mobile app) is expected, mobile traffic took off. In January 2018, Google News was 40% mobile traffic. Now its nearly 60% mobile. This indicates that not only does Google News prefer AMP content, it may be deprioritizing non-AMP content (or if its not actively deprioritizing it, the content is at least not getting traffic anymore, which has a similar outcome for publishers).Google News growth is all mobile, and Google News mobile growth is all AMP (similarly to Google Search), yet its desktop traffic remains steady. While Google News on desktop remains a reliable source of traffic, we cant say the same for non-AMP traffic, which takes a hit. If this downward trend in non-AMP Google News traffic continues, then you could argue that in a couple months, if youre not on AMP, then youre not on (mobile) Google News. Maybe Google News mobile growth will stabilize itself on top, and Google News across all devices will settle back in as a steady-as-she-goes referral source. And before I start humming along to The Raconteurs, its worth noting: it doesnt matter if I report on Google News being the next big thing if, by not using AMP, youre not able to take advantage of its growth. Googleapis deserves a shout-out here for exactly this reason. Its still split between AMP vs. non-AMP traffic, which means publishers dont have the same pressure to be on AMP in the case of googleapis. However, dont hold your breath on that trend staying the same if the other recommending services are an indication. As a user, I love Google News, and apparently Im not alone. I appreciate that Google combined two very similar products that I didnt really need two of. And as a media analyst, Im keeping my eye on GQSB. Google offers clear direction and measurement for its Google News product: its growth is mobile, and its mobile growth is AMP. Google has insisted that AMP articles do not get preferential treatment in search results, and in overall search that may still be true. But as far as GQSB and Google News goes, all I know is that we need to AMP it up. Want data like this sent to you as soon as we publish it? Make sure to sign up for our data newsletter in the form below to get it! And read our latest story on 2018 referral growth trends.\n",
      "Sentences: 83\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "One analysts attempts to demystify the types of traffic Google sends publishersI really love it when one of my go-to news sources sneaks its way into my periphery with a new way of telling a storya new podcast or app, or really any rich new content format. As an Android user, Ive been noticing Googles finding all sorts of ways to sneak in more recommended content, and not just in search results. I started investigating Googles referring urls. Those recommended content elements I was noticing on my phone? Theyre all from the same referring url: Google Quick Searchbox, or GQSB. So what exactly is GQSB and how does its growth compare to Google News? Whats the deal with googleapis since my last analysis? And most importantly, how can publishers identify and gain exposure on these products? (Spoiler alert: Prepare for another discussion about AMP. )As Google optimizes its content-based products, I wanted to understand whats driving traffic and what you should do to get on board. Heres where my analysis took me. View related pages: From a user perspective, sometimes it felt obvious why I was being served certain contentI searched for something, read it, and scrolling up ever-so-slightly on the page, was served very relevant related articles. I noticed that sometimes the recommended content was from the same source, and sometimes I got a variety. I also noticed that this didnt ever happen in the Chrome app; it was in Googles weird little webview, the purgatorial browser between search and site. Check out your feed: Then Google suggested Check out your feed, based on past search queries. Left of home feed: which got me thinking about my beloved left-of-home feedso perfectly curated and relevant, as if Google has known me for years. So much content! So many ways to get it! How can I track this tapestry of Google traffic? A grey area with Google: GQSBSometimes querying Googles referring urls feels a bit like talking to an oracle: never explicit, sifting through circular gibberish. Imagine my surprise when I found that the three content recommendation types that caught my eye all send the same referrer: android-app://com. google. android. googlequicksearchbox ( or some variation thereof ).This referring URL also made a brief appearance in 2016, and then quickly disappeared as the traffic returned to direct. But this time, these arent from the search box! For the sake of this article, Im going to call it by its old nickname, GQSB. GQSB was previously understood to be only organic search from the search bar widget on mobile devices, but now its a blend of search box and recommended content. That includes, but maybe is not limited to: View Related Pages. This appears in a Google webviewnot Chromeon both AMP and non-AMP pages. Check out your feed at the bottom of search inputs. This takes you to interest-based content cards. Left-of-home swipe cards. (Note: These cards arent related to the googleapis referrer, and weve clarified that in this post . )It would be easy to lump all of this together as search traffic because a lot of it is still a click from the actual search box. But its indistinguishable from the url sent by what is clearly recommended, not-searched-for, content. That is: Im not actively searching for a particular topic; Im being recommended content based on my past search history, or at least based on what Im currently reading. And heres why we should care about it: GQSB recs have dwarfed all our other Google content recs. (And that spike in May is all AMP. Color me shocked. )So if GQSB is the referring domain sending the most traffic, whats going on with Google News and googleapis? In May, Google News took over from Googleapis as the fastest growing Google content recommenderWhile GQSB recommends content, its not an aggregator in the same way that Google News and googleapis are. So while GQSB is sending more traffic, Google News takes the title of fastest growing Google content recommender thanks to a change made in May. Up until then, Google recommended content across three main products: Google News (news. google. com) Google Play Newsstand (play. google. com) in-app Chrome recommendations (googleapis. com)In May 2018, Google merged their Play Newsstand app with Google News, creating a single, streamlined news aggregator for desktop, mobile, and app. It makes sense that Google should merge these two practically identical products. In mid-2017, Google News was quietly losing traffic. Meanwhile, Google Newsstand was slowly but surely inching upwards. Since merging, Google News is the fastest growing Google content recommender, growing at a faster rate than googleapis. Today, Google News is the largest and fastest growing Google news aggregator, with 60% mobile traffic. Oh, and about that mobile traffic: it is 85% AMP, and growing. In May 2018, Google News traffic jumped up 25. 7% from the previous month. Over the summer months, Google News average month-over-month percentage growth has remained in the double-digits. Meanwhile, googleapis has stalled to single-digit 2.4% growth. Merging Newsstand brought a dormant Google News back to life. Needless to say, May to August 2018 has been an interesting period for Google content recommendations. It may be hasty to say whether the rate of change will continue, but worth watching. Google News growth is all mobileNot only does de-duplicating similar products just make sense, but merging products indicates Googles investment in growing the Google News mobile offering. So lets unpack the growth that happened post-merger. While the uptick in Google News mobile referrers (after merging a mobile app) is expected, mobile traffic took off. In January 2018, Google News was 40% mobile traffic. Now its nearly 60% mobile. This indicates that not only does Google News prefer AMP content, it may be deprioritizing non-AMP content (or if its not actively deprioritizing it, the content is at least not getting traffic anymore, which has a similar outcome for publishers).Google News growth is all mobile, and Google News mobile growth is all AMP ( similarly to Google Search ), yet its desktop traffic remains steady. While Google News on desktop remains a reliable source of traffic, we cant say the same for non-AMP traffic, which takes a hit. If this downward trend in non-AMP Google News traffic continues, then you could argue that in a couple months, if youre not on AMP, then youre not on (mobile) Google News. Maybe Google News mobile growth will stabilize itself on top, and Google News across all devices will settle back in as a steady-as-she-goes referral source. And before I start humming along to The Raconteurs, its worth noting: it doesnt matter if I report on Google News being the next big thing if, by not using AMP, youre not able to take advantage of its growth. Googleapis deserves a shout-out here for exactly this reason. Its still split between AMP vs. non-AMP traffic, which means publishers dont have the same pressure to be on AMP in the case of googleapis. However, dont hold your breath on that trend staying the same if the other recommending services are an indication. All things circle back to AMPAs a user, I love Google News, and apparently Im not alone . I appreciate that Google combined two very similar products that I didnt really need two of. And as a media analyst, Im keeping my eye on GQSB. Google offers clear direction and measurement for its Google News product: its growth is mobile, and its mobile growth is AMP. Google has insisted that AMP articles do not get preferential treatment in search results, and in overall search that may still be true. But as far as GQSB and Google News goes, all I know is that we need to AMP it up. Want data like this sent to you as soon as we publish it? Make sure to sign up for our data newsletter in the form below to get it! And read our latest story on 2018 referral growth trends .Join the 30,000 people making better decisions with our data newsletter\n",
      "Sentences: 83\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "Parse. ly's Digital Media Expertise, Right in Your InboxYou are now subscribed to the Parse. ly blog! Take me back to the articleJoin the 30,000 people making better decisions                                 with our data newsletterOne analysts attempts to demystify the types of traffic Google sends publishersby Kelsey Arendt September 12, 2018I really love it when one of my go-to news sources sneaks its way into my periphery with a new way of telling a storya new podcast or app, or really any rich new content format. As an Android user, Ive been noticing Googles finding all sorts of ways to sneak in more recommended content, and not just in search results. I started investigating Googles referring urls. Those recommended content elements I was noticing on my phone? Theyre all from the same referring url: Google Quick Searchbox, or GQSB. So what exactly is GQSB and how does its growth compare to Google News? Whats the deal with googleapis since my last analysis? And most importantly, how can publishers identify and gain exposure on these products? (Spoiler alert: Prepare for another discussion about AMP. )As Google optimizes its content-based products, I wanted to understand whats driving traffic and what you should do to get on board. Heres where my analysis took me. View related pages:From a user perspective, sometimes it felt obvious why I was being served certain contentI searched for something, read it, and scrolling up ever-so-slightly on the page, was served very relevant related articles. I noticed that sometimes the recommended content was from the same source, and sometimes I got a variety. I also noticed that this didnt ever happen in the Chrome app; it was in Googles weird little webview, the purgatorial browser between search and site. Check out your feed:Then Google suggested Check out your feed, based on past search queries. Left of home feed:which got me thinking about my beloved left-of-home feedso perfectly curated and relevant, as if Google has known me for years. So much content! So many ways to get it! How can I track this tapestry of Google traffic? A grey area with Google: GQSBSometimes querying Googles referring urls feels a bit like talking to an oracle: never explicit, sifting through circular gibberish. Imagine my surprise when I found that the three content recommendation types that caught my eye all send the same referrer: android-app://com. google. android. googlequicksearchbox ( or some variation thereof ).This referring URL also made a brief appearance in 2016, and then quickly disappeared as the traffic returned to direct. But this time, these arent from the search box! For the sake of this article, Im going to call it by its old nickname, GQSB. GQSB was previously understood to be only organic search from the search bar widget on mobile devices, but now its a blend of search box and recommended content. That includes, but maybe is not limited to:View Related Pages. This appears in a Google webviewnot Chromeon both AMP and non-AMP pages. Check out your feed at the bottom of search inputs. This takes you to interest-based content cards. Left-of-home swipe cards. (Note: These cards arent related to the googleapis referrer, and weve clarified that in this post . )It would be easy to lump all of this together as search traffic because a lot of it is still a click from the actual search box. But its indistinguishable from the url sent by what is clearly recommended, not-searched-for, content. That is: Im not actively searching for a particular topic; Im being recommended content based on my past search history, or at least based on what Im currently reading. And heres why we should care about it: GQSB recs have dwarfed all our other Google content recs. (And that spike in May is all AMP. Color me shocked. )So if GQSB is the referring domain sending the most traffic, whats going on with Google News and googleapis? In May, Google News took over from Googleapis as the fastest growing Google content recommenderWhile GQSB recommends content, its not an aggregator in the same way that Google News and googleapis are. So while GQSB is sending more traffic, Google News takes the title of fastest growing Google content recommender thanks to a change made in May. Up until then, Google recommended content across three main products:Google News (news. google. com)Google Play Newsstand (play. google. com)in-app Chrome recommendations (googleapis. com)In May 2018, Google merged their Play Newsstand app with Google News, creating a single, streamlined news aggregator for desktop, mobile, and app. It makes sense that Google should merge these two practically identical products. In mid-2017, Google News was quietly losing traffic. Meanwhile, Google Newsstand was slowly but surely inching upwards. Since merging, Google News is the fastest growing Google content recommender, growing at a faster rate than googleapis. Today, Google News is the largest and fastest growing Google news aggregator, with 60% mobile traffic. Oh, and about that mobile traffic: it is 85% AMP, and growing. In May 2018, Google News traffic jumped up 25. 7% from the previous month. Over the summer months, Google News average month-over-month percentage growth has remained in the double-digits. Meanwhile, googleapis has stalled to single-digit 2.4% growth. Merging Newsstand brought a dormant Google News back to life. Needless to say, May to August 2018 has been an interesting period for Google content recommendations. It may be hasty to say whether the rate of change will continue, but worth watching. Google News growth is all mobileNot only does de-duplicating similar products just make sense, but merging products indicates Googles investment in growing the Google News mobile offering. So lets unpack the growth that happened post-merger. While the uptick in Google News mobile referrers (after merging a mobile app) is expected, mobile traffic took off. In January 2018, Google News was 40% mobile traffic. Now its nearly 60% mobile. This indicates that not only does Google News prefer AMP content, it may be deprioritizing non-AMP content (or if its not actively deprioritizing it, the content is at least not getting traffic anymore, which has a similar outcome for publishers).Google News growth is all mobile, and Google News mobile growth is all AMP ( similarly to Google Search ), yet its desktop traffic remains steady. While Google News on desktop remains a reliable source of traffic, we cant say the same for non-AMP traffic, which takes a hit. If this downward trend in non-AMP Google News traffic continues, then you could argue that in a couple months, if youre not on AMP, then youre not on (mobile) Google News. Maybe Google News mobile growth will stabilize itself on top, and Google News across all devices will settle back in as a steady-as-she-goes referral source. And before I start humming along to The Raconteurs, its worth noting: it doesnt matter if I report on Google News being the next big thing if, by not using AMP, youre not able to take advantage of its growth. Googleapis deserves a shout-out here for exactly this reason. Its still split between AMP vs. non-AMP traffic, which means publishers dont have the same pressure to be on AMP in the case of googleapis. However, dont hold your breath on that trend staying the same if the other recommending services are an indication. All things circle back to AMPAs a user, I love Google News, and apparently Im not alone . I appreciate that Google combined two very similar products that I didnt really need two of. And as a media analyst, Im keeping my eye on GQSB. Google offers clear direction and measurement for its Google News product: its growth is mobile, and its mobile growth is AMP. Google has insisted that AMP articles do not get preferential treatment in search results, and in overall search that may still be true. But as far as GQSB and Google News goes, all I know is that we need to AMP it up. Want data like this sent to you as soon as we publish it? Make sure to sign up for our data newsletter in the form below to get it! And read our latest story on 2018 referral growth trends .RelatedJoin the 30,000 people making better                                      decisions with our data newsletterJoin the 30,000 people making better decisions                             with our data newsletterProducts\n",
      "Sentences: 86\n",
      "\n",
      "\n",
      "Article: https://www.nytimes.com/2019/02/11/travel/northern-lights-tourism-in-sweden.html\n",
      "NodeRank:\n",
      "On a clear January night in northern Sweden, after hours of squinting and wondering if this or that small cloud might be the northern lights, a shimmering, alien-green ribbon unfurled across the sky. Here, on the shore of the frozen Torne River, just outside the village of Kurravaara, there was none of the mysterious clapping or crackling that Finnish researchers have recorded with the mesmerizing spectacle known as the aurora borealis. Instead, the only sound was the joyous yelp of a couple from Shanghai as they bounded out of the neighboring cabin, Huawei camera phone aloft. In an era when the digital world has winnowed our attention spans, the aurora borealis still demands presence and patience, a long journey to far northern latitudes, and the fortitude to weather arctic conditions. Despite these challenges, color-saturated images of the otherworldly aurora in social and traditional media continue to inspire travelers in growing numbers. And while over-the-top experiences abound  you can charter a bush plane to a remote mountain lodge in Canada, rent a helicopter to fly above the clouds in Iceland, or camp on a glacier in Greenland  the greatest consequence of this tourism boom is the expanding range of aurora-chasing experiences for every type of traveler. ImageIce formations and frosty trees on the shoreline of Lake Torneträsk in northern Sweden. CreditOliver WrightAhhhh! Bucket list! was the first comment my own grainy, overexposed aurora snapshot received on Instagram that night. Indeed, since the phrase bucket list entered common usage  around 2007, when the film with the same name came out  seeing the aurora has been a fixture on many of those lists. Yet, only a decade earlier, the northern lights werent even a blip on most travelers radar. There was no one else doing northern lights tours in the world, said Masa Ando, a 34-year veteran of the Japanese tourism industry in Alaska who today co-owns HAI Shirokuma Tours. In the 1980s and 90s, he said, locals didnt understand the fuss when Japanese tourists began arriving in Fairbanks. They were questioning, Why are they coming to see northern lights? Whats special about this? From those trailblazing Japanese groups, the northern lights as tourist attraction gained momentum around the globe. Its become a must-do thing in life to see the northern lights, said Arne Bergh, an owner and creative director of the Icehotel in Kiruna, Sweden, where every winter aurora hopefuls chase the phenomenon he called natures own fireworks before retiring to their subzero ice rooms. In Alaska, the number of winter visitors last year surpassed 320,000, an increase of 33 percent from a decade earlier, according to the Alaska Travel Industry Association, which credits most of that tourism to the aurora. In Canadas Northwest Territories, meanwhile, the remote capital of Yellowknife has marketed itself as a top northern lights destination, particularly to travelers in Asia. According to a report from the government of the N.W.T., the number of aurora tourists more than quadrupled over the last six years  a trend evident even in the food scene of Yellowknife, population around 20,000. ImageA team of sled dogs on a frozen river near Kiruna, Sweden. CreditOliver WrightWeve seen the addition of a Korean restaurant, a Japanese bakery and a Chinese hot pot restaurant, said Cathie Bolstad, chief executive of Northwest Territories Tourism. More on traveling in Sweden36 Hours in StockholmJuly 6, 201736 Hours in Gothenburg, SwedenMay 10, 2018The same government report estimated that last season those tourists spent over $40 million. Beyond North America, the interest in northern lights travel has spurred improved tourism infrastructure across the arctic region, from northern Russia to Finland, Sweden, Norway, Iceland and Greenland. AlaskaYellowknifeRussiaNorth PoleCanadaFinlandGreenlandIcelandNorwaySweden10 milesAurora Sky StationAbiskoMt. NuoljaSTF Abisko Mountain StationTromsoSwedenDetail areaNorwayKurravaaraFinlandSWEDENKirunaStockholmOfelasAs soon as you get above the polar circle, you can see the aurora very likely, explained Trond Trondsen, an aurora expert in Calgary. A native of Tromso, Norway, Dr. Trondsen grew up fascinated by the mysterious lights he often saw walking home from school, and later earned a doctorate in cosmic geophysics with a focus on auroral imaging. Today he runs a private company designing instrumentation for aurora researchers. Although anyone can summon dazzling videos on a digital screen, he insists theres no substitute for seeing the phenomenon in person. Its so hard to paint a picture of the overwhelming emotional impact that it has, he said. I would call it dancing lights in the sky. Theres a rhythm to it. Theres a color scheme to it. Its almost like heavenly visual music. ImageThe northern lights are one of the reasons more tourists are visiting some parts of northern Sweden in  the winter than the summer. CreditOliver WrightBefore embarking on my own aurora hunt in northern Sweden, I wanted to understand what, exactly, it was I was searching for in the dark arctic sky. The aurora is a manifestation of what we call space weather, Dr. Trondsen explained the day before my flight departed from Stockholm for Kiruna. It all starts at the sun. As the suns magnetic field becomes stronger and weaker, it can become unstable, resulting in solar flares and whats called a coronal mass ejection. Basically the sun burps a piece of itself into space, he said. And these are particles  electrons, as well as a piece of its own magnetic field. These particles blow into space and some reach earth. Our magnetic field eventually draws the particles toward the North and South Poles, where they may enter the atmosphere. Its when these electrons hit our atmosphere that you get the aurora, he continued, comparing the effect to old phosphor-coated television screens bombarded by electron beams from behind. The region in which the lights appear  the auroral oval  rings both geomagnetic poles. The earth rotates us in under the aurora at midnight, or around midnight our local time, he continued, explaining prime aurora-chasing time. As for the characteristic colors, he said, thats indicative of what altitude the reaction happens at, and which gas the electron hits. For example, if it hits oxygen, you get green and red typically. If it hits nitrogen, you get blue light. ImageAn arctic char as seen through a hole in the ice on Lake Torneträsk. CreditOliver WrightImageWinter activities in northern Sweden include ice-fishing. CreditOliver WrightWhat usually limits the ability to see aurora is clouds, explained Urban Braendstroem, an aurora researcher at the Swedish Institute for Space Physics in Kiruna, in an email. Abisko has the most clear nights in Sweden due to meteorological conditions caused by the surrounding mountains. Abisko is a village roughly 125 miles north of the Arctic Circle with a population hovering around the same number. The rural area was primarily a destination among Swedes for late-winter skiing and summer hiking when Chad and Linnea Blakley arrived in 2008 to work at the local mountain lodge, STF Abisko Fjallstation, where I met with the couple in a quiet room above the bustling, hostel-like lobby. This is hard to believe, but at the time, there was really not any northern lights tourism to speak of in Abisko, said Mr. Blakley, a Missouri native. Inspired by the aurora, he began photographing the phenomenon and developed a following online for his dramatic images. In 2010, the couple started Lights over Lapland, a photography tour company, and today they employ 21 people in Abisko. It went from a backwater with really no tourism infrastructure at all to one of the leading aurora destinations on the planet in a decade, he said. In 2017, the lodge reported 30,000 international guest nights. Today groups of tourists dressed in puffy coveralls and insulated boots toddle around Abisko  a modest cluster of houses along one main road with no traffic light  snapping photos while posing beside locals snowmobiles and pet huskies. ImageA Sami man herds reindeer near Kiruna, Sweden. CreditOliver WrightAccording to Louise Johansson, the communications coordinator at the mountain lodge, which is run by the Swedish Tourist Association, the winter months now draw more visitors than the summer season. Just a few years ago, the opposite was true. In January 2018 alone, the lodge hosted guests of 42 different nationalities. Many come for the nearby Aurora Sky Station, a mountaintop-viewing platform reached by a two-seater chair lift built in 1966, where on a recent night, fellow visitors hailed from India, Bolivia, South Korea, France, England and Australia. Tonight it does show cloud cover, Mr. Blakley said, reviewing the evenings forecast before my visit to the Sky Station, but Id bet you a dollar that youll still see the lights if youre out there all night. He owes me a buck. Atop Mount Nuolja, the Aurora Sky Station delivered dramatic views of the glittering town below, situated in a valley beside a black-as-coal lake. But overhead there were only clouds, heavy and wet, accompanied by an arctic wind whipping up shards of ice and snow. A few foolhardy aurora-chasers knelt beside camera tripods staked in the snow, while wiser souls huddled near the mountain hut for a modicum of warmth or retreated to the mediocre cafe within. Lying on my back in the snow, staring fruitlessly for hours at grayscale clouds, the hunt for the northern lights felt like a farce. ImageIn Abisko National Park, wildlife sightings might include moose. CreditOliver WrightSince auroras can be elusive, travelers are wise to incorporate activities into the hourslong night-sky vigil. Around Kiruna in northern Sweden, for example, most traditional winter pastimes have been adapted to the aurora hunt. Kerstin Nilsson, who together with her husband runs Ofelas, an Icelandic horse-riding business on a farm outside Kiruna, created one of the first such pairings when they began nighttime aurora-on-horseback tours in 1997. Today you can also hunt aurora on sleigh-rides and snowshoe tours, chase the lights aboard roaring snowmobiles or on sleds pulled by huskies sailing across frozen lakes. Even when the lights dont come out to play, I can confirm that its an unforgettable rush to bump along forest trails behind a pack of dogs that seems deaf to the mushers cries. Or supplement the northern lights with a cultural excursion for an introduction to Sapmi, the traditional land of the indigenous Sami people and their roaming reindeer herds. In 2012, two Sami photographers, Anette Niia and Ylva Sarri, founded Scandinavian Photoadventures, which offers aurora-focused wilderness tours, photo expeditions and cultural experiences flavored by songs and stories that have been passed down through generations. The Sami culture is a storytelling culture because you had nothing else, Ms. Niia said. No internet, no books, no texts, nothing. You were alone with a fire in the forest together with your family. Many Sami were fearful of the northern lights, Ms. Sarri explained, and believed the aurora was a sacred manifestation of their ancestors souls. Both women said theyd been warned as children about taunting the lights. Me and my sister, we used to go out and tease the northern lights all the time, Ms. Niia recalled, laughing. It was so thrilling. And when it started to move, then we got really scared and ran home. ImageSnowmobiles in Abisko National Park, Sweden. CreditChad Blakley/LightsoverlaplandLove the idea of the lights but not the freezing cold? An increasing number of arctic lodging options promise views of the night sky from bed, ranging from innovative bubble tents to glass-ceiling huts. After the fruitless night I spent shivering on the mountaintop in Abisko, it felt luxurious to look for the lights from inside a SkyNest, one of two cozy, lemming-shaped cabins with transparent walls and ceilings in rural Kurravaara. Admittedly, most aurora watching occurred not from bed, but outside where no branches obstructed the awe-inspiring tableau. Inside, with my toes on a space heater, I could survey the sky, and whenever a vague shimmer appeared through the window, pull on my boots and coat, and crunch through the snow onto the frozen river. Sometimes the gleam faded quickly, but just as often it crescendoed into a turbulent chaotic flow, wisps of green and pink twirling across the night sky. And when the humbling show eventually dissolved into nothingness, mere minutes after it had begun, only a dozen steps returned me to the warmth of the cabin. Many cruise lines now also pursue the aurora. Hurtigruten guarantees that passengers on its 12-day Astronomy Voyage along the Norwegian coast will see the lights. If not, the next cruise is free. And last month, Viking Cruises inaugurated a new 13-day sailing to northern Norway, called In Search of the Northern Lights, which bills itself as the first full-length winter itinerary in the arctic from an American cruise line. According to the company, all six sailings in 2019 sold out. People are more after experiences now, said Torstein Hagen, the chairman of Viking Cruises. Its experience rather than indulgence. ImageTourists taking photos of the northern lights in the shadow of Mount Noulja in Abisko National Park. CreditChad Blakley/LightsoverlaplandRiding in an eight-seat minivan for hours with a group of strangers may not be the sexiest way to chase the aurora, but many knowledgeable guides are ready to turn the back seat into a classroom for travelers with high hopes but little time. I see the aurora somewhere between 90 to 95 percent of the time every day we have tours, said Torsten Aslaksen, a guide in Tromso, Norway, who drives tourists between August and April, a season several weeks longer than most operators. Im an aurora professor, and I have a long-term interest in astronomy, said Dr. Aslaksen, who pivoted to guiding tours in 2016. I also have a background as a weather forecaster, he added. So all of these things are kind of nice to have when youre out there with your tourists. Every tour operator with skin in the game claims that his location is the best for aurora viewing, and Dr. Aslaksen is no exception. He notes Tromsos proximity to the magnetic pole, its well-established infrastructure and varied landscape. The mountains are able to make different weather zones, he said. So when we have clouds on the coast, we can drive inland and find clear skies. Then its just a matter of waiting for the cosmic show. It can be so bright it will cast shadows on the ground, Dr. Aslaksen said. You can read a book from the light from only the aurora when this happens. But generally, the brightest, most powerful aurora  what Chad Blakley called a come-to-Jesus aurora  are less common right now. The sun gets more active and less active in an 11-year cycle approximately, explained Dr. Trondsen, the aurora expert. And right now were in the dump. But that doesnt mean travelers should postpone a trip until the cycles peak, the so-called solar maximum, which is several years away. In the meantime, we will have coronal mass ejections, we will have solar flares, so there will be aurora, Dr. Trondsen said, just not as frequent. But I fully expect that in four or five years, it will blow our minds, he added. Fingers crossed anyway. Ingrid K. Williams, a writer based in Stockholm and northwestern Italy, is a frequent contributor to the Travel section. Follow NY Times Travel on Twitter, Instagram and Facebook. Get weekly updates from our Travel Dispatch newsletter, with tips on traveling smarter, destination coverage and photos from all over the world.\n",
      "Sentences: 127\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "\n",
      "Sentences: 0\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "OfelasBy The New York TimesAs soon as you get above the polar circle, you can see the aurora very likely, explained Trond Trondsen, an aurora expert in Calgary. A native of Tromso, Norway, Dr. Trondsen grew up fascinated by the mysterious lights he often saw walking home from school, and later earned a doctorate in cosmic geophysics with a focus on auroral imaging. Today he runs a private company designing instrumentation for aurora researchers. Although anyone can summon dazzling videos on a digital screen, he insists theres no substitute for seeing the phenomenon in person. Its so hard to paint a picture of the overwhelming emotional impact that it has, he said. I would call it dancing lights in the sky. Theres a rhythm to it. Theres a color scheme to it. Its almost like heavenly visual music. ImageThe northern lights are one of the reasons more tourists are visiting some parts of northern Sweden in  the winter than the summer. CreditOliver WrightBlinded by scienceBefore embarking on my own aurora hunt in northern Sweden, I wanted to understand what, exactly, it was I was searching for in the dark arctic sky. The aurora is a manifestation of what we call space weather, Dr. Trondsen explained the day before my flight departed from Stockholm for Kiruna. It all starts at the sun. As the suns magnetic field becomes stronger and weaker, it can become unstable, resulting in solar flares and whats called a coronal mass ejection. Basically the sun burps a piece of itself into space, he said. And these are particles  electrons, as well as a piece of its own magnetic field. These particles blow into space and some reach earth. Our magnetic field eventually draws the particles toward the North and South Poles, where they may enter the atmosphere. Its when these electrons hit our atmosphere that you get the aurora, he continued, comparing the effect to old phosphor-coated television screens bombarded by electron beams from behind. The region in which the lights appear  the auroral oval  rings both geomagnetic poles. The earth rotates us in under the aurora at midnight, or around midnight our local time, he continued, explaining prime aurora-chasing time. As for the characteristic colors, he said, thats indicative of what altitude the reaction happens at, and which gas the electron hits. For example, if it hits oxygen, you get green and red typically. If it hits nitrogen, you get blue light. ImageAn arctic char as seen through a hole in the ice on Lake Torneträsk. CreditOliver WrightImageWinter activities in northern Sweden include ice-fishing. CreditOliver WrightThe town aurora borealis builtWhat usually limits the ability to see aurora is clouds, explained Urban Braendstroem, an aurora researcher at the Swedish Institute for Space Physics in Kiruna, in an email. Abisko has the most clear nights in Sweden due to meteorological conditions caused by the surrounding mountains. Abisko is a village roughly 125 miles north of the Arctic Circle with a population hovering around the same number. The rural area was primarily a destination among Swedes for late-winter skiing and summer hiking when Chad and Linnea Blakley arrived in 2008 to work at the local mountain lodge, STF Abisko Fjallstation , where I met with the couple in a quiet room above the bustling, hostel-like lobby. This is hard to believe, but at the time, there was really not any northern lights tourism to speak of in Abisko, said Mr. Blakley, a Missouri native. Inspired by the aurora, he began photographing the phenomenon and developed a following online for his dramatic images. In 2010, the couple started Lights over Lapland , a photography tour company, and today they employ 21 people in Abisko. It went from a backwater with really no tourism infrastructure at all to one of the leading aurora destinations on the planet in a decade, he said. In 2017, the lodge reported 30,000 international guest nights. Today groups of tourists dressed in puffy coveralls and insulated boots toddle around Abisko  a modest cluster of houses along one main road with no traffic light  snapping photos while posing beside locals snowmobiles and pet huskies. ImageA Sami man herds reindeer near Kiruna, Sweden. CreditOliver WrightAccording to Louise Johansson, the communications coordinator at the mountain lodge, which is run by the Swedish Tourist Association, the winter months now draw more visitors than the summer season. Just a few years ago, the opposite was true. In January 2018 alone, the lodge hosted guests of 42 different nationalities. Many come for the nearby Aurora Sky Station , a mountaintop-viewing platform reached by a two-seater chair lift built in 1966, where on a recent night, fellow visitors hailed from India, Bolivia, South Korea, France, England and Australia. Tonight it does show cloud cover, Mr. Blakley said, reviewing the evenings forecast before my visit to the Sky Station, but Id bet you a dollar that youll still see the lights if youre out there all night. He owes me a buck. Atop Mount Nuolja, the Aurora Sky Station delivered dramatic views of the glittering town below, situated in a valley beside a black-as-coal lake. But overhead there were only clouds, heavy and wet, accompanied by an arctic wind whipping up shards of ice and snow. A few foolhardy aurora-chasers knelt beside camera tripods staked in the snow, while wiser souls huddled near the mountain hut for a modicum of warmth or retreated to the mediocre cafe within. Lying on my back in the snow, staring fruitlessly for hours at grayscale clouds, the hunt for the northern lights felt like a farce. ImageIn Abisko National Park, wildlife sightings might include moose. CreditOliver WrightAurora adventuresSince auroras can be elusive, travelers are wise to incorporate activities into the hourslong night-sky vigil. Around Kiruna in northern Sweden, for example, most traditional winter pastimes have been adapted to the aurora hunt. Kerstin Nilsson, who together with her husband runs Ofelas, an Icelandic horse-riding business on a farm outside Kiruna, created one of the first such pairings when they began nighttime aurora-on-horseback tours in 1997. Today you can also hunt aurora on sleigh-rides and snowshoe tours, chase the lights aboard roaring snowmobiles or on sleds pulled by huskies sailing across frozen lakes. Even when the lights dont come out to play, I can confirm that its an unforgettable rush to bump along forest trails behind a pack of dogs that seems deaf to the mushers cries. Or supplement the northern lights with a cultural excursion for an introduction to Sapmi, the traditional land of the indigenous Sami people and their roaming reindeer herds. In 2012, two Sami photographers, Anette Niia and Ylva Sarri, founded Scandinavian Photoadventures , which offers aurora-focused wilderness tours, photo expeditions and cultural experiences flavored by songs and stories that have been passed down through generations. The Sami culture is a storytelling culture because you had nothing else, Ms. Niia said. No internet, no books, no texts, nothing. You were alone with a fire in the forest together with your family. Many Sami were fearful of the northern lights, Ms. Sarri explained, and believed the aurora was a sacred manifestation of their ancestors souls. Both women said theyd been warned as children about taunting the lights. Me and my sister, we used to go out and tease the northern lights all the time, Ms. Niia recalled, laughing. It was so thrilling. And when it started to move, then we got really scared and ran home. ImageSnowmobiles in Abisko National Park, Sweden. CreditChad Blakley/LightsoverlaplandThe slow roadLove the idea of the lights but not the freezing cold? An increasing number of arctic lodging options promise views of the night sky from bed, ranging from innovative bubble tents to glass-ceiling huts. After the fruitless night I spent shivering on the mountaintop in Abisko, it felt luxurious to look for the lights from inside a SkyNest, one of two cozy, lemming-shaped cabins with transparent walls and ceilings in rural Kurravaara. Admittedly, most aurora watching occurred not from bed, but outside where no branches obstructed the awe-inspiring tableau. Inside, with my toes on a space heater, I could survey the sky, and whenever a vague shimmer appeared through the window, pull on my boots and coat, and crunch through the snow onto the frozen river. Sometimes the gleam faded quickly, but just as often it crescendoed into a turbulent chaotic flow, wisps of green and pink twirling across the night sky. And when the humbling show eventually dissolved into nothingness, mere minutes after it had begun, only a dozen steps returned me to the warmth of the cabin. Many cruise lines now also pursue the aurora. Hurtigruten guarantees that passengers on its 12-day Astronomy Voyage along the Norwegian coast will see the lights. If not, the next cruise is free. And last month, Viking Cruises inaugurated a new 13-day sailing to northern Norway, called  In Search of the Northern Lights , which bills itself as the first full-length winter itinerary in the arctic from an American cruise line. According to the company, all six sailings in 2019 sold out. People are more after experiences now, said Torstein Hagen, the chairman of Viking Cruises. Its experience rather than indulgence. ImageTourists taking photos of the northern lights in the shadow of Mount Noulja in Abisko National Park. CreditChad Blakley/LightsoverlaplandFavorable forecastsRiding in an eight-seat minivan for hours with a group of strangers may not be the sexiest way to chase the aurora, but many knowledgeable guides are ready to turn the back seat into a classroom for travelers with high hopes but little time. I see the aurora somewhere between 90 to 95 percent of the time every day we have tours, said Torsten Aslaksen, a guide in Tromso, Norway, who drives tourists between August and April, a season several weeks longer than most operators. Im an aurora professor, and I have a long-term interest in astronomy, said Dr. Aslaksen, who pivoted to guiding tours in 2016. I also have a background as a weather forecaster, he added. So all of these things are kind of nice to have when youre out there with your tourists. Every tour operator with skin in the game claims that his location is the best for aurora viewing, and Dr. Aslaksen is no exception. He notes Tromsos proximity to the magnetic pole, its well-established infrastructure and varied landscape. The mountains are able to make different weather zones, he said. So when we have clouds on the coast, we can drive inland and find clear skies. Then its just a matter of waiting for the cosmic show. It can be so bright it will cast shadows on the ground, Dr. Aslaksen said. You can read a book from the light from only the aurora when this happens. But generally, the brightest, most powerful aurora  what Chad Blakley called a come-to-Jesus aurora  are less common right now. The sun gets more active and less active in an 11-year cycle approximately, explained Dr. Trondsen, the aurora expert. And right now were in the dump. But that doesnt mean travelers should postpone a trip until the cycles peak, the so-called solar maximum, which is several years away. In the meantime, we will have coronal mass ejections, we will have solar flares, so there will be aurora, Dr. Trondsen said, just not as frequent. But I fully expect that in four or five years, it will blow our minds, he added. Fingers crossed anyway. Ingrid K. Williams, a writer based in Stockholm and northwestern Italy, is a frequent contributor to the Travel section.\n",
      "Sentences: 99\n",
      "\n",
      "\n",
      "Article: https://techcrunch.com/2019/02/11/us-iphone-users-spent-79-last-year-up-36-from-2017/\n",
      "NodeRank:\n",
      "Apples push to get developers to build subscription-based apps is now having a notable impact on App Store revenues. According to a new report from Sensor Tower due out later this week, revenue generated per U.S. iPhone grew 36 percent, from $58 in 2017 to $79 last year. As is typical, much of that increase can be attributed to mobile gaming, which accounted for more than half of this per-device average. However, more substantial growth took place in the categories outside of gaming  including those categories where subscription-based apps tend to rule the top charts, the firm found. According to the reports findings, per-device app spending in the U.S. grew more over the past year than it did in 2017. From 2017 to 2018, iPhone users spent an average of $21 or more on in-app purchases and paid app downloads  a 36 percent increase compared with the 23 percent increase from 2016 to 2017, when revenue per device grew from $47 to $58. However, 2018s figure was slightly lower than the 42 percent increase in average per-device spending seen between 2015 and 2016, when revenue grew from $33 to $47, noted Sensor Tower. As usual, mobile gaming continued to play a large role in iPhone spending. In 2018, gaming accounted for nearly 56 percent of the average consumer spend  or $44 out of the total $79 spent per iPhone. But whats more interesting is how the non-gaming categories fared this past year. Some categories  including those where subscription-based apps dominate the top charts  saw even higheryear-over-year growth in 2018, the firm found. For example, Entertainment apps grew their spend per device increase by 82 percent to $8 of the total in 2018. Lifestyle apps increased by 86 percent to reach $3.90, up from $2.10. And though it didnt make the top five, Health & Fitness apps also grew 75 percent year-over-year to account for an average of $2.70, up from $1.60 in 2017. Other categories in the top five included Music and Social Networking apps, which both grew by 22 percent. This data indicates that subscription apps are playing a significant role in helping drive iPhone consumer spending higher. The news comes at a time when Apple has reported slowing iPhone sales, which is pushing the company to lean more on servicesto continue to boost its revenue. This includes not just App Store subscriptions, but also things like Apple Music, Apple Pay, iCloud, App Store Search ads, AppleCare and more. As subscriptions become more popular, Apple will need to remain vigilant against those who would abuse the system. For example, a number of sneaky subscription apps were found plaguing the App Store in recent weeks. They were duping users into paid memberships with tricky buttons, hidden text, instant trials that converted in days and the use of other misleading tactics. Apple later cracked down by removing some of the apps, and updated its developer guidelines with stricter rules about how subscriptions should both look and operate. A failure to properly police the App Store or set boundaries to prevent the overuse of subscriptions could end up turning users off from downloading new apps altogether  especially if users begin to think that every app is after a long-term financial commitment. Developers will need to be clever to convert users and retain subscribers amid this shift away from paid apps to those that come with a monthly bill. App makers will need to properly market their subscriptions benefits, and even consider offering bundles to increase the value. But in the near-term, the big takeaway for developers is that there is still good money to be made on the App Store, even if iPhone sales are slowing.\n",
      "Sentences: 26\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "Apples push to get developers to build subscription-based apps is now having a notable impact on App Store revenues. According to a new report from Sensor Tower due out later this week, revenue generated per U.S. iPhone grew 36 percent, from $58 in 2017 to $79 last year. As is typical, much of that increase can be attributed to mobile gaming, which accounted for more than half of this per-device average. However, more substantial growth took place in the categories outside of gaming  including those categories where subscription-based apps tend to rule the top charts, the firm found. According to the reports findings, per-device app spending in the U.S. grew more over the past year than it did in 2017. From 2017 to 2018, iPhone users spent an average of $21 or more on in-app purchases and paid app downloads  a 36 percent increase compared with the 23 percent increase from 2016 to 2017, when revenue per device grew from $47 to $58. However, 2018s figure was slightly lower than the 42 percent increase in average per-device spending seen between 2015 and 2016, when revenue grew from $33 to $47, noted Sensor Tower. As usual, mobile gaming continued to play a large role in iPhone spending. In 2018, gaming accounted for nearly 56 percent of the average consumer spend  or $44 out of the total $79 spent per iPhone. But whats more interesting is how the non-gaming categories fared this past year. Some categories  including those where subscription-based apps dominate the top charts  saw even higheryear-over-year growth in 2018, the firm found. For example, Entertainment apps grew their spend per device increase by 82 percent to $8 of the total in 2018. Lifestyle apps increased by 86 percent to reach $3.90, up from $2.10. And though it didnt make the top five, Health & Fitness apps also grew 75 percent year-over-year to account for an average of $2.70, up from $1.60 in 2017. Other categories in the top five included Music and Social Networking apps, which both grew by 22 percent. This data indicates that subscription apps are playing a significant role in helping drive iPhone consumer spending higher. The news comes at a time when Apple has reported slowing iPhone sales , which is pushing the company to lean more on services to continue to boost its revenue. This includes not just App Store subscriptions, but also things like Apple Music, Apple Pay, iCloud, App Store Search ads, AppleCare and more. As subscriptions become more popular, Apple will need to remain vigilant against those who would abuse the system. For example, a number of sneaky subscription apps were found plaguing the App Store in recent weeks . They were duping users into paid memberships with tricky buttons, hidden text, instant trials that converted in days and the use of other misleading tactics. Apple later cracked down by removing some of the apps, and updated its developer guidelines with stricter rules about how subscriptions should both look and operate. A failure to properly police the App Store or set boundaries to prevent the overuse of subscriptions could end up turning users off from downloading new apps altogether  especially if users begin to think that every app is after a long-term financial commitment. Developers will need to be clever to convert users and retain subscribers amid this shift away from paid apps to those that come with a monthly bill. App makers will need to properly market their subscriptions benefits , and even consider offering bundles to increase the value. But in the near-term, the big takeaway for developers is that there is still good money to be made on the App Store, even if iPhone sales are slowing.\n",
      "Sentences: 26\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "Sarah Perez @sarahintampa / 1 dayApples push to get developers to build subscription-based apps is now having a notable impact on App Store revenues. According to a new report from Sensor Tower due out later this week, revenue generated per U.S. iPhone grew 36 percent, from $58 in 2017 to $79 last year. As is typical, much of that increase can be attributed to mobile gaming, which accounted for more than half of this per-device average. However, more substantial growth took place in the categories outside of gaming  including those categories where subscription-based apps tend to rule the top charts, the firm found. According to the reports findings, per-device app spending in the U.S. grew more over the past year than it did in 2017. From 2017 to 2018, iPhone users spent an average of $21 or more on in-app purchases and paid app downloads  a 36 percent increase compared with the 23 percent increase from 2016 to 2017, when revenue per device grew from $47 to $58. However, 2018s figure was slightly lower than the 42 percent increase in average per-device spending seen between 2015 and 2016, when revenue grew from $33 to $47, noted Sensor Tower. As usual, mobile gaming continued to play a large role in iPhone spending. In 2018, gaming accounted for nearly 56 percent of the average consumer spend  or $44 out of the total $79 spent per iPhone. But whats more interesting is how the non-gaming categories fared this past year. Some categories  including those where subscription-based apps dominate the top charts  saw even higheryear-over-year growth in 2018, the firm found. For example, Entertainment apps grew their spend per device increase by 82 percent to $8 of the total in 2018. Lifestyle apps increased by 86 percent to reach $3.90, up from $2.10. And though it didnt make the top five, Health & Fitness apps also grew 75 percent year-over-year to account for an average of $2.70, up from $1.60 in 2017. Other categories in the top five included Music and Social Networking apps, which both grew by 22 percent. This data indicates that subscription apps are playing a significant role in helping drive iPhone consumer spending higher. The news comes at a time when Apple has reported slowing iPhone sales , which is pushing the company to lean more on services to continue to boost its revenue. This includes not just App Store subscriptions, but also things like Apple Music, Apple Pay, iCloud, App Store Search ads, AppleCare and more. As subscriptions become more popular, Apple will need to remain vigilant against those who would abuse the system. For example, a number of sneaky subscription apps were found plaguing the App Store in recent weeks . They were duping users into paid memberships with tricky buttons, hidden text, instant trials that converted in days and the use of other misleading tactics. Apple later cracked down by removing some of the apps, and updated its developer guidelines with stricter rules about how subscriptions should both look and operate. A failure to properly police the App Store or set boundaries to prevent the overuse of subscriptions could end up turning users off from downloading new apps altogether  especially if users begin to think that every app is after a long-term financial commitment. Developers will need to be clever to convert users and retain subscribers amid this shift away from paid apps to those that come with a monthly bill. App makers will need to properly market their subscriptions benefits , and even consider offering bundles to increase the value. But in the near-term, the big takeaway for developers is that there is still good money to be made on the App Store, even if iPhone sales are slowing.\n",
      "Sentences: 26\n",
      "\n",
      "\n",
      "Article: https://blog.wolfram.com/2019/02/01/the-data-science-of-mathoverflow/\n",
      "NodeRank:\n",
      "This post discusses new Wolfram Language features from the upcoming release of Version 12. Copyable input expressions and a downloadable notebook version of this post will be available when Version 12 is released. Soon there will be 100,000 questions on MathOverflow. net, a question-and-answer site for professional mathematicians! To celebrate this event, I have been working on a Wolfram Language utility package to convert archives of Stack Exchange network websites into Wolfram Language entity stores. The archives are hosted on the Internet Archive and are updated every few months. The package, although not yet publicly available, will be released in the coming weeks as part of Version 12 of the Wolfram Languageso keep watching this space for more news about the release! Although some data analysis can be done with tools such as the Stack Exchange Data Explorer, queries are usually limited in size or computation time, as well as to text-only formats. Additionally, they require some knowledge of SQL. But with a local copy of the data, much more can be done, including images, plots and graphs. With the utility package operating on a local archive, its easy to perform much deeper data analysis using all of the built-in tools in the Wolfram Language. In particular, Version 12 of the Wolfram Language adds support for RDF and SPARQL queries, as well as useful constructs such as FilteredEntityClass and SortedEntityClass. For professional mathematicians who already use Mathematica and the Wolfram Language, this utility allows for seamless investigation into the data on MathOverflow. net or any Stack Exchange network site. Feel free to follow along with me as I do some of this investigation by running the code in a notebook, or just sit back and enjoy the ride as we explore MathOverflow. net with the Wolfram Language! The entity stores created by the utility package allow for quick access to the data in a format thats easy for Wolfram Language processing, such as queries using the Entity framework, machine learning functionality, visualization, etc. Lets start by downloading a pre-generated EntityStore from the Wolfram Cloud to the notebooks directory:&#10005downloadedFile=URLDownload[CloudObject[\"StackExchange2EntityStore/mathoverflow. net. mx\"],NotebookDirectory[]]Import the EntityStore from the downloaded file:&#10005store=Import[downloadedFile];The store is quite large, consisting of nearly three million entities in several entity types:&#10005entityStoreMetaData=AssociationMap[Length[store[#,\"Entities\"]],\"Property Count\"->Length[store[#,\"Properties\"]]|>&,store[]]//ReverseSortBy[Lookup[\"Entity Count\"]];&#10005Dataset[entityStoreMetaData]&#10005Total[#\"Entity Count\"&/@entityStoreMetaData]Lastly, we need to register the EntityStore for use in the current session:This returns a list of all of the new entity types from the EntityStore that are now available through EntityValue (you can access them by registering the EntityStore via EntityRegister).For those who are familiar with the Stack Exchange network, these types may be very familiar. But for those who are not, or if you just need a refresher, heres a basic rundown of a few of the different types:The remaining types not listed are beyond the scope of my post, but you can learn more about them in the README on the archives, or by visiting the frequently asked questions on any Stack Exchange network site. Now that the EntityStore is loaded, we can access it through the Entity framework. Lets look at some random posts:&#10005RandomEntity[\"StackExchange. Mathoverflow:Post\",3]The Post entities are formatted with the post type (Q for question, A for answer), the user who authored the post in square brackets, a short snippet of the post and a hyperlink (the blue ) to the original post on the web. Many of the other entity types format similarlythis is to give proper context, allow for manual exploration on the site itself and give attribution to the original authors (they created the content on the site, after all).Taking just one of these posts, we can find a lot of information about it with a property association:&#10005Entity[\"StackExchange. Mathoverflow:Post\", \"272527\"][{accepted answer,answer count,body,closed date,comment count,comments,community owned date,creation date,duplicate posts,favorite count,id,last activity date,last edit date,last editor,linked posts,owner,post type,score,tags,title,URL,view count},\"PropertyAssociation\"]For example, one may be interested in the posts for a given tag, such as set theory. We can find how many set theory questions have been asked:&#10005EntityValue[EntityClass[\"StackExchange. Mathoverflow:Post\",{\"Tags\"->Entity[\"StackExchange. Mathoverflow:Tag\", \"SetTheory\"],\"PostType\"->Entity[\"StackExchange:PostType\", \"1\"] }],\"EntityCount\"]We can even see the intersections of different tags, such as set theory and plane geometry:&#10005EntityValue[EntityClass[\"StackExchange. Mathoverflow:Post\",{\"Tags\"->ContainsAll[{Entity[\"StackExchange. Mathoverflow:Tag\", \"SetTheory\"],Entity[\"StackExchange. Mathoverflow:Tag\", \"PlaneGeometry\"]}],\"PostType\"->Entity[\"StackExchange:PostType\", \"1\"] }],\"Entities\"]Its important to note that as of this writing, the archives have not been updated to include the 100,000th question, so we can see that there are only 98,165 questions as of December 2, 2018:&#10005EntityClass[\"StackExchange. Mathoverflow:Post\",\"PostType\"->Entity[\"StackExchange:PostType\", \"1\"]][\"EntityCount\"]&#10005EntityValue[Entity[\"StackExchange. Mathoverflow:Post\"],\"LastPostTime\"]Of course, there is a seemingly endless number of queries one can make on this dataset. A few ideas that I had were to find and analyze:The distribution of post scores (specifically the (nearly) 100k questions)Word distributions and frequencies (e.g. -grams)Post Thread networks snippets ( being the language frequently used on Stack Exchange to format equations and other relations)Mathematical propositions (e.g. theorems, lemmas, axioms) mentioned in postsFamous mathematicians and propositions that are named after themLets tackle these one at a time. Since there are over 237,000 posts on MathOverflow in total, the distribution of their scores must be very large. Lets look at this distribution, noting that some post scores can be negative if they are downvoted by users in the community:&#10005allScores=EntityValue[\"StackExchange. Mathoverflow:Post\",EntityProperty[\"StackExchange. Mathoverflow:Post\", \"Score\"]];&#10005postScoreDistribution=Tally[allScores];&#10005ListPlot[postScoreDistribution,PlotRange -> Full,PlotTheme->\"Detailed\",ImageSize->400]Thats hard to readit looks better on a log-log scale, and it becomes mostly straight beyond the first several points:&#10005ListLogLogPlot[postScoreDistribution,PlotRange -> All,PlotTheme->\"Detailed\",ImageSize->400]Lets focus on the positive post scores below 50:&#10005scoresBelowFifty=EntityValue[EntityClass[\"StackExchange. Mathoverflow:Post\",\"Score\"->Between[{1,50}]],\"Score\"];&#10005ListPlot[Tally[scoresBelowFifty],PlotRange->Full,Filling->Axis,PlotTheme->\"Detailed\",ImageSize->400]It looks like it might be a log-normal distribution, so lets find the fitting parameters for it:&#10005distributionParameters=FindDistributionParameters[scoresBelowFifty,LogNormalDistribution[μ,σ]]Plotting both on the same (normalized) scale shows they agree quite well:&#10005With[{pdf=PDF[LogNormalDistribution[μ,σ]/.distributionParameters,x]},Show[{Plot[pdf,{x,0,50},PlotRange -> All,PlotTheme->\"Detailed\",PlotLegends->Placed[{pdf},{Right,0.75}],PlotStyle->Red,ImageSize->400],ListPlot[{#1,#2/Length[scoresBelowFifty]}&@@@Tally[scoresBelowFifty],PlotRange->All,Filling->Axis,PlotStyle->Blue]}]]We can repeat this analysis on the (almost) 100k questions:&#10005allQuestionScores=EntityValue[EntityClass[\"StackExchange. Mathoverflow:Post\",\"PostType\"->Entity[\"StackExchange:PostType\", \"1\"]],EntityProperty[\"StackExchange. Mathoverflow:Post\", \"Score\"]];&#10005allQuestionScores//Length&#10005questionScoreDistribution=Tally[allQuestionScores];&#10005ListLogLogPlot[questionScoreDistribution,PlotRange -> All,PlotTheme->\"Detailed\",ImageSize->400]&#10005questionScoresBelowFifty=EntityValue[EntityClass[\"StackExchange. Mathoverflow:Post\",{\"PostType\"->Entity[\"StackExchange:PostType\", \"1\"],\"Score\"->Between[{1,50}]}],EntityProperty[\"StackExchange. Mathoverflow:Post\", \"Score\"]];questionScoreDistributionParameters=FindDistributionParameters[questionScoresBelowFifty,LogNormalDistribution[μ,σ]]&#10005With[{pdf=PDF[LogNormalDistribution[μ,σ]/.questionScoreDistributionParameters,x]},Show[{Plot[pdf,{x,0,50},PlotRange -> All,PlotTheme->\"Detailed\",PlotLegends->Placed[{pdf},{Right,0.75}],PlotStyle->Red,ImageSize->400],ListPlot[{#1,#2/Length[questionScoresBelowFifty]}&@@@Tally[questionScoresBelowFifty],PlotRange->All,Filling->Axis,PlotStyle->Blue]}]]There are many words that appear in mathematics that are not found in typical English. Some examples include names of mathematicians (e.g. Riemann, Euler, etc.) or words that have special meanings (integral, matrix, group, ring, etc. ).We can start to investigate these words by gathering all of the post bodies:&#10005postBodies=EntityValue[\"StackExchange. Mathoverflow:Post\",\"Body\"];Well need to create functions to normalize strings (normalizeString) and extract sentences (extractSentences), removing HTML tags and replacing any equations with :&#10005normalizeString=StringReplace[{Shortest[\"$$\"~~__~~\"$$\"]:>\"\\[ScriptCapitalM]\\[ScriptCapitalA]\\[ScriptCapitalT]\\[ScriptCapitalH]\",Shortest[\"$\"~~__~~\"$\"]->\"\\[ScriptCapitalM]\\[ScriptCapitalA]\\[ScriptCapitalT]\\[ScriptCapitalH]\",Shortest[\"\\\\begin{equation*}\"~~__~~\"\\\\end{equation*}\"]->\"\\[ScriptCapitalM]\\[ScriptCapitalA]\\[ScriptCapitalT]\\[ScriptCapitalH]\",Shortest[\"\\\\begin{align}\"~~__~~\"\\\\end{align}\"]->\"\\[ScriptCapitalM]\\[ScriptCapitalA]\\[ScriptCapitalT]\\[ScriptCapitalH]\",Shortest[\"\"]->\"\",Shortest[\"\"]->\"\",Shortest[\"\"]->\"\",\"\"->\"\",\"\"->\"-\",\"\\\"\"->\"\",\"&nbsp\"->\"\",Whitespace->\" \"}];extractSentences=RightComposition[normalizeString,StringSplit[#,\".\"|\",\"|\"? \"|\";\"|\":\"|\"(\"|\")\"|\"+\"]&];Well also need to extract, count up and sort the words from all of the post bodies:&#10005wordToCount=postBodies//RightComposition[extractSentences,Flatten/*StringSplit/*ToLowerCase,Flatten,Counts/*ReverseSort];This gives a list of almost 400k words:&#10005wordToCount//LengthWe can trim it down to just the top 500 words, being careful to remove some extra noise with websites, equations, inequalities and single letters:&#10005topWordToCount=KeySelect[wordToCount[[;;1000]],Not[StringMatchQ[#, \"*'*\"|\"*=*\"|\"*1&]//Take[#,UpTo[500]]&;Note that  is the most common word, since all equations were replaced with it:&#10005Dataset[topWordToCount[[;;20]]]Its useful to visualize it as a word cloud:&#10005WordCloud[topWordToCount]Removing  and stopwords like the, is and of from the data will avoid some clutter:&#10005topWordToCountNoStopwords=KeySelect[wordToCount[[;;1000]],Not[StringMatchQ[#, \"*'*\"|\"*=*\"|\"*1&];topWordToCountNoStopwords=KeyTake[topWordToCountNoStopwords,topWordToCountNoStopwords//Keys//DeleteStopwords]//Take[#,UpTo[500]]&;&#10005Dataset[topWordToCountNoStopwords[[;;20]]]Now the results are more interesting and meaningful:&#10005WordCloud[topWordToCountNoStopwords]Of course, we can take this analysis further. We can get the frequencies for the top words in usual English with WordFrequencyData:&#10005wordToEnglishFrequency=WordFrequencyData[Keys[topWordToCountNoStopwords]];//AbsoluteTimingNormalize the counts of the words on MathOverflow, and then join the two as coordinates in 2D frequency space:&#10005wordToMOFrequency=N[topWordToCountNoStopwords/Total[topWordToCountNoStopwords]];&#10005wordToFrequencyCoordinates={wordToEnglishFrequency,wordToMOFrequency}//Merge[Identity]//Select[FreeQ[_Missing]];&#10005wordToFrequencyCoordinates[[;;3]]We can visualize these coordinates, adding a red region to the plot for words more commonly used in typical English than in MathOverflow posts (below  = ), and a gray region for words that are more commonly used in MathOverflow posts by less than a factor of 10 (below  = 10 ).This arbitrary factor allows us to narrow down the words that are much more common to MathOverflow than typical English, which appear in the white region (above  = 10 ):&#10005Show[LogLogPlot[{x,10x},{x,5*10^-11,0.02},PlotStyle->{Red,Gray},Filling->{2->{1},1->Bottom},FillingStyle->{1->Red,2->Gray},PlotStyle->PointSize[0.002],PlotRange->{{8*10^-11,0.02},{3*10^-4,0.02}},ImageSize->Large,PlotTheme->\"Detailed\",FrameLabel->{\"Fraction of English\",\"Fraction of MathOverflow. net\"}],ListLogLogPlot[wordToFrequencyCoordinates,PlotStyle->PointSize[0.002],PlotRange->{{8*10^-11,0.02},{3*10^-4,0.02}},ImageSize->Large,PlotTheme->\"Detailed\"]]We can take this another step further by looking at the words in the white region that are much more likely to occur on MathOverflow than they are in typical English:&#10005wordsMuchMoreCommonInMO=wordToFrequencyCoordinates//Select[Apply[Divide]/*LessThan[1/10]];wordsMuchMoreCommonInMO//Length&#10005ListLogLogPlot[wordsMuchMoreCommonInMO,ImageSize->500,PlotTheme->\"Detailed\",PlotStyle->PointSize[0.002],FrameLabel->{\"Fraction of English\",\"Fraction of MathOverflow. net\"}]Of course, an easy way to visualize this data is in a word cloud, where the words are weighted by combining their frequency of use via Norm:&#10005WordCloud[Norm/@wordsMuchMoreCommonInMO,WordSpacings->2]Of course, individual words are not the only way to analyze the MathOverflow corpus. We can create a function to compute -grams using Partition and recycling extractSentences from earlier:&#10005ClearAll[getNGrams];getNGrams[n_Integer? Positive]:=RightComposition[extractSentences,Map[ToLowerCase/*StringSplit/*Map[Counts[Partition[#,n,1]]&]/*Merge[Total]],Merge[Total],ReverseSort,(* Keep only the top 10,000 to save memory *)Take[#,UpTo[10000]]&];Next, well need to build a function to show the -grams in tables and word clouds, both with and without math (since putting them together would clutter the results a bit):&#10005ClearAll[showNGrams];showNGrams[nGramToCount_Association]:=Module[{phraseToCount,phraseToCountWithMath,phraseToCountWithoutMath},phraseToCount=KeyMap[StringRiffle,nGramToCount];phraseToCountWithMath=Take[phraseToCount,UpTo[200]]//Normal//Select[#,Not@StringFreeQ[#[[1]],\"\\[ScriptCapitalM]\\[ScriptCapitalA]\\[ScriptCapitalT]\\[ScriptCapitalH]\"]&,50]&//Association;phraseToCountWithoutMath=Take[phraseToCount,UpTo[200]]//Normal//Select[#,StringFreeQ[#[[1]],\"\\[ScriptCapitalM]\\[ScriptCapitalA]\\[ScriptCapitalT]\\[ScriptCapitalH]\"]&,50]&//Association;Print@WordCloud[Take[phraseToCountWithMath,UpTo[50]],ImageSize->500,WordSpacings->2];Print[Row[{Column[{Style[\" Including \\[ScriptCapitalM]\\[ScriptCapitalA]\\[ScriptCapitalT]\\[ScriptCapitalH] \",24,FontFamily->\"Source Code Pro\"],Dataset[Take[phraseToCountWithMath,UpTo[20]]]}],Column[{Style[\" Without \\[ScriptCapitalM]\\[ScriptCapitalA]\\[ScriptCapitalT]\\[ScriptCapitalH] \",24],Dataset[Take[phraseToCountWithoutMath,UpTo[20]]]}]},Spacer[20]]];Print@WordCloud[Take[phraseToCountWithoutMath,UpTo[50]],ImageSize->500,WordSpacings->2];];Looking at the 3-grams, there are lots of The number of, The set of, is there a and more. There are definitely signs of if and only if, but theyre not well captured here since were looking at 3-grams. They should show up later in the 4-grams, anyway. There is a lot of of Let  be,  is a, and similarits clear that MathOverflow users frequently use  for mathematical notation:&#10005postBodies//getNGrams[3]//showNGramsExpanding on the 3-grams, the 4-grams give several mathy phrases like if and only if, on the other hand, is it true that and the set of all. We also see more proof-like phrases like Let  be a,  such that  and similar. Its interesting how the two word clouds begin to show the split of proof-like phrases and natural language phrases:&#10005postBodies//getNGrams[4]//showNGramsWe see similar trends with the 5- and 6-grams:&#10005postBodies//getNGrams[5]//showNGrams&#10005postBodies//getNGrams[6]//showNGramsMoving past natural language processing, another way to analyze the MathOverflow site is as a network. We can create a network of MathOverflow users that communicate with each other. One way to do this is to connect two users if one user posts an answer to another users question. In this way, we can create a directed graph of MathOverflow users. Although its possible to do this graph-like traversal and matching with the usual EntityValue syntax, it could get somewhat messy. To start, we can write a symbolic representation of a SPARQL query to find all connections between question writers and the writers of answers, and then do some processing to turn it into a Graph:&#10005Needs[\"GraphStore`\"]&#10005questionerToAnswererGraph=Entity[\"StackExchange. Mathoverflow:Post\"]//RightComposition[SPARQLSelect[{RDFTriple[SPARQLVariable[\"post\"],post type,Entity[\"StackExchange:PostType\", \"1\"]],SPARQLPropertyPath[SPARQLVariable[\"post\"],{owner},SPARQLVariable[\"questioner\"]],SPARQLPropertyPath[SPARQLVariable[\"post\"],{SPARQLInverseProperty[parent post],owner},SPARQLVariable[\"answerer\"]]}->{SPARQLVariable[\"questioner\"],SPARQLVariable[\"answerer\"]}],Lookup[#,{SPARQLVariable[\"answerer\"],SPARQLVariable[\"questioner\"]}]&,Map[Apply[DirectedEdge]],Graph]From the icon of the output, we can see its a very large directed multigraph. Networks of this size have very little hope of being visualized easily, so we should find a way to reduce the size of it. We can trim down the size by writing a similar SPARQL query that limits us to posts with a few numerical mathematics post tags:&#10005questionerToAnswererGraphSmaller=Entity[\"StackExchange. Mathoverflow:Post\"]//RightComposition[SPARQLSelect[{RDFTriple[SPARQLVariable[\"post\"],post type,Entity[\"StackExchange:PostType\", \"1\"]],Alternatives[RDFTriple[SPARQLVariable[\"post\"],tags,Entity[\"StackExchange. Mathoverflow:Tag\", \"NumericalLinearAlgebra\"]],RDFTriple[SPARQLVariable[\"post\"],tags,Entity[\"StackExchange. Mathoverflow:Tag\", \"NumericalAnalysisOfPde\"]],RDFTriple[SPARQLVariable[\"post\"],tags,Entity[\"StackExchange. Mathoverflow:Tag\", \"NumericalIntegration\"]],RDFTriple[SPARQLVariable[\"post\"],tags,Entity[\"StackExchange. Mathoverflow:Tag\", \"RecreationalMathematics\"]]],SPARQLPropertyPath[SPARQLVariable[\"post\"],{owner},SPARQLVariable[\"questioner\"]],SPARQLPropertyPath[SPARQLVariable[\"post\"],{SPARQLInverseProperty[parent post],owner},SPARQLVariable[\"answerer\"]]}->{SPARQLVariable[\"questioner\"],SPARQLVariable[\"answerer\"]}],Lookup[#,{SPARQLVariable[\"answerer\"],SPARQLVariable[\"questioner\"]}]&,Map[Apply[DirectedEdge]],Graph[#,GraphStyle->\"LargeGraph\"]&]This graph is much smaller and can be more reasonably visualized. For simplicity, lets focus only on the largest (weakly) connected component:&#10005questionerToAnswererGraphSmallerConnected=First@WeaklyConnectedGraphComponents[questionerToAnswererGraphSmaller]We can group the vertices of the graph (MathOverflow users) by geography by using the location information users have entered into their profiles. Here, we can use Interpreter[\"Location\"] to handle a variety of input forms, including countries, cities, administrative divisions (such as states) and universities:&#10005userToLocation=EntityValue[VertexList[questionerToAnswererGraphSmallerConnected],\"Location\",\"NonMissingEntityAssociation\"];userToLocation=AssociationThread[Keys[userToLocation],Interpreter[\"Location\"]@RemoveDiacritics@Values[userToLocation]];The results are pretty good, giving over 250 approximate locations:&#10005userToLocation//Values//CountsBy[Head]Of course, these individual locations are not that helpful, as they are very localized. We can use GeoNearest to find the nearest geographic region as a basis for determining groups for the users:&#10005ClearAll[getRegion];getRegion[location_GeoPosition]:=First@getRegion[{location}];getRegion[locations:{__GeoPosition}]:=First[#,Missing[\"NotAvailable\"]]&/@DeleteCases[GeoNearest[GeoVariant[\"GeographicRegion\",\"Center\"],locations],Entity[\"GeographicRegion\", \"World\"],Infinity];Next, we group users into communities based on this geographic information:&#10005userToGeographicRegion=DeleteMissing@AssociationThread[Keys[#],getRegion@Values[#]]&[Select[userToLocation,MatchQ[_GeoPosition]]];geographicRegionToUsers=GroupBy[userToGeographicRegion,Identity,Keys];&#10005Length/@geographicRegionToUsersLastly, we can use CommunityGraphPlot to build a graphic that shows the geographic communities of the questioner-answerer network:&#10005regionToPointColor=Lookup[Darker@,Entity[\"GeographicRegion\", \"NorthAmerica\"]->Darker[Green,0.5],Entity[\"GeographicRegion\", \"Australia\"]->Orange,Entity[\"GeographicRegion\", \"Asia\"]->Purple,Entity[\"GeographicRegion\", \"SouthAmerica\"]->Darker[Red,0.25]|>,#,Brown]&;regionToLabelPlacement=Lookup[Below,Entity[\"GeographicRegion\", \"NorthAmerica\"]->After,Entity[\"GeographicRegion\", \"Asia\"]->Below|>,#,Above]&;regionToRotation=Lookup[-(π/2)|>,#,0]&;allRegionsToUsers=Append[geographicRegionToUsers,\"??? \"->Complement[VertexList[questionerToAnswererGraphSmallerConnected],Flatten@Values[geographicRegionToUsers]]];styledCommunities=KeyValueMap[Function[{region,users},With[{regionPointColor=regionToPointColor[region]},Labeled[Style[users,regionPointColor],Rotate[Style[Framed[region,Background->regionPointColor,RoundingRadius->5,FrameMargins->Small],FontColor->ResourceFunction[\"FontColorFromBackgroundColor\"][regionPointColor],FontSize->12],regionToRotation[region]],regionToLabelPlacement[region]]]],allRegionsToUsers];CommunityGraphPlot[questionerToAnswererGraphSmallerConnected,styledCommunities,ImageSize->500,Method->\"Hierarchical\",EdgeStyle->Directive[Opacity[0.25,Black],Arrowheads[Tiny]],VertexStyle->PointSize[Medium],CommunityRegionStyle->(Opacity[0.1,regionToPointColor[#]]&/@Keys[allRegionsToUsers]),CommunityBoundaryStyle->(Opacity[1,Black]&/@Keys[allRegionsToUsers])]Of course, we could do a similar analysis on connections between post owners and their commenters for posts tagged with linear-programming:&#10005postOwnerToCommenterGraph={Entity[\"StackExchange. Mathoverflow:Post\"],Entity[\"StackExchange. Mathoverflow:Comment\"]}//RightComposition[SPARQLSelect[{SPARQLPropertyPath[SPARQLVariable[\"post\"],{owner},SPARQLVariable[\"owner\"]],RDFTriple[SPARQLVariable[\"post\"],tags,Entity[\"StackExchange. Mathoverflow:Tag\", \"LinearProgramming\"]],SPARQLPropertyPath[SPARQLVariable[\"post\"],{comments,user},SPARQLVariable[\"commenter\"]]}->{SPARQLVariable[\"owner\"],SPARQLVariable[\"commenter\"]}],Lookup[#,{SPARQLVariable[\"commenter\"],SPARQLVariable[\"owner\"]}]&,Map[Apply[DirectedEdge]],Graph[#,GraphStyle->\"LargeGraph\"]&]However, further analysis on this network will be left as an exercise for the reader. , in its various forms, has been around for over 40 years, and is widely used in math and science for typesetting. On MathOverflow, there are not many posts without it, so exploring  snippets can give interesting insights into the content available on the site. First, we need to extract the  snippets from post bodies. Consider a simple example post:&#10005Entity[\"StackExchange. Mathoverflow:Post\", \"40686\"][\"Body\"]We can write a function to extract the  snippets in a string, noting the two main input forms (\"$$$$\" or \"\\\\begin{}\\\\end[...]\"):&#10005ClearAll[extractTeXSnippets];extractTeXSnippets[s_String] :=Module[{dd,d,o},dd=StringCases[s,Shortest[\"$$\"~~__~~\"$$\"]];d=StringCases[StringReplace[s,Shortest[\"$$\"~~__~~\"$$\"]:>\"\"],Shortest[\"$\"~~__~~\"$\"]];o=StringCases[s,Alternatives[Shortest[\"\\\\begin{equation*}\"~~__~~\"\\\\end{equation*}\"],Shortest[\"\\\\begin{align}\"~~__~~\"\\\\end{align}\"]]];StringReplace[Join[dd, d,o], {\".$\":>\"$\"}]]Testing this on the simple example gives the snippets wrapped in dollar signs:&#10005extractTeXSnippets[body]Of course, once we have  snippets, it would be valuable to format them into actual typesetting thats easier on the eyes than the raw  code. We can write a quick function to do this with proper formatting:&#10005blackboardBoldRules=character to double struck;frakturGothicRules=character to gothic;formatTeXSnippet[s_String] :=Which[StringMatchQ[s, \"$\\\\mathbb{\"~~ _ ~~ \"}$\"],StringReplace[s,\"$\\\\mathbb{\"~~ a_ ~~ \"}$\":> a]/.blackboardBoldRules,StringMatchQ[s, \"$\\\\mathbb \"~~ _ ~~ \"$\"],StringReplace[s,\"$\\\\mathbb \"~~ a_ ~~ \"$\":> a]/.blackboardBoldRules,StringMatchQ[s, \"$\\\\mathfrak{\"~~ _ ~~ \"}$\"],StringReplace[s,\"$\\\\mathfrak{\"~~ a_ ~~ \"}$\":> a]/.frakturGothicRules,StringMatchQ[s, \"$\\\\mathfrak \"~~ _ ~~ \"$\"],StringReplace[s,\"$\\\\mathfrak \"~~ a_ ~~ \"$\":> a]/.frakturGothicRules,StringMatchQ[s, \"$\\\\mathcal{\"~~ _ ~~ \"}$\"],Style[StringReplace[s,\"$\\\\mathcal{\"~~ a_ ~~ \"}$\":>a],FontFamily->\"Snell Roundhand\"],StringMatchQ[s, \"$\\\\mathcal \"~~ _ ~~ \"$\"],Style[StringReplace[s,\"$\\\\mathcal \"~~ a_ ~~ \"$\":>a],FontFamily->\"Snell Roundhand\"],True,StringReplace[s,\"\"->\"_\"]//RightComposition[ImportString[#,\"TeX\"]&,FirstCase[#,c:Cell[_,\"InlineFormula\"|\"NumberedEquation\"]:>DisplayForm[c],Missing[\"NotAvailable\"],]&]] We can test the results on the previously extracted  snippets:&#10005AssociationMap[formatTeXSnippet,extractTeXSnippets[body]]//KeyValueMap[List]//Grid[#,Frame->All]&We can also test them on a completely different post:&#10005Entity[\"StackExchange. Mathoverflow:Post\", \"40686\"][\"Body\"] //    extractTeXSnippets // AssociationMap[formatTeXSnippet] //  KeyValueMap[List] // Grid[#, Frame -> All, Alignment -> Left] &This system works well, so we should make it easier to use. We can do this by hooking up these functions as a property for posts, keeping the formatting function separate so that analysis can still be done on the raw strings:&#10005EntityProperty[\"StackExchange. Mathoverflow:Post\",\"TeXSnippets\"][\"Label\"]=\"TEX snippets\";EntityProperty[\"StackExchange. Mathoverflow:Post\",\"TeXSnippets\"][\"DefaultFunction\"]=Function[entity,extractTeXSnippets[entity[\"Body\"]]];Now we can just call the property on an entity instead:&#10005Entity[\"StackExchange. Mathoverflow:Post\", \"67739\"][\"TeXSnippets\"]//Map[formatTeXSnippet]From here, it should be easy to extract all of the  fragments from all of the posts:&#10005allTeXSnippets=EntityValue[\"StackExchange. Mathoverflow:Post\",\"TeXSnippets\"];A simple way to analyze the  snippets is to count up the number of times each fragment is used:&#10005teXToCount=allTeXSnippets//RightComposition[Flatten,Counts,ReverseSort];There are almost one million unique  snippets used in the post bodies on MathOverflow:&#10005teXToCount//LengthWe can also make a simple word cloud from the top 100 snippets:&#10005teXToCount[[;;100]]//KeyMap[formatTeXSnippet]//WordCloud[#,MaxItems->All,ImageSize->400]&Its easy to see that there are a lot of single-letter snippets. But there are a lot more interesting things hiding beyond these top 100. Lets take a look at a few different cases! Integrals are fairly easy to find with some simple string pattern matching:&#10005integralToCount=KeySelect[teXToCount,StringMatchQ[(\"$\"|\"$$\")~~(Whitespace|\"\")~~\"\\\\int\"~~__~~\"$\"]];integralToCount//LengthLooking at the top 50 gives some interesting resultssome very simple, and some rather complex:&#10005WordCloud[integralToCount[[;;50]]//KeyMap[formatTeXSnippet],ImageSize->400,WordSpacings->2]Another interesting subset of  snippets to consider is equations. Again, we can find these with some string pattern matching that requires an equals sign:&#10005equations=KeySelect[teXToCount,StringMatchQ[(\"$\"|\"$$\")~~(Whitespace|\"\")~~__~~\" = \"~~__~~\"$\"]];equations//LengthVisualizing the top 50 gives mostly single-letter variable assignments to numbers:&#10005WordCloud[equations[[;;50]]//KeyMap[formatTeXSnippet],ImageSize->400,WordSpacings->2]If we look at the single-letter variable assignments, we can find the minimum and maximum values of <number> for each <letter>.Note that this includes a list of special  characters, such as \\alpha:&#10005teXRepresentationToCharacter=TeX to character;&#10005specialLettersPattern=Alternatives@@Keys[teXRepresentationToCharacter];&#10005variableEqualsNumberDistributions=equations//Keys//StringCases[(\"$$\"|\"$\")~~lhs:(specialLettersPattern|LetterCharacter)~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~rhs:NumberString~~(Whitespace|\"\")~~(\"$$\"|\"$\"):>Rule[ToUpperCase[lhs/.teXRepresentationToCharacter],ToExpression[rhs]]]//Merge[Counts];&#10005KeySort[MinMax/@Keys/@variableEqualsNumberDistributions]//KeyValueMap[Prepend[N@#2,#1<>\" | \"<>ToLowerCase[#1]]&]//Grid[#,Frame->All]&Its interesting to see that most letters are positive, but S is strangely very negative. Its also interesting to note the very large scale of U, V and W. Perhaps not surprisingly, N is the most common letter, though its neighbor O is the least common:&#10005ReverseSort[Total/@variableEqualsNumberDistributions]//BarChart[#//KeySort//Reverse,ChartLabels->Automatic,PlotTheme->\"Detailed\",ImageSize->400,BarOrigin->Left,AspectRatio->1.5]&Trimming these single-variable assignments out of the original equation word cloud makes the results a bit more diverse:&#10005WordCloud[KeySelect[equations,StringMatchQ[(\"$$\"|\"$\")~~(specialLettersPattern|LetterCharacter)~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~NumberString~~(Whitespace|\"\")~~(\"$$\"|\"$\")]/*Not][[;;50]]//KeyMap[formatTeXSnippet],ImageSize->400,WordSpacings->2]Its interesting to see that there are a lot of letters assigned to (or compared with) another letter. We can make a simple graph that connects two letters in these equations, again taking into account special characters like \\alpha:&#10005letterGraph=Keys[equations]//StringCases[(\"$$\"|\"$\")~~lhs:(specialLettersPattern|LetterCharacter)~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~rhs:LetterCharacter~~(Whitespace|\"\")~~(\"$$\"|\"$\"):>(DirectedEdge[lhs,rhs]/.teXRepresentationToCharacter)]//Flatten//Counts//KeySortBy[First];The graph, without combining upper and lowercase letters, is quite messy:&#10005Graph[Union@Cases[Keys[letterGraph],_String,Infinity],Keys[letterGraph],GraphLayout->\"CircularEmbedding\",EdgeWeight->Normal[letterGraph],EdgeStyle->Normal[Opacity[N@(#/Max[letterGraph])]&/@letterGraph],VertexLabels->Placed[Automatic,Center],VertexLabelStyle->Directive[Bold,Small],VertexSize->Large,PlotTheme->\"Web\",ImageSize->400]If we combine the upper and lowercase letters, the graph becomes a little bit cleaner:&#10005upperCaseLetterGraph=Merge[Normal[letterGraph]/.c_String:>ToUpperCase[c],Total];Graph[Union@Cases[Keys[upperCaseLetterGraph],_String,Infinity],Keys[upperCaseLetterGraph],GraphLayout->\"CircularEmbedding\",EdgeWeight->Normal[upperCaseLetterGraph],EdgeStyle->Normal[Opacity[N@(#/Max[letterGraph])]&/@upperCaseLetterGraph],VertexLabels->Placed[Automatic,Center],VertexLabelStyle->Bold,VertexSize->Large,PlotTheme->\"Web\",ImageSize->400]If we again remove these equation types, the word cloud becomes even cleaner:&#10005WordCloud[KeySelect[equations,StringMatchQ[Alternatives[(\"$$\"|\"$\")~~(specialLettersPattern|LetterCharacter)~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~NumberString~~(Whitespace|\"\")~~(\"$$\"|\"$\"),(\"$$\"|\"$\")~~(specialLettersPattern|LetterCharacter)~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~LetterCharacter~~(Whitespace|\"\")~~(\"$$\"|\"$\")]]/*Not][[;;50]]//KeyMap[formatTeXSnippet],ImageSize->400,WordSpacings->2]Another interesting subset of equations to look into is functional equations. With a little bit of string pattern matching, we can find many examples:&#10005functionalEquations=KeySelect[equations,StringMatchQ[___~~(f:_)~~\"(\"~~__~~\")\"~~__~~(f:_)~~\"(\"~~__~~\")\"~~___]];functionalEquations//Length&#10005functionalEquations[[;;16]]//Keys//Map[formatTeXSnippet]//Multicolumn[#,Frame->All]&By focusing on functional equations that have one function with arguments on the left side of an equals sign, we get fewer results:&#10005functionalEquations2=KeySelect[equations,StringMatchQ[(\"$$\"|\"$\")~~(f:LetterCharacter)~~\"(\"~~__~~\")\"~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~__~~(f:_)~~\"(\"~~__~~\")\"~~___~~(\"$$\"|\"$\")]];functionalEquations2//Length&#10005functionalEquations2[[;;16]]//Keys//Map[formatTeXSnippet]//Multicolumn[#,Frame->All]&However, well need to go further to find equations that are easier to work with. Lets limit ourselves to single-letter, single-argument functions:&#10005functionalEquations3=KeySelect[equations,StringMatchQ[(\"$$\"|\"$\")~~(f:LetterCharacter)~~\"(\"~~x:LetterCharacter~~\")\"~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~__~~(f:_)~~\"(\"~~x:_~~\")\"~~___~~(\"$$\"|\"$\")]];functionalEquations3//Length&#10005functionalEquations3[[;;16]]//Keys//Map[formatTeXSnippet]//Multicolumn[#,Frame->All]&This is much more pointed, but we can go further. If we limit ourselves to functional equations with only one equals sign with single, lowercased arguments that only consist of a single head and argument (modulo operators and parentheses), we find just six equations:&#10005functionalEquations4=KeySelect[equations,StringMatchQ[s:((\"$$\"|\"$\")~~(f:LetterCharacter)~~\"(\"~~x:LetterCharacter? LowerCaseQ~~\")\"~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~__~~(f:_)~~\"(\"~~x:_~~\")\"~~___~~(\"$$\"|\"$\"))/;(StringCount[s,\"=\"]===1&&StringFreeQ[s,\"\\\\\"~~LetterCharacter..]&&Complement[Union[Characters[s]],{\"$\",\"^\",\"(\",\")\",\"-\",\"+\",\"=\",\"{\",\"}\",\". \",\" \",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"}]===Sort[{f,x}])]];functionalEquations4//Length&#10005functionalEquations4//Keys//Map[formatTeXSnippet]//Column[#,Frame->All]&Interestingly, there are only two functionally unique equations in this list:f(x) = 1 + x f(x)^2f(x) = 1 + x^2 f(x)^2If we clean up these functional equations, we can put them through Interpreter[\"TeXExpression\"] to get actual Wolfram Language representations of them:&#10005interpretedFunctionalEquations=Interpreter[\"TeXExpression\"][StringReplace[f:LetterCharacter~~\"(\"~~x:LetterCharacter~~\")^\"~~(\"{\"|\"\")~~n:DigitCharacter~~(\"}\"|\"\"):>\"(\"<>f<>\"(\"<>x<>\")\"<>\")^\"<>n]/@Keys[functionalEquations4]]/.C[x_]:>F[x]Finally, we can solve these equations with RSolve:&#10005AssociationMap[Replace[eqn:((f_)[x_]==rhs_):>RSolve[eqn,f[x],x]],interpretedFunctionalEquations]Moving past equations, another common notation among mathematicians is big O notation. Frequently used in computational complexity and numerical error scaling, this notation should surely appear somewhat frequently on MathOverflow. Lets take a look by finding  snippets wrapped in O that consist of a single argument and have equal numbers of open-and-close parentheses:&#10005bigONotationArguments=StringCases[Keys[teXToCount],Shortest[\"O(\"~~args__~~\")\"]/;StringFreeQ[args,\",\"|\";\"]&&StringCount[args,\"(\"]===StringCount[args,\")\"]:>\"$$\"<>args<>\"$$\"]//Flatten//Counts//ReverseSort;bigONotationArguments//LengthThe results are varied:&#10005bigONotationArguments[[;;100]]//KeyMap[formatTeXSnippet]//WordCloud[#,WordSpacings->3]&One can note that many of these results are functionally equivalentthey differ only in the letter chosen for the variable. We can clean these cases up with a little bit of effort:&#10005normalizeBigOStrings=StringReplace[{(* Any constant number  1 *)\"$$\"~~NumberString~~\"$$\"->\"$$1$$\",\"$$\"~~LetterCharacter~~\"$$\"->\"$$n$$\",\"$$\"~~LetterCharacter~~\"^\"~~exp:(DigitCharacter|(\"{\"~~__~~\"}\"))~~\"$$\":>\"$$n^\"<>exp<>\"$$\",\"$$\"~~LetterCharacter~~\"\\\\log \"~~LetterCharacter~~\"$$\":>\"$$n\\\\log n$$\",\"$$\"~~b:DigitCharacter~~\"^\"~~x_~~\"/\"~~x_~~\"$$\":>\"$$\"<>b<>\"^n/n$$\",\"$$\"~~numerator:DigitCharacter~~\"/\"~~LetterCharacter~~\"$$\":>\"$$\"<>numerator<>\"/n$$\",\"$$\"~~factor:DigitCharacter~~LetterCharacter~~\"$$\":>\"$$\"<>factor<>\"n$$\",\"$$1/\"~~LetterCharacter~~\"^\"~~exp:(DigitCharacter|(\"{\"~~__~~\"}\"))~~\"$$\":>\"$$1/n^\"<>exp<>\"$$\",\"$$1/|\"~~LetterCharacter~~\"|^\"~~exp:(DigitCharacter|(\"{\"~~__~~\"}\"))~~\"$$\":>\"$$1/|n|^\"<>exp<>\"$$\",\"$$\"~~LetterCharacter~~\"^\"~~exp:(DigitCharacter|(\"{\"~~__~~\"}\"))~~\"\\\\log \"~~LetterCharacter~~\"$$\":>\"$$n^\"<>exp<>\"\\\\log n$$\",\"$$|\"~~LetterCharacter~~\"|^\"~~exp:(DigitCharacter|(\"{\"~~__~~\"}\"))~~\"$$\":>\"$$|n|^\"<>exp<>\"$$\",\"$$\\\\\"~~op:(\"log\"|\"dot\")~~Whitespace ~~LetterCharacter~~\"$$\":>\"$$\\\\\"<>op<>\" n$$\",\"$$\\\\log|\"~~LetterCharacter~~\"|$$\"->\"$$\\\\log|n|$$\",\"$$\\\\sqrt{\"~~LetterCharacter~~\"}$$\":>\"$$\\\\sqrt{n}$$\",\"$$\"~~LetterCharacter~~\"/\\\\log \"~~LetterCharacter~~\"$$\":>\"$$n/\\\\log n$$\"}/.LetterCharacter->(specialLettersPattern|LetterCharacter)];&#10005normalizedBigOArguments=Normal[bigONotationArguments]//RightComposition[GroupBy[#,First/*normalizeBigOStrings->Last,Total]&,ReverseSort];Now the data is much cleaner:&#10005normalizedBigOArguments[[;;15]]//KeyMap[formatTeXSnippet]//Dataset&#10005normalizedBigOArguments[[;;100]]//Keys//Map[formatTeXSnippet]//MulticolumnAnd the word cloud looks much nicer:&#10005wc=normalizedBigOArguments[[;;50]]//KeyMap[formatTeXSnippet]//WordCloud[#,WordSpacings->3,ImageSize->400,ScalingFunctions->Sqrt]&Lastly, since these are arguments to O, lets set the word cloud as an argument of O to make a nice picture:&#10005Style[HoldForm[O][wc],90]//TraditionalForm Another way to analyze MathOverflow is to look at the mathematical propositions and famous mathematicians that are mentioned in the post bodies. An easy way to do this is to use more entity stores to keep track of the different types. To begin, lets set up an EntityStore for mathematical propositions and their types. Specifically, we can set up \"MathematicalPropositionType\" for base words like theorem, hypothesis and conjecture, and \"MathematicalProposition\" for specific propositions like the mean value theorem and Zorns lemma. The proposition types will serve as a means of programmatically finding the specific propositions, so well need to pre-populate \"MathematicalPropositionType\" with entities, but we can leave it empty of entities for nowwell populate that type in the store by processing the post bodies, but well do that next. Note that Ive added some properties to keep track of the propositions found in each post. Specifically, \"Wordings\" will hold an Association with strings for the keys and the counts of each of those strings for the values. Additionally, well set up \"MentionedPostCount\" to keep track of the number of times a post is mentioned:&#10005propositionStore=EntityStore[{\"MathematicalPropositionType\"->\"theorem\"|>,\"Hypothesis\"->\"hypothesis\"|>,\"Principle\"->\"principle\"|>,\"Conjecture\"->\"conjecture\"|>,\"Thesis\"->\"thesis\"|>,\"Lemma\"->\"lemma\"|>,\"Corollary\"->\"corollary\"|>,\"Axiom\"->\"axiom\"|>|>,\"Properties\"->\"label\"|>|>|>,\"MathematicalProposition\"->,\"Properties\"->\"proposition type\"|>,\"Wordings\"->\"wordings\",\"DefaultFunction\"->Function[],\"FormattingFunction\"->ReverseSort|>,\"Label\"->\"label\",\"DefaultFunction\"->Function[entity,entity[\"Wordings\"]//Keys//First]|>,\"MentionedPostCount\"->\"mentioned post count\",\"DefaultFunction\"->Function[0]|>|>|>}];EntityUnregister/@propositionStore[];EntityRegister[propositionStore]Now that the EntityStore is set up and registered, we can use the properties I set up in the store. Lets start with a list of theorems that dont have names in them:&#10005$specialTheorems={\"prime number theorem\",\"central limit theorem\",\"implicit function theorem\",\"spectral theorem\",\"incompleteness theorem\",\"universal coefficient theorem\",\"intermediate value theorem\",\"mean value theorem\",\"uniformization theorem\",\"inverse function theorem\",\"four color theorem\",\"binomial theorem\",\"index theorem\",\"fundamental theorem of algebra\",\"residue theorem\",\"dominated convergence theorem\",\"open mapping theorem\",\"ergodic theorem\",\"fundamental theorem of calculus\",\"h-cobordism theorem\",\"closed graph theorem\",\"modularity theorem\",\"adjoint functor theorem\",\"geometrization theorem\",\"primitive element theorem\",\"fundamental theorem of arithmetic\",\"fixed point theorem\",\"4-color theorem\",\"four colour theorem\",\"isotopy extension theorem\",\"proper base change theorem\",\"well-ordering theorem\",\"loop theorem\",\"slice theorem\",\"odd order theorem\",\"isogeny theorem\",\"group completion theorem\",\"convolution theorem\",\"reconstruction theorem\",\"equidistribution theorem\",\"contraction mapping theorem\",\"principal ideal theorem\",\"ergodic decomposition theorem\",\"orbit-stabilizer theorem\",\"4-colour theorem\",\"tubular neighborhood theorem\",\"three-squares theorem\",\"martingale representation theorem\",\"purity theorem\",\"triangulation theorem\",\"multinomial theorem\",\"graph minor theorem\",\"strong approximation theorem\",\"universal coefficients theorem\",\"localization theorem\",\"positive mass theorem\",\"identity theorem\",\"cellular approximation theorem\",\"transfer theorem\",\"bounded convergence theorem\",\"fundamental theorem of symmetric functions\",\"subadditive ergodic theorem\",\"annulus theorem\",\"rank-nullity theorem\",\"elliptization theorem\"};Next, we can build a function that will introduce new \"MathematicalProposition\" entities, keeping track of how often they are mentioned, their types and specific wordings for later use in cleaning things up. Note that we strip off any possessives and remove special characters via RemoveDiacritics:&#10005ToCamelCase[s_String]:=First@ToCamelCase[{s}];ToCamelCase[s:{__String}]:=StringSplit[RemoveDiacritics[s]]//Map[Capitalize/*StringJoin];toStandardName=ToCamelCase[StringReplace[StringRiffle@StringTrim[StringSplit[#],\"'s\"],{\"'\"->\"\",Except[LetterCharacter|DigitCharacter]->\" \"}]]&;With[{propositionTypePattern=Alternatives@@EntityValue[EntityList[\"MathematicalPropositionType\"],label]},ClearAll[addPropositionEntity];addPropositionEntity[proposition_String]:=With[{entity=Entity[\"MathematicalProposition\",toStandardName[proposition]]},(* Keep track of mentions *)entity[\"MentionedPostCount\"]=Replace[entity[\"MentionedPostCount\"],{i_Integer:>i+1,_->0}];(* Keep track of specific wordings and their counts *)entity[\"Wordings\"]=Replace[Replace[entity[\"Wordings\"],Except[_Association]->],a_Association:>Append[a,proposition->Lookup[a,proposition,0]+1]];(* Extract PropositionType *)entity[\"PropositionType\"]=StringCases[proposition,propositionTypePattern,IgnoreCase->True]//Replace[{{x_String,___}:>Entity[\"MathematicalPropositionType\",Capitalize[ToLowerCase[x]]],_->Missing[\"NotAvailable\"]}];entity]];Note that there are currently no proposition entities:&#10005EntityList[\"MathematicalProposition\"]But if we run the list of special theorems through the function&#10005addPropositionEntity/@$specialTheorems; then there are proposition entities defined:&#10005EntityList[\"MathematicalProposition\"]//Take[#,UpTo[5]]&We should reset counters for the introduced entities to keep things uniform (the list I provided was fabricatedthose strings did not come from actual posts, so not resetting these values may throw off the numbers a bit):&#10005#[\"MentionedPostCount\"]=0;&/@EntityList[\"MathematicalProposition\"];#[\"Wordings\"]=Association[#[\"Label\"]->1];&/@EntityList[\"MathematicalProposition\"];Of course, we can go further and detect other forms of propositions. Specifically, lets look for propositions of the following forms: One of the special theorems we just introduced<person name> theorem (and similar)theorem of <person name> (and similar)When we find these propositions, we can add them as entities to the proposition EntityStore (via addPropositionEntity), as well as store them with the posts so lookups are faster (as they will already be stored in memory through the EntityStore).To start, well need to do some normalization. Heres a useful function that uses a list of words that should always be lowercased:&#10005$lowercaseWords= list of words that should be lowercased;lowerCaseSpecificWords= StringReplace[(#->ToLowerCase[#])&/@$lowercaseWords];Additionally, heres a list of ordinals and how to normalize them (including Lastfor example, as in Fermats last theorem):&#10005$ordinalToWord=\"First\",\"2nd\"->\"Second\",\"3rd\"->\"Third\",\"4th\"->\"Fourth\",\"5th\"->\"Fifth\",\"6th\"->\"Sixth\",\"7th\"->\"Seventh\",\"8th\"->\"Eighth\",\"9th\"->\"Ninth\",\"10th\"->\"Tenth\",\"last\"->\"Last\"|>;$ordinals=Join[Flatten[{ToLowerCase[#],#}&/@Values[$ordinalToWord]],List@@Keys[$ordinalToWord]]//ReverseSortBy[StringLength];Now we can create a function to extract propositions from strings, normalize them with normalizeString from earlier and then create new \"MathematicalProposition\" entities using addPropositionEntity:&#10005With[{propositionTypePattern=EntityList[\"MathematicalPropositionType\"]//label//Join[#,Capitalize/@#]&//Apply[Alternatives],upperCaseWordPattern=(WordBoundary|Whitespace ~~(_?UpperCaseQ ~~ (LetterCharacter| \"-\"|\"'\")..)~~WordBoundary|Whitespace),anyCaseWordPattern=(WordBoundary|Whitespace ~~((Alternatives@@$ordinals)|( (LetterCharacter| \"-\"|\"'\")..))~~WordBoundary|Whitespace),possibleOrdinalPattern=(Alternatives@@$ordinals~~Whitespace)|\"\"},extractNamedPropositions=RightComposition[normalizeString,DeleteDuplicates@Join[(* Case 1: E.g. \"central limit theorem\" *)StringCases[#,Alternatives@@$specialTheorems,IgnoreCase->True],StringCases[#,Alternatives[(* Case 3: \"(nth) Theorem of Something (Something (Something))\" *)Shortest[(WordBoundary|Whitespace)~~possibleOrdinalPattern~~propositionTypePattern ~~ Whitespace~~\"of\"~~Longest@Repeated[upperCaseWordPattern,3]],(* Case 2: \"Something (something (something)) Theorem\" *)Shortest[(WordBoundary|Whitespace)~~(x:upperCaseWordPattern)~~Longest@Repeated[anyCaseWordPattern,2]~~propositionTypePattern]/;(Not@StringMatchQ[StringTrim[x],\"The\"|\"A\"|\"Use\",IgnoreCase->True])]]]&,(* Remove cases with useless words in them *)Select[With[(* Ignore \"of\" so that case #3 is allowed *){split=DeleteCases[StringSplit[ToLowerCase[#],Whitespace|\"-\"],\"of\"]},split===DeleteCases[DeleteStopwords[split],\"using\"|\"like\"|\"phd\"|\"understanding\"|\"finally\"|\"concerning\"|\"regarding\"|\"ℳℋ\"|\"\"|\"satisfies\"|\"following\"|\"stated\"|\"usually\"|\"implies\"|\"hence\"|\"course\"|\"assuming\"|\"wikipedia\"|\"article\"|\"usual\"|\"actually\"|\"analysis\"|\"entitled\"|\"apply\"]]&],StringReplace[Normal@$ordinalToWord],lowerCaseSpecificWords,StringTrim,DeleteDuplicates,Map[addPropositionEntity]]];Lets try the function on a simple example:&#10005Entity[\"StackExchange. Mathoverflow:Post\", \"40686\"][\"Body\"]//extractNamedPropositionsWe can see that an entity was added to the store:&#10005EntityList[\"MathematicalProposition\"][[-3;;]]We can also see that its properties were populated:&#10005Entity[\"MathematicalProposition\", \"MartinAxiom\"][\"PropertyAssociation\"]Of course, we can automate this a bit more by introducing this function as a property for MathOverflow posts that will store the results in the EntityStore itself:&#10005EntityProperty[\"StackExchange. Mathoverflow:Post\",\"NamedPropositions\"][\"Label\"]=\"named propositions\";EntityProperty[\"StackExchange. Mathoverflow:Post\",\"NamedPropositions\"][\"DefaultFunction\"]=Function[entity,entity[\"NamedPropositions\"]=extractNamedPropositions[entity[\"Body\"]]];Lets test out the property on the same Entity as before:&#10005Entity[\"StackExchange. Mathoverflow:Post\", \"40686\"][\"NamedPropositions\"]We can see that the in-memory store has been populated:&#10005Entity[\"MathematicalProposition\"][\"EntityStore\"]Pro tip: in case you want to continue to work on an EntityStore youve been modifying in-memory in a future Wolfram Language session, you can Export Entity[\"type\"][\"EntityStore\"] to an MX file and then Import it in the new session. Just dont forget to register it with EntityRegister! At this point, we can now gather propositions mentioned in all of the MathOverflow posts, taking care to reset the counters again to avoid contamination of the results:&#10005(* Reset counters again to avoid contaminating the results *)#[\"MentionedPostCount\"]=0;&/@EntityList[\"MathematicalProposition\"];#[\"Wordings\"]=Association[#[\"Label\"]->1];&/@EntityList[\"MathematicalProposition\"];Note that this will take a while to run (it took about 20 minutes on my machine), but it will allow for a very thorough analysis of the sites content. After processing all of the posts, there are now over 10k entities in the proposition EntityStore:&#10005postToNamedPropositions=EntityValue[\"StackExchange. Mathoverflow:Post\",\"NamedPropositions\",\"EntityAssociation\"];&#10005EntityValue[\"MathematicalProposition\",\"Entities\"]//LengthHaving kept track of the wordings for each proposition was a good choicenow we can see that proposition entities will format with the most commonly used wording. For example, look at Stokes theorem:&#10005Entity[\"MathematicalProposition\", \"StokesTheorem\"][\"Wordings\"]Its named after George Gabriel Stokes, and so the correct possessive form ends in s, not s, despite about 15 percent of mentions using the incorrect form. Ill admit that this normalization is not perfectwhen someone removes the first s altogether, it is picked up in a different entity:&#10005Entity[\"MathematicalProposition\", \"StokeTheorem\"][\"Wordings\"]Rather than spend a lot of time and effort to normalize these small issues, Ill move on and work around these problems for now. Now that we have a lot of data on the propositions mentioned in the post bodies, we can visualize the most commonly mentioned propositions in a word cloud:&#10005topPropositionToCount=ReverseSort@EntityValue[EntityClass[\"MathematicalProposition\",{\"MentionedPostCount\"->TakeLargest[100]}],\"MentionedPostCount\",\"EntityAssociation\"];&#10005WordCloud[topPropositionToCount[[;;20]],ImageSize->400,WordOrientation->{{π/4,-(π/4)}}]It seems that the prime number theorem is the most commonly mentioned:&#10005topPropositionToCount[[;;20]]//DatasetWe can also see that about two-thirds of all propositions are theorems:&#10005propositionTypeBreakdown=EntityValue[\"MathematicalProposition\",\"PropositionType\"]//Counts//ReverseSort&#10005PieChart[propositionTypeBreakdown,ChartLabels->Placed[Automatic,\"RadialCallout\"],ImageSize->400,PlotRange->All]Now that we have the propositions, we can look for mathematician names in the propositions. To start, we can find all of the labels for the propositions:&#10005propositionToLabel=EntityValue[\"MathematicalProposition\",\"Label\",\"EntityAssociation\"];Next, well need to find the words in the labels, drop proposition types and stopwords and count them up (taking care to not separate names that start with de or von):&#10005commonWordsInPropositions=propositionToLabel//RightComposition[Values,StringReplace[prefix:\"Van der\"|\"de\"|\"De\"|\"von\"|\"Von\"~~(Whitespace|\"-\")~~name:(_?UpperCaseQ~~LetterCharacter):>StringReplace[prefix,Whitespace->\"_\"]<>\"_\"<>name],StringSplit/*Flatten/*DeleteStopwords,Counts/*ReverseSort,KeyDrop[Flatten@EntityValue[EntityClass[\"MathematicalPropositionType\",All],{\"CanonicalName\",\"Label\"}]]];Next, we can look for groups of mathematician names joined by dashes (taking care to remove words that are obviously not names):&#10005namePattern=((_?UpperCaseQ|(\"von\"|\"de\"))~~(LetterCharacter|\"_\")..);mathematiciansJoinedByDashes=Select[Keys[commonWordsInPropositions],StringMatchQ[namePattern~~Repeated[\"-\"~~namePattern,{1,Infinity}]]]//StringSplit[#,\"-\"]&//Flatten//Union//StringReplace[\"_\"->\" \"];mathematiciansJoinedByDashes=DeleteCases[mathematiciansJoinedByDashes,\"Anti\"|\"Foundation\"|\"Max\"|\"Flow\"|\"Min\"|\"Cut\"];mathematiciansJoinedByDashes//LengthAnother source of names is possessive words, as in Zorns lemma:&#10005possesiveNames=Keys[commonWordsInPropositions]//Select[StringEndsQ[\"'s\"|\"s'\"]];possesiveNames//LengthAfter some cleanup (e.g. removing inline  snippets and references to the Clay Mathematics Institutes Millennium Prize problems), most of these are likely last names for mathematicians:&#10005mathematicansByPossessives=StringTrim[possesiveNames,\"'s\"|\"s'\"]//RightComposition[StringSplit[#,\"-\"]&,Flatten/*DeleteDuplicates,StringReplace[\"_\"->\" \"],Select[StringMatchQ[#,_?UpperCaseQ~~(_?LowerCaseQ)~~___]&],Select[StringFreeQ[\"ℳℋ\"]]];mathematicansByPossessives=DeleteCases[mathematicansByPossessives,\"Clay\"|\"Millennium\"];mathematicansByPossessives//LengthCombining these two lists should result in a fairly complete list of mathematician names:&#10005allMathematicians=Union[mathematiciansJoinedByDashes,mathematicansByPossessives];allMathematicians//LengthThe results seem pretty decent:&#10005Multicolumn[RandomSample[allMathematicians,16]]Now we need to find possible ways to write down the names of each mathematician. After looking through the data, I found a few cases that needed to be corrected manually. Specifically, a few names are written out for the famous Jacob Lurie and R. Ranga Rao, so they need to be corrected to clean up the results a bit:&#10005mathematicianToPossibleNames=GroupBy[allMathematicians,RemoveDiacritics/*StringReplace[\"_\"->\" \"]/*toStandardName];mathematicianToPossibleNames[\"Lurie\"]=PrependTo[mathematicianToPossibleNames[\"Lurie\"],\"Jacob Lurie\"];mathematicianToPossibleNames[\"Ranga\"]=PrependTo[mathematicianToPossibleNames[\"Ranga\"],\"Ranga Rao\"];Now, we need to construct an Association to point from individual name words to their mathematicians:&#10005possibleNameToMathematicians=mathematicianToPossibleNames//RightComposition[KeyValueMap[Thread[Rule[##]]&],Flatten,GroupBy[Last->First],Map[Entity[\"Mathematician\",#]&/@Flatten[#]&]];Note that these entities do not formatwell create the EntityStore for them soon:&#10005possibleNameToMathematicians[[;;10]]From here, we can break the proposition labels into words that we can use to look up mathematicians (taking care to fix a few cases that need special attention for full names written out):&#10005propositonToLabelWords=propositionToLabel//RightComposition[Map[StringReplace[prefix:(\"De\"|\"de\"|\"von\"|\"Von\"|\"Jacob\"|\"Alex\"|\"Yoon\"|\"Ranga\")~~Whitespace|\"-\"~~name:(namePattern|\"Ho\"|\"Lurie\"|\"Lee\"|\"Rao\"):>prefix<>\"_\"<>name]/*(StringSplit[#,Whitespace|\"-\"]&)/*(StringTrim[#,\"'s\"|\"s'\"]&)/*StringReplace[\"_\"->\" \"]]];&#10005propositionToMathematicians=DeleteDuplicates@Flatten@DeleteMissing@Lookup[possibleNameToMathematicians,#]&/@propositonToLabelWords;With this done, we can add this data to the proposition store as a new property:&#10005EntityProperty[\"MathematicalProposition\",\"NamedMathematicians\"][\"Label\"]=\"named mathematicians\";KeyValueMap[(#1[\"NamedMathematicians\"]=#2)&,propositionToMathematicians];And by rearranging the data, we can build the data to create an EntityStore for mathematicians:&#10005mathematicanToPropositions=propositionToMathematicians//RightComposition[KeyValueMap[Thread[Rule[##]]&],Flatten,GroupBy[Last->First]];&#10005KeyTake[mathematicanToPropositions,{Entity[\"Mathematician\", \"DeFinnetti\"],Entity[\"Mathematician\", \"Lurie\"],Entity[\"Mathematician\", \"Ranga\"]}]Using this data, we can now build an EntityStore for mathematicians, taking into account the data weve accumulated:&#10005mathematicanData=Association@KeyValueMap[Rule[#1,First[#2],\"PossibleNames\"->#2,\"NamedPropositions\"->Lookup[mathematicanToPropositions,Entity[\"Mathematician\",#1]]|>]&,mathematicianToPossibleNames];&#10005mathematicianStore=EntityStore[{\"Mathematician\"->mathematicanData,\"Properties\"->\"label\"|>,\"PossibleNames\"->\"possible names\"|>,\"NamedPropositions\"->\"named propositions\"|>|>|>}];EntityUnregister/@mathematicianStore[];EntityRegister[mathematicianStore]Now we can see the propositions named after a specific mathematician, such as Euler:&#10005EntityValue[Entity[\"Mathematician\", \"Euler\"],\"NamedPropositions\"]At this point, many interesting queries are possible. As a start, lets look at a network that connects two mathematicians if they appear in the same proposition name. It sounds somewhat similar to the Mathematics Genealogy Project. An example might be the GrothendieckTarski axiom:&#10005Entity[\"MathematicalProposition\", \"GrothendieckTarskiAxiom\"][\"NamedMathematicians\"]First, grab all of the named mathematicians for each proposition, taking only those that have at least two:&#10005groupedMathematicianData=Select[EntityValue[\"MathematicalProposition\",\"NamedMathematicians\",\"EntityAssociation\"],Length[#]>=2&];&#10005groupedMathematicianData[[;;10]]With a little bit of analysis, we can see that the majority of propositions have two mathematicians, fewer have three, there are a few groups of four and there is only one group of five:&#10005groupedMathematicianData//Values//CountsBy[Length]//KeySortHere is the group of five, mentioned in this answer:&#10005TakeLargestBy[groupedMathematicianData,Length,1]Here are the top 25 results:&#10005TakeLargestBy[groupedMathematicianData,Length,25]//Map[Apply[UndirectedEdge]]//KeyValueMap[List]//Grid[#,Alignment->Center,Dividers->All]&We can complete our task by constructing a network of all grouped mathematicians:&#10005groupedMathematicianEdges=Flatten[(UndirectedEdge@@@Subsets[#,{2}])&/@Values[groupedMathematicianData]];groupedMathematicianEdges//LengthThe network is quite large and messy.&#10005Graph[groupedMathematicianEdges,ImageSize->400]With some more processing, we can add weights for repeated edges and use them to determine their opacity:&#10005weightedMathematicanEdges=ReverseSort[Counts[groupedMathematicianEdges]];&#10005connectedMathematiciansGraph=Graph[Keys[weightedMathematicanEdges],EdgeWeight->Normal[weightedMathematicanEdges],EdgeStyle->Normal[Opacity[Sqrt@N[#/Max[weightedMathematicanEdges]]]&/@weightedMathematicanEdges],PlotTheme->\"LargeGraph\",ImageSize->400]Here are the most common pairings of mathematicians:&#10005weightedMathematicanEdges[[;;10]]And heres an easy way to see who has the most propositions named after them, which can be done with SortedEntityClass, new in Mathematica 12:&#10005mathematicianToPropositions=EntityValue[SortedEntityClass[\"Mathematician\",EntityFunction[entity,Length[entity[\"NamedPropositions\"]]]->\"Descending\"],\"NamedPropositions\",\"EntityAssociation\"];mathematicianToPropositions//LengthHere are the top 25, along with the number of propositions in which they are mentioned on MathOverflow:&#10005Length/@mathematicianToPropositions[[;;25]]Despite his common appearance in theorems, Euler is surprisingly very low on this list at number 61:&#10005Position[Keys[mathematicianToPropositions],Entity[\"Mathematician\", \"Euler\"]]And heres the full distribution of mentions (on a log scale for easier viewing):&#10005Histogram[Length/@Values[mathematicianToPropositions],PlotRange->Full,ImageSize->400,PlotTheme->\"Detailed\",ScalingFunctions->\"Log\",FrameLabel->{\"# of Named Propositions\",\"# of Mathematicians\"}]Be sure to explore further using the newest functions coming soon in Version 12 of the Wolfram Language! Although I used a lot of Wolfram technology to explore the data on MathOverflow. net, here are some open areas that still remain to be explored:What fraction of all questions are unanswered? Which tags have the most answered or unanswered questions? Find named mathematical structures (e.g. integral, group, Riemann sum, etc. )Align entities to MathWorld for further investigationInvestigate the time series ratio of user count to post countIn fact, you can download and register the entity stores used in this post here:Pre-generated, incomplete MathOverflow. net EntityStoreCompleted MathOverflow. net EntityStoreCompleted mathematical proposition EntityStoreCompleted mathematician EntityStoreBrush up on your math skills with Fast Introduction for Math Students\n",
      "Sentences: 146\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "This post discusses new Wolfram Language features from the upcoming release of Version 12. Copyable input expressions and a downloadable notebook version of this post will be available when Version 12 is released. New Archive Conversion Utility in Version 12Soon there will be 100,000 questions on MathOverflow. net , a question-and-answer site for professional mathematicians! To celebrate this event, I have been working on a Wolfram Language utility package to convert archives of Stack Exchange network websites into Wolfram Language entity stores .The archives are hosted on the Internet Archive and are updated every few months. The package, although not yet publicly available, will be released in the coming weeks as part of Version 12 of the Wolfram Languageso keep watching this space for more news about the release! Although some data analysis can be done with tools such as the Stack Exchange Data Explorer , queries are usually limited in size or computation time, as well as to text-only formats. Additionally, they require some knowledge of SQL. But with a local copy of the data, much more can be done, including images, plots and graphs. With the utility package operating on a local archive, its easy to perform much deeper data analysis using all of the built-in tools in the Wolfram Language. In particular, Version 12 of the Wolfram Language adds support for RDF and SPARQL queries, as well as useful constructs such as FilteredEntityClass and SortedEntityClass .For professional mathematicians who already use Mathematica and the Wolfram Language, this utility allows for seamless investigation into the data on MathOverflow. net or any Stack Exchange network site. Feel free to follow along with me as I do some of this investigation by running the code in a notebook, or just sit back and enjoy the ride as we explore MathOverflow. net with the Wolfram Language! Importing a MathOverflow EntityStoreThe entity stores created by the utility package allow for quick access to the data in a format thats easy for Wolfram Language processing, such as queries using the Entity framework, machine learning functionality, visualization, etc. Lets start by downloading a pre-generated EntityStore from the Wolfram Cloud to the notebooks directory:&#10005 downloadedFile=URLDownload[CloudObject[\"StackExchange2EntityStore/mathoverflow. net. mx\"],NotebookDirectory[]]Import the EntityStore from the downloaded file:&#10005 store=Import[downloadedFile];The store is quite large, consisting of nearly three million entities in several entity types:&#10005 entityStoreMetaData=AssociationMap[Length[store[#,\"Entities\"]],\"Property Count\"->Length[store[#,\"Properties\"]]|>&,store[]]//ReverseSortBy[Lookup[\"Entity Count\"]];&#10005 Total[#\"Entity Count\"&/@entityStoreMetaData]Lastly, we need to register the EntityStore for use in the current session:This returns a list of all of the new entity types from the EntityStore that are now available through EntityValue (you can access them by registering the EntityStore via EntityRegister ).For those who are familiar with the Stack Exchange network, these types may be very familiar. But for those who are not, or if you just need a refresher, heres a basic rundown of a few of the different types:The remaining types not listed are beyond the scope of my post, but you can learn more about them in the README on the archives , or by visiting the frequently asked questions on any Stack Exchange network site. Accessing MathOverflow. net PostsNow that the EntityStore is loaded, we can access it through the Entity framework. Lets look at some random posts:&#10005 RandomEntity[\"StackExchange. Mathoverflow:Post\",3]The Post entities are formatted with the post type ( Q for question, A for answer), the user who authored the post in square brackets, a short snippet of the post and a hyperlink (the blue ) to the original post on the web. Many of the other entity types format similarlythis is to give proper context, allow for manual exploration on the site itself and give attribution to the original authors (they created the content on the site, after all).Taking just one of these posts, we can find a lot of information about it with a property association:&#10005 Entity[\"StackExchange. Mathoverflow:Post\", \"272527\"][{accepted answer,answer count,body,closed date,comment count,comments,community owned date,creation date,duplicate posts,favorite count,id,last activity date,last edit date,last editor,linked posts,owner,post type,score,tags,title,URL,view count},\"PropertyAssociation\"]For example, one may be interested in the posts for a given tag, such as set theory. We can find how many set theory questions have been asked:&#10005 EntityValue[EntityClass[\"StackExchange. Mathoverflow:Post\",{\"Tags\"->Entity[\"StackExchange. Mathoverflow:Tag\", \"SetTheory\"],\"PostType\"->Entity[\"StackExchange:PostType\", \"1\"] }],\"EntityCount\"]We can even see the intersections of different tags, such as set theory and plane geometry:&#10005 EntityValue[EntityClass[\"StackExchange. Mathoverflow:Post\",{\"Tags\"->ContainsAll[{Entity[\"StackExchange. Mathoverflow:Tag\", \"SetTheory\"],Entity[\"StackExchange. Mathoverflow:Tag\", \"PlaneGeometry\"]}],\"PostType\"->Entity[\"StackExchange:PostType\", \"1\"] }],\"Entities\"]Its important to note that as of this writing, the archives have not been updated to include the 100,000th question, so we can see that there are only 98,165 questions as of December 2, 2018:&#10005 EntityClass[\"StackExchange. Mathoverflow:Post\",\"PostType\"->Entity[\"StackExchange:PostType\", \"1\"]][\"EntityCount\"]&#10005 EntityValue[Entity[\"StackExchange. Mathoverflow:Post\"],\"LastPostTime\"]Of course, there is a seemingly endless number of queries one can make on this dataset. A few ideas that I had were to find and analyze: The distribution of post scores (specifically the (nearly) 100k questions) Word distributions and frequencies (e.g. -grams) Post Thread networks snippets ( being the language frequently used on Stack Exchange to format equations and other relations) Mathematical propositions (e.g. theorems, lemmas, axioms) mentioned in posts Famous mathematicians and propositions that are named after themLets tackle these one at a time. Since there are over 237,000 posts on MathOverflow in total, the distribution of their scores must be very large. Lets look at this distribution, noting that some post scores can be negative if they are downvoted by users in the community:&#10005 allScores=EntityValue[\"StackExchange. Mathoverflow:Post\",EntityProperty[\"StackExchange. Mathoverflow:Post\", \"Score\"]];&#10005 postScoreDistribution=Tally[allScores];&#10005 ListPlot[postScoreDistribution,PlotRange -> Full,PlotTheme->\"Detailed\",ImageSize->400]Thats hard to readit looks better on a log-log scale, and it becomes mostly straight beyond the first several points:&#10005 ListLogLogPlot[postScoreDistribution,PlotRange -> All,PlotTheme->\"Detailed\",ImageSize->400]Lets focus on the positive post scores below 50:&#10005 ListPlot[Tally[scoresBelowFifty],PlotRange->Full,Filling->Axis,PlotTheme->\"Detailed\",ImageSize->400]It looks like it might be a log-normal distribution, so lets find the fitting parameters for it:&#10005 distributionParameters=FindDistributionParameters[scoresBelowFifty,LogNormalDistribution[μ,σ]]Plotting both on the same (normalized) scale shows they agree quite well:&#10005 With[ {pdf=PDF[LogNormalDistribution[μ,σ]/.distributionParameters,x]}, Show[ { Plot[pdf,{x,0,50}, PlotRange -> All,PlotTheme->\"Detailed\", PlotLegends->Placed[{pdf},{Right,0.75}], PlotStyle->Red,ImageSize->400 ], ListPlot[ {#1,#2/Length[scoresBelowFifty]}&@@@Tally[scoresBelowFifty], PlotRange->All, Filling->Axis, PlotStyle->Blue ] } ] ]We can repeat this analysis on the (almost) 100k questions:&#10005 allQuestionScores=EntityValue[EntityClass[\"StackExchange. Mathoverflow:Post\",\"PostType\"->Entity[\"StackExchange:PostType\", \"1\"]],EntityProperty[\"StackExchange. Mathoverflow:Post\", \"Score\"]];&#10005 allQuestionScores//Length&#10005 questionScoreDistribution=Tally[allQuestionScores];&#10005 ListLogLogPlot[questionScoreDistribution,PlotRange -> All,PlotTheme->\"Detailed\",ImageSize->400]&#10005 questionScoresBelowFifty=EntityValue[EntityClass[\"StackExchange. Mathoverflow:Post\",{\"PostType\"->Entity[\"StackExchange:PostType\", \"1\"],\"Score\"->Between[{1,50}]}],EntityProperty[\"StackExchange. Mathoverflow:Post\", \"Score\"]]; questionScoreDistributionParameters=FindDistributionParameters[questionScoresBelowFifty,LogNormalDistribution[μ,σ]]&#10005 With[ {pdf=PDF[LogNormalDistribution[μ,σ]/.questionScoreDistributionParameters,x]}, Show[ { Plot[pdf,{x,0,50}, PlotRange -> All,PlotTheme->\"Detailed\", PlotLegends->Placed[{pdf},{Right,0.75}], PlotStyle->Red,ImageSize->400 ], ListPlot[ {#1,#2/Length[questionScoresBelowFifty]}&@@@Tally[questionScoresBelowFifty], PlotRange->All, Filling->Axis, PlotStyle->Blue ] } ] ]Words Much More Common in Mathematics Than Normal LanguageThere are many words that appear in mathematics that are not found in typical English. Some examples include names of mathematicians (e.g. Riemann, Euler, etc.) or words that have special meanings (integral, matrix, group, ring, etc. ).We can start to investigate these words by gathering all of the post bodies:&#10005 postBodies=EntityValue[\"StackExchange. Mathoverflow:Post\",\"Body\"];Well need to create functions to normalize strings ( normalizeString ) and extract sentences ( extractSentences ), removing HTML tags and replacing any equations with  :&#10005 normalizeString=StringReplace[ { Shortest[\"$$\"~~__~~\"$$\"]:>\"\\[ScriptCapitalM]\\[ScriptCapitalA]\\[ScriptCapitalT]\\[ScriptCapitalH]\", Shortest[\"$\"~~__~~\"$\"]->\"\\[ScriptCapitalM]\\[ScriptCapitalA]\\[ScriptCapitalT]\\[ScriptCapitalH]\", Shortest[\"\\\\begin{equation*}\"~~__~~\"\\\\end{equation*}\"]->\"\\[ScriptCapitalM]\\[ScriptCapitalA]\\[ScriptCapitalT]\\[ScriptCapitalH]\", Shortest[\"\\\\begin{align}\"~~__~~\"\\\\end{align}\"]->\"\\[ScriptCapitalM]\\[ScriptCapitalA]\\[ScriptCapitalT]\\[ScriptCapitalH]\", Shortest[\" \"]->\"\", Shortest[\" \"]->\"\", Shortest[\" \"]->\"\", \"\"->\"\", \"\"->\"-\", \"\\\"\"->\"\", \"&nbsp\"->\"\", Whitespace->\" \" } ]; extractSentences=RightComposition[ normalizeString, StringSplit[#,\".\"|\",\"|\"? \"|\";\"|\":\"|\"(\"|\")\"|\"+\"]& ];Well also need to extract, count up and sort the words from all of the post bodies:&#10005 wordToCount=postBodies//RightComposition[ extractSentences, Flatten/*StringSplit/*ToLowerCase, Flatten, Counts/*ReverseSort ];This gives a list of almost 400k words:&#10005 wordToCount//LengthWe can trim it down to just the top 500 words, being careful to remove some extra noise with websites, equations, inequalities and single letters:&#10005 topWordToCount=KeySelect[ wordToCount[[;;1000]], Not[StringMatchQ[#, \"*'*\"|\"*=*\"|\"*1& ]//Take[#,UpTo[500]]&;Note that   is the most common word, since all equations were replaced with it:&#10005 Dataset[topWordToCount[[;;20]]]&#10005 topWordToCountNoStopwords=KeySelect[ wordToCount[[;;1000]], Not[StringMatchQ[#, \"*'*\"|\"*=*\"|\"*1& ]; topWordToCountNoStopwords=KeyTake[topWordToCountNoStopwords,topWordToCountNoStopwords//Keys//DeleteStopwords]//Take[#,UpTo[500]]&;&#10005 Dataset[topWordToCountNoStopwords[[;;20]]]Now the results are more interesting and meaningful:Of course, we can take this analysis further. We can get the frequencies for the top words in usual English with WordFrequencyData :&#10005 wordToEnglishFrequency=WordFrequencyData[Keys[topWordToCountNoStopwords]];//AbsoluteTiming&#10005 wordToFrequencyCoordinates={wordToEnglishFrequency,wordToMOFrequency}//Merge[Identity]//Select[FreeQ[_Missing]];&#10005 wordToFrequencyCoordinates[[;;3]]We can visualize these coordinates, adding a red region to the plot for words more commonly used in typical English than in MathOverflow posts (below = ), and a gray region for words that are more commonly used in MathOverflow posts by less than a factor of 10 (below = 10 ).This arbitrary factor allows us to narrow down the words that are much more common to MathOverflow than typical English, which appear in the white region (above = 10 ):&#10005 Show[ LogLogPlot[{x,10x},{x,5*10^-11,0.02}, PlotStyle->{Red,Gray}, Filling->{2->{1},1->Bottom}, FillingStyle->{1->Red,2->Gray}, PlotStyle->PointSize[0.002], PlotRange->{{8*10^-11,0.02},{3*10^-4,0.02}}, ImageSize->Large, PlotTheme->\"Detailed\", FrameLabel->{\"Fraction of English\",\"Fraction of MathOverflow. net\"} ], ListLogLogPlot[ wordToFrequencyCoordinates, PlotStyle->PointSize[0.002], PlotRange->{{8*10^-11,0.02},{3*10^-4,0.02}}, ImageSize->Large, PlotTheme->\"Detailed\" ] ]We can take this another step further by looking at the words in the white region that are much more likely to occur on MathOverflow than they are in typical English:&#10005 wordsMuchMoreCommonInMO=wordToFrequencyCoordinates//Select[Apply[Divide]/*LessThan[1/10]]; wordsMuchMoreCommonInMO//Length&#10005 ListLogLogPlot[wordsMuchMoreCommonInMO,ImageSize->500,PlotTheme->\"Detailed\",PlotStyle->PointSize[0.002],FrameLabel->{\"Fraction of English\",\"Fraction of MathOverflow. net\"}]Of course, an easy way to visualize this data is in a word cloud, where the words are weighted by combining their frequency of use via Norm :&#10005 WordCloud[Norm/@wordsMuchMoreCommonInMO,WordSpacings->2]Analysis of n -GramsOf course, individual words are not the only way to analyze the MathOverflow corpus. We can create a function to compute -grams using Partition and recycling extractSentences from earlier:&#10005 ClearAll[getNGrams]; getNGrams[n_Integer? Positive]:=RightComposition[ extractSentences, Map[ ToLowerCase/*StringSplit/*Map[Counts[Partition[#,n,1]]&]/*Merge[Total] ], Merge[Total], ReverseSort, (* Keep only the top 10,000 to save memory *) Take[#,UpTo[10000]]& ];Next, well need to build a function to show the -grams in tables and word clouds, both with and without math (since putting them together would clutter the results a bit):&#10005 ClearAll[showNGrams]; showNGrams[nGramToCount_Association]:=Module[ {phraseToCount,phraseToCountWithMath,phraseToCountWithoutMath}, phraseToCount=KeyMap[StringRiffle,nGramToCount]; phraseToCountWithMath=Take[phraseToCount,UpTo[200]]//Normal//Select[#,Not@StringFreeQ[#[[1]],\"\\[ScriptCapitalM]\\[ScriptCapitalA]\\[ScriptCapitalT]\\[ScriptCapitalH]\"]&,50]&//Association; phraseToCountWithoutMath=Take[phraseToCount,UpTo[200]]//Normal//Select[#,StringFreeQ[#[[1]],\"\\[ScriptCapitalM]\\[ScriptCapitalA]\\[ScriptCapitalT]\\[ScriptCapitalH]\"]&,50]&//Association; Print@WordCloud[Take[phraseToCountWithMath,UpTo[50]],ImageSize->500,WordSpacings->2]; Print[ Row[ { Column[{Style[\" Including \\[ScriptCapitalM]\\[ScriptCapitalA]\\[ScriptCapitalT]\\[ScriptCapitalH] \",24,FontFamily->\"Source Code Pro\"],Dataset[Take[phraseToCountWithMath,UpTo[20]]]}], Column[{Style[\" Without \\[ScriptCapitalM]\\[ScriptCapitalA]\\[ScriptCapitalT]\\[ScriptCapitalH] \",24],Dataset[Take[phraseToCountWithoutMath,UpTo[20]]]}] }, Spacer[20] ] ]; Print@WordCloud[Take[phraseToCountWithoutMath,UpTo[50]],ImageSize->500,WordSpacings->2]; ];3-gramsLooking at the 3-grams, there are lots of The number of, The set of, is there a and more. There are definitely signs of if and only if, but theyre not well captured here since were looking at 3-grams. They should show up later in the 4-grams, anyway. There is a lot of of Let be,  is a, and similarits clear that MathOverflow users frequently use for mathematical notation:&#10005 postBodies//getNGrams[3]//showNGrams4-gramsExpanding on the 3-grams, the 4-grams give several mathy phrases like if and only if, on the other hand, is it true that and the set of all. We also see more proof-like phrases like Let be a,  such that  and similar. Its interesting how the two word clouds begin to show the split of proof-like phrases and natural language phrases:&#10005 postBodies//getNGrams[4]//showNGrams&#10005 postBodies//getNGrams[6]//showNGramsPost Thread NetworksMoving past natural language processing, another way to analyze the MathOverflow site is as a network. We can create a network of MathOverflow users that communicate with each other. One way to do this is to connect two users if one user posts an answer to another users question. In this way, we can create a directed graph of MathOverflow users. Although its possible to do this graph-like traversal and matching with the usual EntityValue syntax, it could get somewhat messy. Questioner-Answerer NetworkTo start, we can write a symbolic representation of a SPARQL query to find all connections between question writers and the writers of answers, and then do some processing to turn it into a Graph :&#10005 Needs[\"GraphStore`\"]&#10005 questionerToAnswererGraph=Entity[\"StackExchange. Mathoverflow:Post\"]//RightComposition[ SPARQLSelect[ { RDFTriple[SPARQLVariable[\"post\"],post type,Entity[\"StackExchange:PostType\", \"1\"]], SPARQLPropertyPath[SPARQLVariable[\"post\"],{owner},SPARQLVariable[\"questioner\"]], SPARQLPropertyPath[SPARQLVariable[\"post\"],{SPARQLInverseProperty[parent post],owner},SPARQLVariable[\"answerer\"]] }->{SPARQLVariable[\"questioner\"],SPARQLVariable[\"answerer\"]} ], Lookup[#,{SPARQLVariable[\"answerer\"],SPARQLVariable[\"questioner\"]}]&, Map[Apply[DirectedEdge]], Graph ]From the icon of the output, we can see its a very large directed multigraph. Networks of this size have very little hope of being visualized easily, so we should find a way to reduce the size of it. Smaller Questioner-Answerer NetworkWe can trim down the size by writing a similar SPARQL query that limits us to posts with a few numerical mathematics post tags:&#10005 questionerToAnswererGraphSmaller=Entity[\"StackExchange. Mathoverflow:Post\"]//RightComposition[ SPARQLSelect[ { RDFTriple[SPARQLVariable[\"post\"],post type,Entity[\"StackExchange:PostType\", \"1\"]], Alternatives[ RDFTriple[SPARQLVariable[\"post\"],tags,Entity[\"StackExchange. Mathoverflow:Tag\", \"NumericalLinearAlgebra\"]], RDFTriple[SPARQLVariable[\"post\"],tags,Entity[\"StackExchange. Mathoverflow:Tag\", \"NumericalAnalysisOfPde\"]], RDFTriple[SPARQLVariable[\"post\"],tags,Entity[\"StackExchange. Mathoverflow:Tag\", \"NumericalIntegration\"]], RDFTriple[SPARQLVariable[\"post\"],tags,Entity[\"StackExchange. Mathoverflow:Tag\", \"RecreationalMathematics\"]] ], SPARQLPropertyPath[SPARQLVariable[\"post\"],{owner},SPARQLVariable[\"questioner\"]], SPARQLPropertyPath[SPARQLVariable[\"post\"],{SPARQLInverseProperty[parent post],owner},SPARQLVariable[\"answerer\"]] }->{SPARQLVariable[\"questioner\"],SPARQLVariable[\"answerer\"]} ], Lookup[#,{SPARQLVariable[\"answerer\"],SPARQLVariable[\"questioner\"]}]&, Map[Apply[DirectedEdge]], Graph[#,GraphStyle->\"LargeGraph\"]& ]This graph is much smaller and can be more reasonably visualized. For simplicity, lets focus only on the largest (weakly) connected component:&#10005 questionerToAnswererGraphSmallerConnected=First@WeaklyConnectedGraphComponents[questionerToAnswererGraphSmaller]Questioner-Answerer Communities by Geographic RegionWe can group the vertices of the graph (MathOverflow users) by geography by using the location information users have entered into their profiles. Here, we can use Interpreter [\"Location\"] to handle a variety of input forms, including countries, cities, administrative divisions (such as states) and universities:&#10005 userToLocation=EntityValue[VertexList[questionerToAnswererGraphSmallerConnected],\"Location\",\"NonMissingEntityAssociation\"]; userToLocation=AssociationThread[Keys[userToLocation],Interpreter[\"Location\"]@RemoveDiacritics@Values[userToLocation]];The results are pretty good, giving over 250 approximate locations:&#10005 userToLocation//Values//CountsBy[Head]Of course, these individual locations are not that helpful, as they are very localized. We can use GeoNearest to find the nearest geographic region as a basis for determining groups for the users:&#10005 ClearAll[getRegion]; getRegion[location_GeoPosition]:=First@getRegion[{location}]; getRegion[locations:{__GeoPosition}]:=First[#,Missing[\"NotAvailable\"]]&/@DeleteCases[GeoNearest[GeoVariant[\"GeographicRegion\",\"Center\"],locations],Entity[\"GeographicRegion\", \"World\"],Infinity];Next, we group users into communities based on this geographic information:&#10005 userToGeographicRegion=DeleteMissing@AssociationThread[Keys[#],getRegion@Values[#]]&[Select[userToLocation,MatchQ[_GeoPosition]]]; geographicRegionToUsers=GroupBy[userToGeographicRegion,Identity,Keys];&#10005 Length/@geographicRegionToUsersLastly, we can use CommunityGraphPlot to build a graphic that shows the geographic communities of the questioner-answerer network:&#10005 regionToPointColor=Lookup[ Darker@,Entity[\"GeographicRegion\", \"NorthAmerica\"]->Darker[Green,0.5],Entity[\"GeographicRegion\", \"Australia\"]->Orange,Entity[\"GeographicRegion\", \"Asia\"]->Purple,Entity[\"GeographicRegion\", \"SouthAmerica\"]->Darker[Red,0.25] |>, #,Brown]&; regionToLabelPlacement=Lookup[ Below,Entity[\"GeographicRegion\", \"NorthAmerica\"]->After,Entity[\"GeographicRegion\", \"Asia\"]->Below |>, #,Above]&; regionToRotation=Lookup[-(π/2)|>,#,0]&; allRegionsToUsers=Append[geographicRegionToUsers,\"??? \"->Complement[VertexList[questionerToAnswererGraphSmallerConnected],Flatten@Values[geographicRegionToUsers]]]; styledCommunities=KeyValueMap[ Function[ {region,users}, With[ {regionPointColor=regionToPointColor[region]}, Labeled[ Style[users,regionPointColor], Rotate[ Style[ Framed[region,Background->regionPointColor,RoundingRadius->5,FrameMargins->Small], FontColor->ResourceFunction[\"FontColorFromBackgroundColor\"][regionPointColor], FontSize->12 ], regionToRotation[region] ], regionToLabelPlacement[region] ] ] ], allRegionsToUsers ]; CommunityGraphPlot[questionerToAnswererGraphSmallerConnected,styledCommunities, ImageSize->500, Method->\"Hierarchical\", EdgeStyle->Directive[Opacity[0.25,Black],Arrowheads[Tiny]], VertexStyle->PointSize[Medium], CommunityRegionStyle->(Opacity[0.1,regionToPointColor[#]]&/@Keys[allRegionsToUsers]), CommunityBoundaryStyle->(Opacity[1,Black]&/@Keys[allRegionsToUsers]) ]Post Owner-Commenter NetworkOf course, we could do a similar analysis on connections between post owners and their commenters for posts tagged with linear-programming:&#10005 postOwnerToCommenterGraph={ Entity[\"StackExchange. Mathoverflow:Post\"],Entity[\"StackExchange. Mathoverflow:Comment\"] }//RightComposition[ SPARQLSelect[ { SPARQLPropertyPath[SPARQLVariable[\"post\"],{owner},SPARQLVariable[\"owner\"]], RDFTriple[SPARQLVariable[\"post\"],tags,Entity[\"StackExchange. Mathoverflow:Tag\", \"LinearProgramming\"]], SPARQLPropertyPath[SPARQLVariable[\"post\"],{comments,user},SPARQLVariable[\"commenter\"]] }->{SPARQLVariable[\"owner\"],SPARQLVariable[\"commenter\"]} ], Lookup[#,{SPARQLVariable[\"commenter\"],SPARQLVariable[\"owner\"]}]&, Map[Apply[DirectedEdge]], Graph[#,GraphStyle->\"LargeGraph\"]& ]However, further analysis on this network will be left as an exercise for the reader. Analyzing T E X Snippets, in its various forms, has been around for over 40 years, and is widely used in math and science for typesetting. On MathOverflow, there are not many posts without it, so exploring snippets can give interesting insights into the content available on the site. Extract T E X SnippetsFirst, we need to extract the snippets from post bodies. Consider a simple example post:&#10005 Entity[\"StackExchange. Mathoverflow:Post\", \"40686\"][\"Body\"]We can write a function to extract the snippets in a string, noting the two main input forms (\" $ $  $ $ \" or \"\\\\begin{}\\\\end[...]\"):&#10005 ClearAll[extractTeXSnippets]; extractTeXSnippets[s_String] := Module[ {dd,d,o}, dd=StringCases[s,Shortest[\"$$\"~~__~~\"$$\"]]; d=StringCases[StringReplace[s,Shortest[\"$$\"~~__~~\"$$\"]:>\"\"],Shortest[\"$\"~~__~~\"$\"]]; o=StringCases[ s, Alternatives[ Shortest[\"\\\\begin{equation*}\"~~__~~\"\\\\end{equation*}\"], Shortest[\"\\\\begin{align}\"~~__~~\"\\\\end{align}\"] ] ]; StringReplace[Join[dd, d,o], {\".$\":>\"$\"}] ]Testing this on the simple example gives the snippets wrapped in dollar signs:&#10005 extractTeXSnippets[body]Format T E X SnippetsOf course, once we have snippets, it would be valuable to format them into actual typesetting thats easier on the eyes than the raw code. We can write a quick function to do this with proper formatting:&#10005 blackboardBoldRules=character to double struck; frakturGothicRules=character to gothic; formatTeXSnippet[s_String] := Which[ StringMatchQ[s, \"$\\\\mathbb{\"~~ _ ~~ \"}$\"], StringReplace[s,\"$\\\\mathbb{\"~~ a_ ~~ \"}$\":> a]/.blackboardBoldRules, StringMatchQ[s, \"$\\\\mathbb \"~~ _ ~~ \"$\"], StringReplace[s,\"$\\\\mathbb \"~~ a_ ~~ \"$\":> a]/.blackboardBoldRules, StringMatchQ[s, \"$\\\\mathfrak{\"~~ _ ~~ \"}$\"], StringReplace[s,\"$\\\\mathfrak{\"~~ a_ ~~ \"}$\":> a]/.frakturGothicRules, StringMatchQ[s, \"$\\\\mathfrak \"~~ _ ~~ \"$\"], StringReplace[s,\"$\\\\mathfrak \"~~ a_ ~~ \"$\":> a]/.frakturGothicRules, StringMatchQ[s, \"$\\\\mathcal{\"~~ _ ~~ \"}$\"], Style[StringReplace[s,\"$\\\\mathcal{\"~~ a_ ~~ \"}$\":>a],FontFamily->\"Snell Roundhand\"], StringMatchQ[s, \"$\\\\mathcal \"~~ _ ~~ \"$\"], Style[StringReplace[s,\"$\\\\mathcal \"~~ a_ ~~ \"$\":>a],FontFamily->\"Snell Roundhand\"], True, StringReplace[s,\"\"->\"_\"]//RightComposition[ ImportString[#,\"TeX\"]&, FirstCase[#,c:Cell[_,\"InlineFormula\"|\"NumberedEquation\"]:>DisplayForm[c],Missing[\"NotAvailable\"],]& ] ]We can test the results on the previously extracted snippets:&#10005 AssociationMap[formatTeXSnippet,extractTeXSnippets[body]]//KeyValueMap[List]//Grid[#,Frame->All]&We can also test them on a completely different post:&#10005 Entity[\"StackExchange. Mathoverflow:Post\", \"40686\"][\"Body\"] // extractTeXSnippets // AssociationMap[formatTeXSnippet] // KeyValueMap[List] // Grid[#, Frame -> All, Alignment -> Left] &Set Up T E X Snippets PropertyThis system works well, so we should make it easier to use. We can do this by hooking up these functions as a property for posts, keeping the formatting function separate so that analysis can still be done on the raw strings:&#10005 EntityProperty[\"StackExchange. Mathoverflow:Post\",\"TeXSnippets\"][\"Label\"]=\"TEX snippets\"; EntityProperty[\"StackExchange. Mathoverflow:Post\",\"TeXSnippets\"][\"DefaultFunction\"]=Function[entity, extractTeXSnippets[entity[\"Body\"]] ];Now we can just call the property on an entity instead:&#10005 Entity[\"StackExchange. Mathoverflow:Post\", \"67739\"][\"TeXSnippets\"]//Map[formatTeXSnippet]Create a T E X Word CloudA simple way to analyze the snippets is to count up the number of times each fragment is used:&#10005 teXToCount=allTeXSnippets//RightComposition[Flatten,Counts,ReverseSort];There are almost one million unique snippets used in the post bodies on MathOverflow:&#10005 teXToCount//LengthWe can also make a simple word cloud from the top 100 snippets:&#10005 teXToCount[[;;100]]//KeyMap[formatTeXSnippet]//WordCloud[#,MaxItems->All,ImageSize->400]&Its easy to see that there are a lot of single-letter snippets. But there are a lot more interesting things hiding beyond these top 100. Lets take a look at a few different cases! Integrals are fairly easy to find with some simple string pattern matching:&#10005 integralToCount=KeySelect[teXToCount,StringMatchQ[(\"$\"|\"$$\")~~(Whitespace|\"\")~~\"\\\\int\"~~__~~\"$\"]]; integralToCount//LengthLooking at the top 50 gives some interesting resultssome very simple, and some rather complex:&#10005 WordCloud[integralToCount[[;;50]]//KeyMap[formatTeXSnippet],ImageSize->400,WordSpacings->2]Analyze EquationsAnother interesting subset of snippets to consider is equations. Again, we can find these with some string pattern matching that requires an equals sign:&#10005 equations=KeySelect[teXToCount,StringMatchQ[(\"$\"|\"$$\")~~(Whitespace|\"\")~~__~~\" = \"~~__~~\"$\"]]; equations//LengthVisualizing the top 50 gives mostly single-letter variable assignments to numbers:Equations of the Form <letter> = <number>If we look at the single-letter variable assignments, we can find the minimum and maximum values of <number> for each <letter> .Note that this includes a list of special characters, such as \\alpha:&#10005 specialLettersPattern=Alternatives@@Keys[teXRepresentationToCharacter];&#10005 variableEqualsNumberDistributions=equations//Keys//StringCases[(\"$$\"|\"$\")~~lhs:(specialLettersPattern|LetterCharacter)~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~rhs:NumberString~~(Whitespace|\"\")~~(\"$$\"|\"$\"):>Rule[ToUpperCase[lhs/.teXRepresentationToCharacter],ToExpression[rhs]]]//Merge[Counts];&#10005 KeySort[MinMax/@Keys/@variableEqualsNumberDistributions]//KeyValueMap[Prepend[N@#2,#1<>\" | \"<>ToLowerCase[#1]]&]//Grid[#,Frame->All]&Its interesting to see that most letters are positive, but S is strangely very negative. Its also interesting to note the very large scale of U , V and W . Perhaps not surprisingly, N is the most common letter, though its neighbor O is the least common:&#10005 ReverseSort[Total/@variableEqualsNumberDistributions]//BarChart[#//KeySort//Reverse,ChartLabels->Automatic,PlotTheme->\"Detailed\",ImageSize->400,BarOrigin->Left,AspectRatio->1.5]&Trimming these single-variable assignments out of the original equation word cloud makes the results a bit more diverse:&#10005 WordCloud[ KeySelect[equations,StringMatchQ[(\"$$\"|\"$\")~~(specialLettersPattern|LetterCharacter)~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~NumberString~~(Whitespace|\"\")~~(\"$$\"|\"$\")]/*Not][[;;50]]//KeyMap[formatTeXSnippet], ImageSize->400, WordSpacings->2 ]Equations of the Form <letter> = <letter>Its interesting to see that there are a lot of letters assigned to (or compared with) another letter. We can make a simple graph that connects two letters in these equations, again taking into account special characters like \\alpha:&#10005 letterGraph=Keys[equations]//StringCases[(\"$$\"|\"$\")~~lhs:(specialLettersPattern|LetterCharacter)~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~rhs:LetterCharacter~~(Whitespace|\"\")~~(\"$$\"|\"$\"):>(DirectedEdge[lhs,rhs]/.teXRepresentationToCharacter)]//Flatten//Counts//KeySortBy[First];The graph, without combining upper and lowercase letters, is quite messy:&#10005 Graph[ Union@Cases[Keys[letterGraph],_String,Infinity], Keys[letterGraph], GraphLayout->\"CircularEmbedding\", EdgeWeight->Normal[letterGraph], EdgeStyle->Normal[Opacity[N@(#/Max[letterGraph])]&/@letterGraph], VertexLabels->Placed[Automatic,Center], VertexLabelStyle->Directive[Bold,Small], VertexSize->Large, PlotTheme->\"Web\", ImageSize->400 ]If we combine the upper and lowercase letters, the graph becomes a little bit cleaner:&#10005 upperCaseLetterGraph=Merge[Normal[letterGraph]/.c_String:>ToUpperCase[c],Total]; Graph[ Union@Cases[Keys[upperCaseLetterGraph],_String,Infinity], Keys[upperCaseLetterGraph], GraphLayout->\"CircularEmbedding\", EdgeWeight->Normal[upperCaseLetterGraph], EdgeStyle->Normal[Opacity[N@(#/Max[letterGraph])]&/@upperCaseLetterGraph], VertexLabels->Placed[Automatic,Center], VertexLabelStyle->Bold, VertexSize->Large, PlotTheme->\"Web\", ImageSize->400 ]If we again remove these equation types, the word cloud becomes even cleaner:&#10005 WordCloud[ KeySelect[equations,StringMatchQ[ Alternatives[ (\"$$\"|\"$\")~~(specialLettersPattern|LetterCharacter)~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~NumberString~~(Whitespace|\"\")~~(\"$$\"|\"$\"), (\"$$\"|\"$\")~~(specialLettersPattern|LetterCharacter)~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~LetterCharacter~~(Whitespace|\"\")~~(\"$$\"|\"$\") ] ]/*Not][[;;50]]//KeyMap[formatTeXSnippet], ImageSize->400, WordSpacings->2 ]Functional EquationsAnother interesting subset of equations to look into is functional equations. With a little bit of string pattern matching, we can find many examples:&#10005 functionalEquations=KeySelect[equations,StringMatchQ[___~~(f:_)~~\"(\"~~__~~\")\"~~__~~(f:_)~~\"(\"~~__~~\")\"~~___]]; functionalEquations//Length&#10005 functionalEquations2=KeySelect[equations,StringMatchQ[(\"$$\"|\"$\")~~(f:LetterCharacter)~~\"(\"~~__~~\")\"~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~__~~(f:_)~~\"(\"~~__~~\")\"~~___~~(\"$$\"|\"$\")]]; functionalEquations2//Length&#10005 functionalEquations2[[;;16]]//Keys//Map[formatTeXSnippet]//Multicolumn[#,Frame->All]&However, well need to go further to find equations that are easier to work with. Lets limit ourselves to single-letter, single-argument functions:&#10005 functionalEquations3=KeySelect[equations,StringMatchQ[(\"$$\"|\"$\")~~(f:LetterCharacter)~~\"(\"~~x:LetterCharacter~~\")\"~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~__~~(f:_)~~\"(\"~~x:_~~\")\"~~___~~(\"$$\"|\"$\")]]; functionalEquations3//Length&#10005 functionalEquations3[[;;16]]//Keys//Map[formatTeXSnippet]//Multicolumn[#,Frame->All]&This is much more pointed, but we can go further. If we limit ourselves to functional equations with only one equals sign with single, lowercased arguments that only consist of a single head and argument (modulo operators and parentheses), we find just six equations:&#10005 functionalEquations4=KeySelect[ equations, StringMatchQ[s:((\"$$\"|\"$\")~~(f:LetterCharacter)~~\"(\"~~x:LetterCharacter? LowerCaseQ~~\")\"~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~__~~(f:_)~~\"(\"~~x:_~~\")\"~~___~~(\"$$\"|\"$\"))/;(StringCount[s,\"=\"]===1&&StringFreeQ[s,\"\\\\\"~~LetterCharacter..]&&Complement[Union[Characters[s]],{\"$\",\"^\",\"(\",\")\",\"-\",\"+\",\"=\",\"{\",\"}\",\". \",\" \",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"}]===Sort[{f,x}])] ]; functionalEquations4//Length&#10005 functionalEquations4//Keys//Map[formatTeXSnippet]//Column[#,Frame->All]&Interestingly, there are only two functionally unique equations in this list:f(x) = 1 + x f(x)^2 f(x) = 1 + x^2 f(x)^2If we clean up these functional equations, we can put them through Interpreter[\"TeXExpression\"] to get actual Wolfram Language representations of them:&#10005 interpretedFunctionalEquations=Interpreter[\"TeXExpression\"][StringReplace[f:LetterCharacter~~\"(\"~~x:LetterCharacter~~\")^\"~~(\"{\"|\"\")~~n:DigitCharacter~~(\"}\"|\"\"):>\"(\"<>f<>\"(\"<>x<>\")\"<>\")^\"<>n]/@Keys[functionalEquations4]]/.C[x_]:>F[x]Finally, we can solve these equations with RSolve :&#10005 AssociationMap[ Replace[eqn:((f_)[x_]==rhs_):>RSolve[eqn,f[x],x]], interpretedFunctionalEquations ]Analyze Big O Notation ArgumentsMoving past equations, another common notation among mathematicians is big O notation. Frequently used in computational complexity and numerical error scaling, this notation should surely appear somewhat frequently on MathOverflow. Lets take a look by finding snippets wrapped in O that consist of a single argument and have equal numbers of open-and-close parentheses:&#10005 bigONotationArguments=StringCases[Keys[teXToCount],Shortest[\"O(\"~~args__~~\")\"]/;StringFreeQ[args,\",\"|\";\"]&&StringCount[args,\"(\"]===StringCount[args,\")\"]:>\"$$\"<>args<>\"$$\"]//Flatten//Counts//ReverseSort; bigONotationArguments//LengthThe results are varied:&#10005 bigONotationArguments[[;;100]]//KeyMap[formatTeXSnippet]//WordCloud[#,WordSpacings->3]&One can note that many of these results are functionally equivalentthey differ only in the letter chosen for the variable. We can clean these cases up with a little bit of effort:&#10005 normalizeBigOStrings=StringReplace[ { (* Any constant number  1 *) \"$$\"~~NumberString~~\"$$\"->\"$$1$$\", \"$$\"~~LetterCharacter~~\"$$\"->\"$$n$$\", \"$$\"~~LetterCharacter~~\"^\"~~exp:(DigitCharacter|(\"{\"~~__~~\"}\"))~~\"$$\":>\"$$n^\"<>exp<>\"$$\", \"$$\"~~LetterCharacter~~\"\\\\log \"~~LetterCharacter~~\"$$\":>\"$$n\\\\log n$$\", \"$$\"~~b:DigitCharacter~~\"^\"~~x_~~\"/\"~~x_~~\"$$\":>\"$$\"<>b<>\"^n/n$$\", \"$$\"~~numerator:DigitCharacter~~\"/\"~~LetterCharacter~~\"$$\":>\"$$\"<>numerator<>\"/n$$\", \"$$\"~~factor:DigitCharacter~~LetterCharacter~~\"$$\":>\"$$\"<>factor<>\"n$$\", \"$$1/\"~~LetterCharacter~~\"^\"~~exp:(DigitCharacter|(\"{\"~~__~~\"}\"))~~\"$$\":>\"$$1/n^\"<>exp<>\"$$\", \"$$1/|\"~~LetterCharacter~~\"|^\"~~exp:(DigitCharacter|(\"{\"~~__~~\"}\"))~~\"$$\":>\"$$1/|n|^\"<>exp<>\"$$\", \"$$\"~~LetterCharacter~~\"^\"~~exp:(DigitCharacter|(\"{\"~~__~~\"}\"))~~\"\\\\log \"~~LetterCharacter~~\"$$\":>\"$$n^\"<>exp<>\"\\\\log n$$\", \"$$|\"~~LetterCharacter~~\"|^\"~~exp:(DigitCharacter|(\"{\"~~__~~\"}\"))~~\"$$\":>\"$$|n|^\"<>exp<>\"$$\", \"$$\\\\\"~~op:(\"log\"|\"dot\")~~Whitespace ~~LetterCharacter~~\"$$\":>\"$$\\\\\"<>op<>\" n$$\", \"$$\\\\log|\"~~LetterCharacter~~\"|$$\"->\"$$\\\\log|n|$$\", \"$$\\\\sqrt{\"~~LetterCharacter~~\"}$$\":>\"$$\\\\sqrt{n}$$\", \"$$\"~~LetterCharacter~~\"/\\\\log \"~~LetterCharacter~~\"$$\":>\"$$n/\\\\log n$$\" }/.LetterCharacter->(specialLettersPattern|LetterCharacter) ];&#10005 normalizedBigOArguments=Normal[bigONotationArguments]//RightComposition[ GroupBy[#,First/*normalizeBigOStrings->Last,Total]&, ReverseSort ];Now the data is much cleaner:&#10005 normalizedBigOArguments[[;;15]]//KeyMap[formatTeXSnippet]//Dataset&#10005 normalizedBigOArguments[[;;100]]//Keys//Map[formatTeXSnippet]//MulticolumnAnd the word cloud looks much nicer:&#10005 wc=normalizedBigOArguments[[;;50]]//KeyMap[formatTeXSnippet]//WordCloud[#,WordSpacings->3,ImageSize->400,ScalingFunctions->Sqrt]&Lastly, since these are arguments to O, lets set the word cloud as an argument of O to make a nice picture:&#10005 Style[HoldForm[O][wc],90]//TraditionalFormMentioned Propositions and MathematiciansAnother way to analyze MathOverflow is to look at the mathematical propositions and famous mathematicians that are mentioned in the post bodies. An easy way to do this is to use more entity stores to keep track of the different types. Mathematical Propositions: Build EntityStoreTo begin, lets set up an EntityStore for mathematical propositions and their types. Specifically, we can set up \"MathematicalPropositionType\" for base words like theorem, hypothesis and conjecture, and \"MathematicalProposition\" for specific propositions like the mean value theorem and Zorns lemma. The proposition types will serve as a means of programmatically finding the specific propositions, so well need to pre-populate \"MathematicalPropositionType\" with entities, but we can leave it empty of entities for nowwell populate that type in the store by processing the post bodies, but well do that next. Note that Ive added some properties to keep track of the propositions found in each post. Specifically, \"Wordings\" will hold an Association with strings for the keys and the counts of each of those strings for the values. Additionally, well set up \"MentionedPostCount\" to keep track of the number of times a post is mentioned:&#10005 propositionStore=EntityStore[ { \"MathematicalPropositionType\"->\"theorem\"|>, \"Hypothesis\"->\"hypothesis\"|>, \"Principle\"->\"principle\"|>, \"Conjecture\"->\"conjecture\"|>, \"Thesis\"->\"thesis\"|>, \"Lemma\"->\"lemma\"|>, \"Corollary\"->\"corollary\"|>, \"Axiom\"->\"axiom\"|> |>, \"Properties\"->\"label\"|> |> |>, \"MathematicalProposition\"->, \"Properties\"->\"proposition type\" |>, \"Wordings\"->\"wordings\", \"DefaultFunction\"->Function[], \"FormattingFunction\"->ReverseSort |>, \"Label\"->\"label\", \"DefaultFunction\"->Function[entity,entity[\"Wordings\"]//Keys//First] |>, \"MentionedPostCount\"->\"mentioned post count\", \"DefaultFunction\"->Function[0] |> |> |> } ]; EntityUnregister/@propositionStore[]; EntityRegister[propositionStore]Now that the EntityStore is set up and registered, we can use the properties I set up in the store. Lets start with a list of theorems that dont have names in them:&#10005 $specialTheorems={\"prime number theorem\",\"central limit theorem\",\"implicit function theorem\",\"spectral theorem\",\"incompleteness theorem\",\"universal coefficient theorem\",\"intermediate value theorem\",\"mean value theorem\",\"uniformization theorem\",\"inverse function theorem\",\"four color theorem\",\"binomial theorem\",\"index theorem\",\"fundamental theorem of algebra\",\"residue theorem\",\"dominated convergence theorem\",\"open mapping theorem\",\"ergodic theorem\",\"fundamental theorem of calculus\",\"h-cobordism theorem\",\"closed graph theorem\",\"modularity theorem\",\"adjoint functor theorem\",\"geometrization theorem\",\"primitive element theorem\",\"fundamental theorem of arithmetic\",\"fixed point theorem\",\"4-color theorem\",\"four colour theorem\",\"isotopy extension theorem\",\"proper base change theorem\",\"well-ordering theorem\",\"loop theorem\",\"slice theorem\",\"odd order theorem\",\"isogeny theorem\",\"group completion theorem\",\"convolution theorem\",\"reconstruction theorem\",\"equidistribution theorem\",\"contraction mapping theorem\",\"principal ideal theorem\",\"ergodic decomposition theorem\",\"orbit-stabilizer theorem\",\"4-colour theorem\",\"tubular neighborhood theorem\",\"three-squares theorem\",\"martingale representation theorem\",\"purity theorem\",\"triangulation theorem\",\"multinomial theorem\",\"graph minor theorem\",\"strong approximation theorem\",\"universal coefficients theorem\",\"localization theorem\",\"positive mass theorem\",\"identity theorem\",\"cellular approximation theorem\",\"transfer theorem\",\"bounded convergence theorem\",\"fundamental theorem of symmetric functions\",\"subadditive ergodic theorem\",\"annulus theorem\",\"rank-nullity theorem\",\"elliptization theorem\"};Next, we can build a function that will introduce new \"MathematicalProposition\" entities, keeping track of how often they are mentioned, their types and specific wordings for later use in cleaning things up. Note that we strip off any possessives and remove special characters via RemoveDiacritics :&#10005 ToCamelCase[s_String]:=First@ToCamelCase[{s}]; ToCamelCase[s:{__String}]:=StringSplit[RemoveDiacritics[s]]//Map[Capitalize/*StringJoin]; toStandardName=ToCamelCase[StringReplace[StringRiffle@StringTrim[StringSplit[#],\"'s\"],{\"'\"->\"\",Except[LetterCharacter|DigitCharacter]->\" \"}]]&; With[ {propositionTypePattern=Alternatives@@EntityValue[EntityList[\"MathematicalPropositionType\"],label]}, ClearAll[addPropositionEntity]; addPropositionEntity[proposition_String]:=With[ { entity=Entity[\"MathematicalProposition\",toStandardName[proposition]] }, (* Keep track of mentions *) entity[\"MentionedPostCount\"]=Replace[entity[\"MentionedPostCount\"],{i_Integer:>i+1,_->0}]; (* Keep track of specific wordings and their counts *) entity[\"Wordings\"]=Replace[Replace[entity[\"Wordings\"],Except[_Association]->],a_Association:>Append[a,proposition->Lookup[a,proposition,0]+1]]; (* Extract PropositionType *) entity[\"PropositionType\"]=StringCases[proposition,propositionTypePattern,IgnoreCase->True]//Replace[{{x_String,___}:>Entity[\"MathematicalPropositionType\",Capitalize[ToLowerCase[x]]],_->Missing[\"NotAvailable\"]}]; entity ] ];Note that there are currently no proposition entities:&#10005 EntityList[\"MathematicalProposition\"]&#10005 addPropositionEntity/@$specialTheorems; then there are proposition entities defined:&#10005 EntityList[\"MathematicalProposition\"]//Take[#,UpTo[5]]&We should reset counters for the introduced entities to keep things uniform (the list I provided was fabricatedthose strings did not come from actual posts, so not resetting these values may throw off the numbers a bit):&#10005 #[\"MentionedPostCount\"]=0;&/@EntityList[\"MathematicalProposition\"]; #[\"Wordings\"]=Association[#[\"Label\"]->1];&/@EntityList[\"MathematicalProposition\"];Of course, we can go further and detect other forms of propositions. Specifically, lets look for propositions of the following forms: One of the special theorems we just introduced  <person name> theorem (and similar) theorem of <person name>  (and similar)When we find these propositions, we can add them as entities to the proposition EntityStore (via addPropositionEntity ), as well as store them with the posts so lookups are faster (as they will already be stored in memory through the EntityStore ).To start, well need to do some normalization. Heres a useful function that uses a list of words that should always be lowercased:&#10005 $lowercaseWords= list of words that should be lowercased; lowerCaseSpecificWords= StringReplace[(#->ToLowerCase[#])&/@$lowercaseWords];Additionally, heres a list of ordinals and how to normalize them (including Lastfor example, as in Fermats last theorem):&#10005 $ordinalToWord=\"First\",\"2nd\"->\"Second\",\"3rd\"->\"Third\",\"4th\"->\"Fourth\",\"5th\"->\"Fifth\",\"6th\"->\"Sixth\",\"7th\"->\"Seventh\",\"8th\"->\"Eighth\",\"9th\"->\"Ninth\",\"10th\"->\"Tenth\",\"last\"->\"Last\"|>; $ordinals=Join[ Flatten[{ToLowerCase[#],#}&/@Values[$ordinalToWord]], List@@Keys[$ordinalToWord] ]//ReverseSortBy[StringLength];Now we can create a function to extract propositions from strings, normalize them with normalizeString from earlier and then create new \"MathematicalProposition\" entities using addPropositionEntity :&#10005 With[ { propositionTypePattern=EntityList[\"MathematicalPropositionType\"]//label//Join[#,Capitalize/@#]&//Apply[Alternatives], upperCaseWordPattern=(WordBoundary|Whitespace ~~(_?UpperCaseQ ~~ (LetterCharacter| \"-\"|\"'\")..)~~WordBoundary|Whitespace), anyCaseWordPattern=(WordBoundary|Whitespace ~~((Alternatives@@$ordinals)|( (LetterCharacter| \"-\"|\"'\")..))~~WordBoundary|Whitespace), possibleOrdinalPattern=(Alternatives@@$ordinals~~Whitespace)|\"\" }, extractNamedPropositions=RightComposition[ normalizeString, DeleteDuplicates@Join[ (* Case 1: E.g. \"central limit theorem\" *) StringCases[#,Alternatives@@$specialTheorems,IgnoreCase->True], StringCases[ #, Alternatives[ (* Case 3: \"(nth) Theorem of Something (Something (Something))\" *) Shortest[(WordBoundary|Whitespace)~~possibleOrdinalPattern~~propositionTypePattern ~~ Whitespace~~\"of\"~~Longest@Repeated[upperCaseWordPattern,3]], (* Case 2: \"Something (something (something)) Theorem\" *) Shortest[(WordBoundary|Whitespace)~~(x:upperCaseWordPattern)~~Longest@Repeated[anyCaseWordPattern,2]~~propositionTypePattern]/;(Not@StringMatchQ[StringTrim[x],\"The\"|\"A\"|\"Use\",IgnoreCase->True]) ] ] ]&, (* Remove cases with useless words in them *) Select[ With[ (* Ignore \"of\" so that case #3 is allowed *) {split=DeleteCases[StringSplit[ToLowerCase[#],Whitespace|\"-\"],\"of\"]}, split===DeleteCases[DeleteStopwords[split],\"using\"|\"like\"|\"phd\"|\"understanding\"|\"finally\"|\"concerning\"|\"regarding\"|\"ℳℋ\"|\"\"|\"satisfies\"|\"following\"|\"stated\"|\"usually\"|\"implies\"|\"hence\"|\"course\"|\"assuming\"|\"wikipedia\"|\"article\"|\"usual\"|\"actually\"|\"analysis\"|\"entitled\"|\"apply\"] ]& ], StringReplace[Normal@$ordinalToWord], lowerCaseSpecificWords, StringTrim, DeleteDuplicates, Map[addPropositionEntity] ] ];Lets try the function on a simple example:&#10005 Entity[\"StackExchange. Mathoverflow:Post\", \"40686\"][\"Body\"]//extractNamedPropositionsWe can see that an entity was added to the store:&#10005 EntityList[\"MathematicalProposition\"][[-3;;]]&#10005 Entity[\"MathematicalProposition\", \"MartinAxiom\"][\"PropertyAssociation\"]Of course, we can automate this a bit more by introducing this function as a property for MathOverflow posts that will store the results in the EntityStore itself:&#10005 EntityProperty[\"StackExchange. Mathoverflow:Post\",\"NamedPropositions\"][\"Label\"]=\"named propositions\"; EntityProperty[\"StackExchange. Mathoverflow:Post\",\"NamedPropositions\"][\"DefaultFunction\"]=Function[entity,entity[\"NamedPropositions\"]=extractNamedPropositions[entity[\"Body\"]]];Lets test out the property on the same Entity as before:&#10005 Entity[\"StackExchange. Mathoverflow:Post\", \"40686\"][\"NamedPropositions\"]We can see that the in-memory store has been populated:&#10005 Entity[\"MathematicalProposition\"][\"EntityStore\"]Pro tip: in case you want to continue to work on an EntityStore youve been modifying in-memory in a future Wolfram Language session, you can Export Entity[\"type\"][\"EntityStore\"] to an MX file and then Import it in the new session. Just dont forget to register it with EntityRegister !At this point, we can now gather propositions mentioned in all of the MathOverflow posts, taking care to reset the counters again to avoid contamination of the results:&#10005 (* Reset counters again to avoid contaminating the results *) #[\"MentionedPostCount\"]=0;&/@EntityList[\"MathematicalProposition\"]; #[\"Wordings\"]=Association[#[\"Label\"]->1];&/@EntityList[\"MathematicalProposition\"];Note that this will take a while to run (it took about 20 minutes on my machine), but it will allow for a very thorough analysis of the sites content. After processing all of the posts, there are now over 10k entities in the proposition EntityStore :&#10005 postToNamedPropositions=EntityValue[\"StackExchange. Mathoverflow:Post\",\"NamedPropositions\",\"EntityAssociation\"];&#10005 EntityValue[\"MathematicalProposition\",\"Entities\"]//LengthData CleanlinessHaving kept track of the wordings for each proposition was a good choicenow we can see that proposition entities will format with the most commonly used wording. For example, look at Stokes theorem:&#10005 Entity[\"MathematicalProposition\", \"StokesTheorem\"][\"Wordings\"]Its named after George Gabriel Stokes, and so the correct possessive form ends in s, not s, despite about 15 percent of mentions using the incorrect form. Ill admit that this normalization is not perfectwhen someone removes the first s altogether, it is picked up in a different entity:&#10005 Entity[\"MathematicalProposition\", \"StokeTheorem\"][\"Wordings\"]Rather than spend a lot of time and effort to normalize these small issues, Ill move on and work around these problems for now. Proposition AnalysisNow that we have a lot of data on the propositions mentioned in the post bodies, we can visualize the most commonly mentioned propositions in a word cloud:&#10005 topPropositionToCount=ReverseSort@EntityValue[EntityClass[\"MathematicalProposition\",{\"MentionedPostCount\"->TakeLargest[100]}],\"MentionedPostCount\",\"EntityAssociation\"];&#10005 WordCloud[topPropositionToCount[[;;20]],ImageSize->400,WordOrientation->{{π/4,-(π/4)}}]We can also see that about two-thirds of all propositions are theorems:&#10005 propositionTypeBreakdown=EntityValue[\"MathematicalProposition\",\"PropositionType\"]//Counts//ReverseSort&#10005 PieChart[propositionTypeBreakdown,ChartLabels->Placed[Automatic,\"RadialCallout\"],ImageSize->400,PlotRange->All]Now that we have the propositions, we can look for mathematician names in the propositions. To start, we can find all of the labels for the propositions:&#10005 propositionToLabel=EntityValue[\"MathematicalProposition\",\"Label\",\"EntityAssociation\"];Next, well need to find the words in the labels, drop proposition types and stopwords and count them up (taking care to not separate names that start with de or von):&#10005 commonWordsInPropositions=propositionToLabel//RightComposition[ Values, StringReplace[prefix:\"Van der\"|\"de\"|\"De\"|\"von\"|\"Von\"~~(Whitespace|\"-\")~~name:(_?UpperCaseQ~~LetterCharacter):>StringReplace[prefix,Whitespace->\"_\"]<>\"_\"<>name], StringSplit/*Flatten/*DeleteStopwords, Counts/*ReverseSort, KeyDrop[Flatten@EntityValue[EntityClass[\"MathematicalPropositionType\",All],{\"CanonicalName\",\"Label\"}]] ];Next, we can look for groups of mathematician names joined by dashes (taking care to remove words that are obviously not names):&#10005 namePattern=((_?UpperCaseQ|(\"von\"|\"de\"))~~(LetterCharacter|\"_\")..); mathematiciansJoinedByDashes=Select[Keys[commonWordsInPropositions],StringMatchQ[namePattern~~Repeated[\"-\"~~namePattern,{1,Infinity}]]]//StringSplit[#,\"-\"]&//Flatten//Union//StringReplace[\"_\"->\" \"]; mathematiciansJoinedByDashes=DeleteCases[mathematiciansJoinedByDashes,\"Anti\"|\"Foundation\"|\"Max\"|\"Flow\"|\"Min\"|\"Cut\"]; mathematiciansJoinedByDashes//LengthAnother source of names is possessive words, as in Zorns lemma:&#10005 possesiveNames=Keys[commonWordsInPropositions]//Select[StringEndsQ[\"'s\"|\"s'\"]]; possesiveNames//LengthAfter some cleanup (e.g. removing inline snippets and references to the Clay Mathematics Institutes Millennium Prize problems), most of these are likely last names for mathematicians:&#10005 mathematicansByPossessives=StringTrim[possesiveNames,\"'s\"|\"s'\"]//RightComposition[ StringSplit[#,\"-\"]&, Flatten/*DeleteDuplicates, StringReplace[\"_\"->\" \"], Select[StringMatchQ[#,_?UpperCaseQ~~(_?LowerCaseQ)~~___]&], Select[StringFreeQ[\"ℳℋ\"]] ]; mathematicansByPossessives=DeleteCases[mathematicansByPossessives,\"Clay\"|\"Millennium\"]; mathematicansByPossessives//LengthCombining these two lists should result in a fairly complete list of mathematician names:&#10005 allMathematicians=Union[mathematiciansJoinedByDashes,mathematicansByPossessives]; allMathematicians//LengthThe results seem pretty decent:&#10005 Multicolumn[RandomSample[allMathematicians,16]]Now we need to find possible ways to write down the names of each mathematician. After looking through the data, I found a few cases that needed to be corrected manually. Specifically, a few names are written out for the famous Jacob Lurie and R. Ranga Rao, so they need to be corrected to clean up the results a bit:&#10005 mathematicianToPossibleNames=GroupBy[allMathematicians,RemoveDiacritics/*StringReplace[\"_\"->\" \"]/*toStandardName]; mathematicianToPossibleNames[\"Lurie\"]=PrependTo[mathematicianToPossibleNames[\"Lurie\"],\"Jacob Lurie\"]; mathematicianToPossibleNames[\"Ranga\"]=PrependTo[mathematicianToPossibleNames[\"Ranga\"],\"Ranga Rao\"];Now, we need to construct an Association to point from individual name words to their mathematicians:&#10005 possibleNameToMathematicians=mathematicianToPossibleNames//RightComposition[ KeyValueMap[Thread[Rule[##]]&], Flatten, GroupBy[Last->First], Map[Entity[\"Mathematician\",#]&/@Flatten[#]&] ];Note that these entities do not formatwell create the EntityStore for them soon:&#10005 possibleNameToMathematicians[[;;10]]From here, we can break the proposition labels into words that we can use to look up mathematicians (taking care to fix a few cases that need special attention for full names written out):&#10005 propositonToLabelWords=propositionToLabel//RightComposition[ Map[ StringReplace[prefix:(\"De\"|\"de\"|\"von\"|\"Von\"|\"Jacob\"|\"Alex\"|\"Yoon\"|\"Ranga\")~~Whitespace|\"-\"~~name:(namePattern|\"Ho\"|\"Lurie\"|\"Lee\"|\"Rao\"):>prefix<>\"_\"<>name]/*(StringSplit[#,Whitespace|\"-\"]&)/*(StringTrim[#,\"'s\"|\"s'\"]&)/*StringReplace[\"_\"->\" \"] ] ];&#10005 propositionToMathematicians=DeleteDuplicates@Flatten@DeleteMissing@Lookup[possibleNameToMathematicians,#]&/@propositonToLabelWords;With this done, we can add this data to the proposition store as a new property:&#10005 EntityProperty[\"MathematicalProposition\",\"NamedMathematicians\"][\"Label\"]=\"named mathematicians\"; KeyValueMap[(#1[\"NamedMathematicians\"]=#2)&,propositionToMathematicians];And by rearranging the data, we can build the data to create an EntityStore for mathematicians:&#10005 KeyTake[mathematicanToPropositions,{Entity[\"Mathematician\", \"DeFinnetti\"],Entity[\"Mathematician\", \"Lurie\"],Entity[\"Mathematician\", \"Ranga\"]}]Using this data, we can now build an EntityStore for mathematicians, taking into account the data weve accumulated:&#10005 mathematicanData=Association@KeyValueMap[ Rule[ #1, First[#2], \"PossibleNames\"->#2, \"NamedPropositions\"->Lookup[mathematicanToPropositions,Entity[\"Mathematician\",#1]] |> ]&, mathematicianToPossibleNames ];&#10005 mathematicianStore=EntityStore[ { \"Mathematician\"->mathematicanData, \"Properties\"->\"label\"|>, \"PossibleNames\"->\"possible names\"|>, \"NamedPropositions\"->\"named propositions\"|> |> |> } ]; EntityUnregister/@mathematicianStore[]; EntityRegister[mathematicianStore]Now we can see the propositions named after a specific mathematician, such as Euler:&#10005 EntityValue[Entity[\"Mathematician\", \"Euler\"],\"NamedPropositions\"]At this point, many interesting queries are possible. As a start, lets look at a network that connects two mathematicians if they appear in the same proposition name. It sounds somewhat similar to the Mathematics Genealogy Project .An example might be the GrothendieckTarski axiom:&#10005 Entity[\"MathematicalProposition\", \"GrothendieckTarskiAxiom\"][\"NamedMathematicians\"]First, grab all of the named mathematicians for each proposition, taking only those that have at least two:&#10005 groupedMathematicianData=Select[EntityValue[\"MathematicalProposition\",\"NamedMathematicians\",\"EntityAssociation\"],Length[#]>=2&];&#10005 groupedMathematicianData[[;;10]]With a little bit of analysis, we can see that the majority of propositions have two mathematicians, fewer have three, there are a few groups of four and there is only one group of five:&#10005 groupedMathematicianData//Values//CountsBy[Length]//KeySortHere are the top 25 results:&#10005 TakeLargestBy[groupedMathematicianData,Length,25]//Map[Apply[UndirectedEdge]]//KeyValueMap[List]//Grid[#,Alignment->Center,Dividers->All]&We can complete our task by constructing a network of all grouped mathematicians:&#10005 groupedMathematicianEdges=Flatten[(UndirectedEdge@@@Subsets[#,{2}])&/@Values[groupedMathematicianData]]; groupedMathematicianEdges//LengthThe network is quite large and messy.&#10005 Graph[groupedMathematicianEdges,ImageSize->400]With some more processing, we can add weights for repeated edges and use them to determine their opacity:&#10005 weightedMathematicanEdges=ReverseSort[Counts[groupedMathematicianEdges]];&#10005 connectedMathematiciansGraph=Graph[ Keys[weightedMathematicanEdges], EdgeWeight->Normal[weightedMathematicanEdges], EdgeStyle->Normal[Opacity[Sqrt@N[#/Max[weightedMathematicanEdges]]]&/@weightedMathematicanEdges], PlotTheme->\"LargeGraph\", ImageSize->400 ]Here are the most common pairings of mathematicians:&#10005 weightedMathematicanEdges[[;;10]]And heres an easy way to see who has the most propositions named after them, which can be done with SortedEntityClass , new in Mathematica 12:&#10005 mathematicianToPropositions=EntityValue[ SortedEntityClass[\"Mathematician\",EntityFunction[entity,Length[entity[\"NamedPropositions\"]]]->\"Descending\"], \"NamedPropositions\", \"EntityAssociation\" ]; mathematicianToPropositions//LengthHere are the top 25, along with the number of propositions in which they are mentioned on MathOverflow:&#10005 Length/@mathematicianToPropositions[[;;25]]Despite his common appearance in theorems, Euler is surprisingly very low on this list at number 61:&#10005 Position[Keys[mathematicianToPropositions],Entity[\"Mathematician\", \"Euler\"]]And heres the full distribution of mentions (on a log scale for easier viewing):&#10005 Histogram[ Length/@Values[mathematicianToPropositions], PlotRange->Full, ImageSize->400, PlotTheme->\"Detailed\", ScalingFunctions->\"Log\", FrameLabel->{\"# of Named Propositions\",\"# of Mathematicians\"} ]Do It YourselfBe sure to explore further using the newest functions coming soon in Version 12 of the Wolfram Language! Although I used a lot of Wolfram technology to explore the data on MathOverflow. net, here are some open areas that still remain to be explored: What fraction of all questions are unanswered? Which tags have the most answered or unanswered questions? Find named mathematical structures (e.g. integral, group, Riemann sum, etc.) Align entities to MathWorld for further investigation Investigate the time series ratio of user count to post countIn fact, you can download and register the entity stores used in this post here: Pre-generated, incomplete MathOverflow. net EntityStore Completed MathOverflow. net EntityStore Completed mathematical proposition EntityStore Completed mathematician EntityStoreBrush up on your math skills with Fast Introduction for Math Students\n",
      "Sentences: 142\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "variableEqualsNumberDistributions=equations//Keys//StringCases[(\"$$\"|\"$\")~~lhs:(specialLettersPattern|LetterCharacter)~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~rhs:NumberString~~(Whitespace|\"\")~~(\"$$\"|\"$\"):>Rule[ToUpperCase[lhs/.teXRepresentationToCharacter],ToExpression[rhs]]]//Merge[Counts];&#10005KeySort[MinMax/@Keys/@variableEqualsNumberDistributions]//KeyValueMap[Prepend[N@#2,#1<>\" | \"<>ToLowerCase[#1]]&]//Grid[#,Frame->All]&Its interesting to see that most letters are positive, but S is strangely very negative. Its also interesting to note the very large scale of U, V and W. Perhaps not surprisingly, N is the most common letter, though its neighbor O is the least common:&#10005ReverseSort[Total/@variableEqualsNumberDistributions]//BarChart[#//KeySort//Reverse,ChartLabels->Automatic,PlotTheme->\"Detailed\",ImageSize->400,BarOrigin->Left,AspectRatio->1.5]&Trimming these single-variable assignments out of the original equation word cloud makes the results a bit more diverse:&#10005WordCloud[ KeySelect[equations,StringMatchQ[(\"$$\"|\"$\")~~(specialLettersPattern|LetterCharacter)~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~NumberString~~(Whitespace|\"\")~~(\"$$\"|\"$\")]/*Not][[;;50]]//KeyMap[formatTeXSnippet], ImageSize->400, WordSpacings->2 ]Equations of the Form <letter> = <letter>Its interesting to see that there are a lot of letters assigned to (or compared with) another letter. We can make a simple graph that connects two letters in these equations, again taking into account special characters like \\alpha:&#10005letterGraph=Keys[equations]//StringCases[(\"$$\"|\"$\")~~lhs:(specialLettersPattern|LetterCharacter)~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~rhs:LetterCharacter~~(Whitespace|\"\")~~(\"$$\"|\"$\"):>(DirectedEdge[lhs,rhs]/.teXRepresentationToCharacter)]//Flatten//Counts//KeySortBy[First];The graph, without combining upper and lowercase letters, is quite messy:&#10005Graph[ Union@Cases[Keys[letterGraph],_String,Infinity], Keys[letterGraph], GraphLayout->\"CircularEmbedding\", EdgeWeight->Normal[letterGraph], EdgeStyle->Normal[Opacity[N@(#/Max[letterGraph])]&/@letterGraph], VertexLabels->Placed[Automatic,Center], VertexLabelStyle->Directive[Bold,Small], VertexSize->Large, PlotTheme->\"Web\", ImageSize->400 ]If we combine the upper and lowercase letters, the graph becomes a little bit cleaner:&#10005upperCaseLetterGraph=Merge[Normal[letterGraph]/.c_String:>ToUpperCase[c],Total]; Graph[ Union@Cases[Keys[upperCaseLetterGraph],_String,Infinity], Keys[upperCaseLetterGraph], GraphLayout->\"CircularEmbedding\", EdgeWeight->Normal[upperCaseLetterGraph], EdgeStyle->Normal[Opacity[N@(#/Max[letterGraph])]&/@upperCaseLetterGraph], VertexLabels->Placed[Automatic,Center], VertexLabelStyle->Bold, VertexSize->Large, PlotTheme->\"Web\", ImageSize->400 ]If we again remove these equation types, the word cloud becomes even cleaner:&#10005WordCloud[ KeySelect[equations,StringMatchQ[ Alternatives[ (\"$$\"|\"$\")~~(specialLettersPattern|LetterCharacter)~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~NumberString~~(Whitespace|\"\")~~(\"$$\"|\"$\"), (\"$$\"|\"$\")~~(specialLettersPattern|LetterCharacter)~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~LetterCharacter~~(Whitespace|\"\")~~(\"$$\"|\"$\") ] ]/*Not][[;;50]]//KeyMap[formatTeXSnippet], ImageSize->400, WordSpacings->2 ]Functional EquationsAnother interesting subset of equations to look into is functional equations. With a little bit of string pattern matching, we can find many examples:&#10005functionalEquations=KeySelect[equations,StringMatchQ[___~~(f:_)~~\"(\"~~__~~\")\"~~__~~(f:_)~~\"(\"~~__~~\")\"~~___]]; functionalEquations//Length&#10005functionalEquations[[;;16]]//Keys//Map[formatTeXSnippet]//Multicolumn[#,Frame->All]&By focusing on functional equations that have one function with arguments on the left side of an equals sign, we get fewer results:&#10005functionalEquations2=KeySelect[equations,StringMatchQ[(\"$$\"|\"$\")~~(f:LetterCharacter)~~\"(\"~~__~~\")\"~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~__~~(f:_)~~\"(\"~~__~~\")\"~~___~~(\"$$\"|\"$\")]]; functionalEquations2//Length&#10005functionalEquations2[[;;16]]//Keys//Map[formatTeXSnippet]//Multicolumn[#,Frame->All]&However, well need to go further to find equations that are easier to work with. Lets limit ourselves to single-letter, single-argument functions:&#10005functionalEquations3=KeySelect[equations,StringMatchQ[(\"$$\"|\"$\")~~(f:LetterCharacter)~~\"(\"~~x:LetterCharacter~~\")\"~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~__~~(f:_)~~\"(\"~~x:_~~\")\"~~___~~(\"$$\"|\"$\")]]; functionalEquations3//Length&#10005functionalEquations3[[;;16]]//Keys//Map[formatTeXSnippet]//Multicolumn[#,Frame->All]&This is much more pointed, but we can go further. If we limit ourselves to functional equations with only one equals sign with single, lowercased arguments that only consist of a single head and argument (modulo operators and parentheses), we find just six equations:&#10005functionalEquations4=KeySelect[ equations, StringMatchQ[s:((\"$$\"|\"$\")~~(f:LetterCharacter)~~\"(\"~~x:LetterCharacter? LowerCaseQ~~\")\"~~(Whitespace|\"\")~~\"=\"~~(Whitespace|\"\")~~__~~(f:_)~~\"(\"~~x:_~~\")\"~~___~~(\"$$\"|\"$\"))/;(StringCount[s,\"=\"]===1&&StringFreeQ[s,\"\\\\\"~~LetterCharacter..]&&Complement[Union[Characters[s]],{\"$\",\"^\",\"(\",\")\",\"-\",\"+\",\"=\",\"{\",\"}\",\". \",\" \",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"}]===Sort[{f,x}])] ]; functionalEquations4//Length&#10005functionalEquations4//Keys//Map[formatTeXSnippet]//Column[#,Frame->All]&Interestingly, there are only two functionally unique equations in this list:f(x) = 1 + x f(x)^2f(x) = 1 + x^2 f(x)^2If we clean up these functional equations, we can put them through Interpreter[\"TeXExpression\"] to get actual Wolfram Language representations of them:&#10005interpretedFunctionalEquations=Interpreter[\"TeXExpression\"][StringReplace[f:LetterCharacter~~\"(\"~~x:LetterCharacter~~\")^\"~~(\"{\"|\"\")~~n:DigitCharacter~~(\"}\"|\"\"):>\"(\"<>f<>\"(\"<>x<>\")\"<>\")^\"<>n]/@Keys[functionalEquations4]]/.C[x_]:>F[x]Finally, we can solve these equations with RSolve :&#10005AssociationMap[ Replace[eqn:((f_)[x_]==rhs_):>RSolve[eqn,f[x],x]], interpretedFunctionalEquations ]Analyze Big O Notation ArgumentsMoving past equations, another common notation among mathematicians is big O notation. Frequently used in computational complexity and numerical error scaling, this notation should surely appear somewhat frequently on MathOverflow. Lets take a look by findingsnippets wrapped in O that consist of a single argument and have equal numbers of open-and-close parentheses:&#10005bigONotationArguments=StringCases[Keys[teXToCount],Shortest[\"O(\"~~args__~~\")\"]/;StringFreeQ[args,\",\"|\";\"]&&StringCount[args,\"(\"]===StringCount[args,\")\"]:>\"$$\"<>args<>\"$$\"]//Flatten//Counts//ReverseSort; bigONotationArguments//LengthThe results are varied:&#10005bigONotationArguments[[;;100]]//KeyMap[formatTeXSnippet]//WordCloud[#,WordSpacings->3]&One can note that many of these results are functionally equivalentthey differ only in the letter chosen for the variable. We can clean these cases up with a little bit of effort:&#10005normalizeBigOStrings=StringReplace[ { (* Any constant number  1 *) \"$$\"~~NumberString~~\"$$\"->\"$$1$$\", \"$$\"~~LetterCharacter~~\"$$\"->\"$$n$$\", \"$$\"~~LetterCharacter~~\"^\"~~exp:(DigitCharacter|(\"{\"~~__~~\"}\"))~~\"$$\":>\"$$n^\"<>exp<>\"$$\", \"$$\"~~LetterCharacter~~\"\\\\log \"~~LetterCharacter~~\"$$\":>\"$$n\\\\log n$$\", \"$$\"~~b:DigitCharacter~~\"^\"~~x_~~\"/\"~~x_~~\"$$\":>\"$$\"<>b<>\"^n/n$$\", \"$$\"~~numerator:DigitCharacter~~\"/\"~~LetterCharacter~~\"$$\":>\"$$\"<>numerator<>\"/n$$\", \"$$\"~~factor:DigitCharacter~~LetterCharacter~~\"$$\":>\"$$\"<>factor<>\"n$$\", \"$$1/\"~~LetterCharacter~~\"^\"~~exp:(DigitCharacter|(\"{\"~~__~~\"}\"))~~\"$$\":>\"$$1/n^\"<>exp<>\"$$\", \"$$1/|\"~~LetterCharacter~~\"|^\"~~exp:(DigitCharacter|(\"{\"~~__~~\"}\"))~~\"$$\":>\"$$1/|n|^\"<>exp<>\"$$\", \"$$\"~~LetterCharacter~~\"^\"~~exp:(DigitCharacter|(\"{\"~~__~~\"}\"))~~\"\\\\log \"~~LetterCharacter~~\"$$\":>\"$$n^\"<>exp<>\"\\\\log n$$\", \"$$|\"~~LetterCharacter~~\"|^\"~~exp:(DigitCharacter|(\"{\"~~__~~\"}\"))~~\"$$\":>\"$$|n|^\"<>exp<>\"$$\", \"$$\\\\\"~~op:(\"log\"|\"dot\")~~Whitespace ~~LetterCharacter~~\"$$\":>\"$$\\\\\"<>op<>\" n$$\", \"$$\\\\log|\"~~LetterCharacter~~\"|$$\"->\"$$\\\\log|n|$$\", \"$$\\\\sqrt{\"~~LetterCharacter~~\"}$$\":>\"$$\\\\sqrt{n}$$\", \"$$\"~~LetterCharacter~~\"/\\\\log \"~~LetterCharacter~~\"$$\":>\"$$n/\\\\log n$$\" }/.LetterCharacter->(specialLettersPattern|LetterCharacter) ];&#10005normalizedBigOArguments=Normal[bigONotationArguments]//RightComposition[ GroupBy[#,First/*normalizeBigOStrings->Last,Total]&, ReverseSort ];Now the data is much cleaner:&#10005normalizedBigOArguments[[;;15]]//KeyMap[formatTeXSnippet]//Dataset&#10005normalizedBigOArguments[[;;100]]//Keys//Map[formatTeXSnippet]//MulticolumnAnd the word cloud looks much nicer:&#10005wc=normalizedBigOArguments[[;;50]]//KeyMap[formatTeXSnippet]//WordCloud[#,WordSpacings->3,ImageSize->400,ScalingFunctions->Sqrt]&Lastly, since these are arguments to O, lets set the word cloud as an argument of O to make a nice picture:&#10005Style[HoldForm[O][wc],90]//TraditionalFormMentioned Propositions and MathematiciansAnother way to analyze MathOverflow is to look at the mathematical propositions and famous mathematicians that are mentioned in the post bodies. An easy way to do this is to use more entity stores to keep track of the different types. Mathematical Propositions: Build EntityStoreTo begin, lets set up an EntityStore for mathematical propositions and their types. Specifically, we can set up \"MathematicalPropositionType\" for base words like theorem, hypothesis and conjecture, and \"MathematicalProposition\" for specific propositions like the mean value theorem and Zorns lemma. The proposition types will serve as a means of programmatically finding the specific propositions, so well need to pre-populate \"MathematicalPropositionType\" with entities, but we can leave it empty of entities for nowwell populate that type in the store by processing the post bodies, but well do that next. Note that Ive added some properties to keep track of the propositions found in each post. Specifically, \"Wordings\" will hold an Association with strings for the keys and the counts of each of those strings for the values. Additionally, well set up \"MentionedPostCount\" to keep track of the number of times a post is mentioned:&#10005propositionStore=EntityStore[ { \"MathematicalPropositionType\"-><| \"Entities\"-><| \"Theorem\"-><|\"Label\"->\"theorem\"|>, \"Hypothesis\"-><|\"Label\"->\"hypothesis\"|>, \"Principle\"-><|\"Label\"->\"principle\"|>, \"Conjecture\"-><|\"Label\"->\"conjecture\"|>, \"Thesis\"-><|\"Label\"->\"thesis\"|>, \"Lemma\"-><|\"Label\"->\"lemma\"|>, \"Corollary\"-><|\"Label\"->\"corollary\"|>, \"Axiom\"-><|\"Label\"->\"axiom\"|> |>, \"Properties\"-><| \"Label\"-><|\"Label\"->\"label\"|> |> |>,  \"MathematicalProposition\"-><| \"Entities\"-><||>, \"Properties\"-><| \"PropositionType\"-><| \"Label\"->\"proposition type\" |>, \"Wordings\"-><| \"Label\"->\"wordings\", \"DefaultFunction\"->Function[<||>], \"FormattingFunction\"->ReverseSort |>, \"Label\"-><| \"Label\"->\"label\", \"DefaultFunction\"->Function[entity,entity[\"Wordings\"]//Keys//First] |>, \"MentionedPostCount\"-><| \"Label\"->\"mentioned post count\", \"DefaultFunction\"->Function[0] |> |> |> } ]; EntityUnregister/@propositionStore[]; EntityRegister[propositionStore]PopulateNow that the EntityStore is set up and registered, we can use the properties I set up in the store. Lets start with a list of theorems that dont have names in them:&#10005$specialTheorems={\"prime number theorem\",\"central limit theorem\",\"implicit function theorem\",\"spectral theorem\",\"incompleteness theorem\",\"universal coefficient theorem\",\"intermediate value theorem\",\"mean value theorem\",\"uniformization theorem\",\"inverse function theorem\",\"four color theorem\",\"binomial theorem\",\"index theorem\",\"fundamental theorem of algebra\",\"residue theorem\",\"dominated convergence theorem\",\"open mapping theorem\",\"ergodic theorem\",\"fundamental theorem of calculus\",\"h-cobordism theorem\",\"closed graph theorem\",\"modularity theorem\",\"adjoint functor theorem\",\"geometrization theorem\",\"primitive element theorem\",\"fundamental theorem of arithmetic\",\"fixed point theorem\",\"4-color theorem\",\"four colour theorem\",\"isotopy extension theorem\",\"proper base change theorem\",\"well-ordering theorem\",\"loop theorem\",\"slice theorem\",\"odd order theorem\",\"isogeny theorem\",\"group completion theorem\",\"convolution theorem\",\"reconstruction theorem\",\"equidistribution theorem\",\"contraction mapping theorem\",\"principal ideal theorem\",\"ergodic decomposition theorem\",\"orbit-stabilizer theorem\",\"4-colour theorem\",\"tubular neighborhood theorem\",\"three-squares theorem\",\"martingale representation theorem\",\"purity theorem\",\"triangulation theorem\",\"multinomial theorem\",\"graph minor theorem\",\"strong approximation theorem\",\"universal coefficients theorem\",\"localization theorem\",\"positive mass theorem\",\"identity theorem\",\"cellular approximation theorem\",\"transfer theorem\",\"bounded convergence theorem\",\"fundamental theorem of symmetric functions\",\"subadditive ergodic theorem\",\"annulus theorem\",\"rank-nullity theorem\",\"elliptization theorem\"};Next, we can build a function that will introduce new \"MathematicalProposition\" entities, keeping track of how often they are mentioned, their types and specific wordings for later use in cleaning things up. Note that we strip off any possessives and remove special characters via RemoveDiacritics :&#10005ToCamelCase[s_String]:=First@ToCamelCase[{s}]; ToCamelCase[s:{__String}]:=StringSplit[RemoveDiacritics[s]]//Map[Capitalize/*StringJoin]; toStandardName=ToCamelCase[StringReplace[StringRiffle@StringTrim[StringSplit[#],\"'s\"],{\"'\"->\"\",Except[LetterCharacter|DigitCharacter]->\" \"}]]&; With[ {propositionTypePattern=Alternatives@@EntityValue[EntityList[\"MathematicalPropositionType\"],label]}, ClearAll[addPropositionEntity]; addPropositionEntity[proposition_String]:=With[ { entity=Entity[\"MathematicalProposition\",toStandardName[proposition]] },  (* Keep track of mentions *) entity[\"MentionedPostCount\"]=Replace[entity[\"MentionedPostCount\"],{i_Integer:>i+1,_->0}];  (* Keep track of specific wordings and their counts *) entity[\"Wordings\"]=Replace[Replace[entity[\"Wordings\"],Except[_Association]-><||>],a_Association:>Append[a,proposition->Lookup[a,proposition,0]+1]];  (* Extract PropositionType *) entity[\"PropositionType\"]=StringCases[proposition,propositionTypePattern,IgnoreCase->True]//Replace[{{x_String,___}:>Entity[\"MathematicalPropositionType\",Capitalize[ToLowerCase[x]]],_->Missing[\"NotAvailable\"]}];  entity ] ];Note that there are currently no proposition entities:&#10005But if we run the list of special theorems through the function&#10005 then there are proposition entities defined:&#10005EntityList[\"MathematicalProposition\"]//Take[#,UpTo[5]]&We should reset counters for the introduced entities to keep things uniform (the list I provided was fabricatedthose strings did not come from actual posts, so not resetting these values may throw off the numbers a bit):&#10005#[\"MentionedPostCount\"]=0;&/@EntityList[\"MathematicalProposition\"]; #[\"Wordings\"]=Association[#[\"Label\"]->1];&/@EntityList[\"MathematicalProposition\"];Of course, we can go further and detect other forms of propositions. Specifically, lets look for propositions of the following forms:One of the special theorems we just introduced<person name> theorem (and similar)theorem of <person name> (and similar)When we find these propositions, we can add them as entities to the proposition EntityStore (via addPropositionEntity), as well as store them with the posts so lookups are faster (as they will already be stored in memory through the EntityStore).To start, well need to do some normalization. Heres a useful function that uses a list of words that should always be lowercased:&#10005$lowercaseWords= list of words that should be lowercased; lowerCaseSpecificWords= StringReplace[(#->ToLowerCase[#])&/@$lowercaseWords];Additionally, heres a list of ordinals and how to normalize them (including Lastfor example, as in Fermats last theorem):&#10005$ordinalToWord=<|\"1st\"->\"First\",\"2nd\"->\"Second\",\"3rd\"->\"Third\",\"4th\"->\"Fourth\",\"5th\"->\"Fifth\",\"6th\"->\"Sixth\",\"7th\"->\"Seventh\",\"8th\"->\"Eighth\",\"9th\"->\"Ninth\",\"10th\"->\"Tenth\",\"last\"->\"Last\"|>; $ordinals=Join[ Flatten[{ToLowerCase[#],#}&/@Values[$ordinalToWord]], List@@Keys[$ordinalToWord] ]//ReverseSortBy[StringLength];Now we can create a function to extract propositions from strings, normalize them with normalizeString from earlier and then create new \"MathematicalProposition\" entities using addPropositionEntity:&#10005With[ { propositionTypePattern=EntityList[\"MathematicalPropositionType\"]//label//Join[#,Capitalize/@#]&//Apply[Alternatives], upperCaseWordPattern=(WordBoundary|Whitespace ~~(_?UpperCaseQ ~~ (LetterCharacter| \"-\"|\"'\")..)~~WordBoundary|Whitespace), anyCaseWordPattern=(WordBoundary|Whitespace ~~((Alternatives@@$ordinals)|( (LetterCharacter| \"-\"|\"'\")..))~~WordBoundary|Whitespace), possibleOrdinalPattern=(Alternatives@@$ordinals~~Whitespace)|\"\" }, extractNamedPropositions=RightComposition[ normalizeString,  DeleteDuplicates@Join[  (* Case 1: E.g. \"central limit theorem\" *) StringCases[#,Alternatives@@$specialTheorems,IgnoreCase->True],  StringCases[ #, Alternatives[  (* Case 3: \"(nth) Theorem of Something (Something (Something))\" *) Shortest[(WordBoundary|Whitespace)~~possibleOrdinalPattern~~propositionTypePattern ~~ Whitespace~~\"of\"~~Longest@Repeated[upperCaseWordPattern,3]],  (* Case 2: \"Something (something (something)) Theorem\" *) Shortest[(WordBoundary|Whitespace)~~(x:upperCaseWordPattern)~~Longest@Repeated[anyCaseWordPattern,2]~~propositionTypePattern]/;(Not@StringMatchQ[StringTrim[x],\"The\"|\"A\"|\"Use\",IgnoreCase->True]) ] ] ]&,  (* Remove cases with useless words in them *) Select[ With[ (* Ignore \"of\" so that case #3 is allowed *) {split=DeleteCases[StringSplit[ToLowerCase[#],Whitespace|\"-\"],\"of\"]}, split===DeleteCases[DeleteStopwords[split],\"using\"|\"like\"|\"phd\"|\"understanding\"|\"finally\"|\"concerning\"|\"regarding\"|\"ℳℋ\"|\"\"|\"satisfies\"|\"following\"|\"stated\"|\"usually\"|\"implies\"|\"hence\"|\"course\"|\"assuming\"|\"wikipedia\"|\"article\"|\"usual\"|\"actually\"|\"analysis\"|\"entitled\"|\"apply\"] ]& ],  StringReplace[Normal@$ordinalToWord], lowerCaseSpecificWords, StringTrim, DeleteDuplicates, Map[addPropositionEntity] ] ];Lets try the function on a simple example:&#10005We can see that an entity was added to the store:&#10005We can also see that its properties were populated:&#10005Entity[\"MathematicalProposition\", \"MartinAxiom\"][\"PropertyAssociation\"]Of course, we can automate this a bit more by introducing this function as a property for MathOverflow posts that will store the results in the EntityStore itself:&#10005EntityProperty[\"StackExchange. Mathoverflow:Post\",\"NamedPropositions\"][\"Label\"]=\"named propositions\"; EntityProperty[\"StackExchange. Mathoverflow:Post\",\"NamedPropositions\"][\"DefaultFunction\"]=Function[entity,entity[\"NamedPropositions\"]=extractNamedPropositions[entity[\"Body\"]]];Lets test out the property on the same Entity as before:&#10005We can see that the in-memory store has been populated:&#10005Entity[\"MathematicalProposition\"][\"EntityStore\"]Pro tip: in case you want to continue to work on an EntityStore youve been modifying in-memory in a future Wolfram Language session, you can Export Entity[\"type\"][\"EntityStore\"] to an MX file and then Import it in the new session. Just dont forget to register it with EntityRegister! At this point, we can now gather propositions mentioned in all of the MathOverflow posts, taking care to reset the counters again to avoid contamination of the results:&#10005(* Reset counters again to avoid contaminating the results *) #[\"MentionedPostCount\"]=0;&/@EntityList[\"MathematicalProposition\"]; #[\"Wordings\"]=Association[#[\"Label\"]->1];&/@EntityList[\"MathematicalProposition\"];Note that this will take a while to run (it took about 20 minutes on my machine), but it will allow for a very thorough analysis of the sites content. After processing all of the posts, there are now over 10k entities in the proposition EntityStore:&#10005\n",
      "Sentences: 28\n",
      "\n",
      "\n",
      "Article: http://www.cat-bus.com/2017/12/gadgetbahn/\n",
      "NodeRank:\n",
      "Previous postNext post December 3rd, 2017 by ant6nFor some time, Ive been meaning to write about German transportation systems like whats an S-Bahn or whats a Stadtbahn. With the recent news out of Quebec, I figured Id instead talk about a transportation concept that doesnt actually transport anybody at all: the Gadgetbahn. The word is a portemanteau of the English Gadget and the German word bahn, which means rail or train. A gadgetbahn is a speculative transportation concept that proposes to solve planning and financial issues via some sort of magical techno-fix, likely some technology that doesnt even exist yet. Classic examples of gadgetbahns are: monorails, personal rapid transit, maglevs, or the newest addition to the family, the hyperloop. Proponents of these technologies may be referred to as gadgetbahn enthusiasts, or more derogatorily, pod-people. They often promise the sky in terms of reduced cost and increased speed and comfort, often with little consideration for capacity, risk, safety or realism. Back to Quebec: the prime minister decided he wants a fast transit link between Montreal and Quebec City, but not a train, because we can do much more modern things now. Instead, he wants something futuristic, something from the minds of Quebecers. (note from editor: Sorry to pop his bubble, but Quebec is not exactly known for having a long history on the bleeding edge of transit technology. )The problem of the gadgetbahn isnt necessarily that its a techno-fix. Its that it is a technology for the sake of technology, a shiny gizmo to brag about, with little regard to solving the actual transportation problem. The transportation minister clarified his position himself: he wants anything you can conceive of, any project; innovate, come up with new ideas! And somehow, some consortium sprung up ready with proposals, renderings, promises and deadlines to build a high speed monorail (MGV), and all they want is a quarter of a billion dollars to develop it. The beauty of proposing a gadgetbahn is that since it doesnt exist, proponents can make up all sorts of quasi-magical properties for their technology, which supposedly make it superior. Since there arent real-world examples, proponents can use the most optimistic theoretical scenarios they can come up with, and compare them with the actual performance of projects that have been built and which are bound to the constraints of the real world. For example, the high speed monorail promoters claim their system is cheaper than conventional rail, because they could just use existing highway medians, with an elevated rail system where vehicles are suspended from above. The MGV (monorail a grande vitesse), high-speed pods on elevated tracks, suspended from aboveWhen you look at its basic structure, compared to conventional rail, this monorail essentially differs in the method of propulsion: instead of two wheels on two rails below the train, we have two wheels on one rail above the train. The main conceptual difference between conventional rail and MGV:the location of the wheelsLike for any gadgetbahn, the claim is that this new technology provides more speed, more comfort at lower cost. But in the real world, these three aspects are always intricately connected and subject to tradeoffs  due to simple geometry and physics. Want faster transportation? Then you need very straight tracks. Dont have straight tracks but still want high speed? Then your trip will become a barf-ride, so less comfort. Want to build cheaply in an already existing highway median? Well the highway curves are made for cars going at 100km/h, so your choices are:slow down (less speed),run inside the existing geometry at higher speed (less comfort),straighten the curves (more expensive).These problems will always come together. At the end of the day, youre still pushing a metal can full of people at high speeds you cant get out of issues of geometry and physics by changing where you put the wheels. Going further, once it appears the basic problems have been overcome, the next issues become capacity, safety, energy and access (stations).For example, Elon Musk is proposing to build tunnels with his Boring Company, which will supposedly be cheap, because the tunnels will be relatively narrow. This reduces capacity, because only smaller vehicles will fit in the tunnel. To compensate, he may propose that vehicles will run super close together  which will represent a huge safety issue unless they run slowly. A small tunnel with a lot of vehicles that are barely smaller than the tunnel diameter, running at high speed may become a death trap in case of emergency, if there isnt enough space and facilities for egress. If somehow he manages to solve all these issues, theres still the problem of getting everybody into this tube of his. All these considerations come down to the one fundamental constraint on which everything else depends: the right of way  how much space do you have available, how straight is your path, how and where is the downtown access, and how much space is available at stations. An example of this issue is the maglev technology (essentially magnetically elevated monorails). The concept has been around for ages. The Germans and Japanese have been developing the technologies for a long time. In the early 2000s, the Germans were able to sell their Transrapid technology to China for the Shanghai Maglev airport connector, a 30-km line. It was a pilot project, with the hope to eventually cover the whole country with maglev network. The Maglev was seen as the next generation of trains, mostly by being faster. But there was also the hope that it could even be cheaper, by putting the whole track on stilts, without having large elevated bridge-like structures. But in the end, conventional high speed rail and maglev require similar geometries, a similar kind of infrastructure, and it turns out that the speeds are not that different (Shanghai Maglev up to 430km/h, conventional rail up to 350km/h, both with a maximum experimental speed of around 600km/h).At more than 300km/h, a lot of energy is spent to overcome the air drag of the trains themselves. Issues also become noise, and the required straightness of the infrastructure. And the issue that with stops, decreasing travel time by increasing maximum speeds becomes marginal. Overall, speeds above 300km/h tend to become uneconomical, no matter the propulsion system. From a distance, elevated conventional high speed rail (left) and maglev (right) appear strangely similar  the requirements on the tracks are due to physics and geometry, so the propulsion method alone wont give one system a clear advantage over the other in terms of infrastructure costAll of these issues come together to make the technological choice a bit of a wash. In the end it makes more economic sense to build from conventional technology which has been developed for a longer time, has multiple vendors, has existing infrastructure that can be built on top of. This way, you can upgrade lines for high speed service, while being able to run the new high speed trains on existing tracks inside cities or to other cities. This provides a huge economic advantage. So indeed, although the Shanghai Maglev works, in an economic sense its a failure. It convinced the Chinese to focus on high speed rail: ten years after the opening of the Maglev, the country still had the same 30km of maglev, but built 20,000km of high speed rail. (Oh and btw, Ive taken the maglev, its not a very smooth ride; it rumbles like a rollercoaster)The real issue of using a gadgetbahn to solve a transportation problem is not really technical. Its that it re-frames the problem to build an infrastructure as the problem to develop a new technology  now youve got two problems to solve! The technical problem should really be solved through private investment, not public funding. Thus I view the offer to develop the high speed mono-rail for only 250M$ with a great amount of distrust. If the idea is viable and the people behind it are competent, it should have attracted private investment, as there should be a potential to make profit selling the technology. After all, the proposal has been around for 23 years. When proposing a completely new technology in order to solve a specific transportation problem, the major problem is Risk:Technological risks: Will it even work? Will it deliver on claims? Will it have sufficient capacity? Cost-related risks: How much will it cost to develop? How much will one kilometre cost once the technology developed? Regulatory/safety risks: Will it be safe? What will safety requirements be? How fast will we be allowed to run? Risks because timelines are not understood: How long will it take to develop? How long will it take to plan & construct? Operating Risks: How much will it cost to operate? How much will it cost to maintain the infrastructure? Will automated/unattended operation actually be possible? Risks because we need to build complete systems at once, rather than incrementally updating existing infrastructure. Also, new technology means relying on a single vendor, which is again extremely risky, both in terms of cost and availability of the technology in the future: Will the vendor exist in the future? How much will they charge us once we depend on them, given the lack of competition? For any technology thats already existed and been researched for decades, and that may have tens of thousands or hundreds of thousands of kilometers of infrastructure built today, all these question are much easier to answer. There will be much less risk, and thus less cost. Any time somebody in power proposes to solve an infrastructure problem by first developing some new technology, or by using some proposed technology that hasnt been delployed yet, we should be distrustful. Often, the proposal may simply be an excuse to not invest in infrastructure today, because tomorrow some techno-fix will come along and solve all our problems for free. This tactic may work especially well if the proposal includes an appeal to some futuristic dream or a nationalistic project. The gadgetbahn may really just be a big diversion. For example in Quebec, the most realistic scenario to get a fast link between Montreal and Quebec is the high-frequency train proposed by VIA rail, which would link the cities with conventional rail. After repeatedly proposing high speed rail for the last 40 years and getting no support from the governments, VIA decided to propose a system thats not true high speed rail  but it would use dedicated passenger rail tracks, allowing somewhat faster speed than today, and no interference with freight. This would hit an economic sweet spot for VIA, allowing them to almost finance and build the system themselves, with only relatively little public support. But the main problem is the access to Montreal  VIA hoped to use the Mont-Royal rail tunnel to access Gare Centrale. But with the REM light rail project pushed hard by the prime minister, the tunnel would be converted to light rail, an incompatible technology, which will cut VIA off. In some sense, the prime ministers announcement that he wants some gadgetbahn to Quebec City means VIAs rail project would become obsolete, and the conflict would become moot. So is it really just a diversionary tactic to hide the regional rail planning problems in Montreal? Interesting related reading (and with thanks for some inspirations) by Alon Levy: Loopy Ideas Are Fine, If Youre an EntrepreneurPublished  on Sunday, December 3rd, 2017 at 20:30  Filed under Montréal, technology, trains. Comments RSS |  trackback2017 in ReviewHow VIA Rail Torpedoed its Own High-Frequency Rail Project and Montreals chance for Regional RailThe REM Proposal is the Best Transit Project Montreal has seen in 30 Years Peter L Says:December 4th, 2017 at 13:07 Nicely done. I usually end up not reading your blog, not because I dont like it or you, but because I get angry and cant think straight when reading about REM! Here, I think youve done a good job covering the gadgetbahn phenomenon something certainly not limited to Montreal or Quebec (see, for example, the gondola ride proposed for NYC).I expect to be posting a link to this in many discussions going forward! And remember:  monorails, maglevs, and now hypeloops and robot cars are the transportation of the future! Always have been, always will be! Mr. Chinaski Says:December 4th, 2017 at 19:20 Not one mention of the Chuo Shinkansen Maglev instead of the Chinese one? Cmon Daniel Says:December 5th, 2017 at 03:51 One note: Despite gadgetbahn may look german, it is not. Though Gadget and Bahn are known. Isnt the new mayor of ville de REM against the mentioned mount royal solution? ant6n Says:December 5th, 2017 at 04:28 @Mr ChinaskiI mentioned Japenese development in the area; and the maximum speed (~600km/h) comes from that project as well. Tue Shanghai Maglev was mentioned to make a particular point. If you think this article is supposed to be a survey of speculative transportation technology, I think you may have missed the point. Zweisystem Says:December 5th, 2017 at 19:13 Vancouvers SkyTrain is a good example of gadgetbahnenn. With much hype and hoopla, the driverless ALRT/ART was said to revolutionize public transport. Well that was in 1984 and in 2017 Vancouvers SkyTrain has not lived up to its promises of revolutionizing transit. Only 7 such system have been built in almost 40 years and its marketing name has been changed at least four times. ICTS > ALRT > ALM > ART. Even though SkyTrain is driverless, it costs about 40% more to operate when compared to light rail systems. Very expensive to build, up to 10 times more than LRT (1982 IBI Study for the TTCs ART Study) to build. Vancouvers SkyTrain system also has limited capacity and its Transport Canada Operating certificate limits capacity to 15,000 pphpd, unless about $3 billion is spent up grading the line and re building stations. To date, the taxpayer has spent over $10 billion on SkyTrain or about $7 billion more than if light rail were built instead. The big problem is that politicians , engineers, bureaucrats and academics have been bamboozled by the proprietary light metro and continue planning for more. The have deified the damn thing. The real winners of gadgetbahnenn are the owners of the patents, as  all gadgetbahnen are indeed proprietary railways. And who own the patents for Vancouvers SkyTrain? Bombardier and SNC Lavalin. Build with gadgetbahnen, better have a lot of cash, because they cost a lot, lot, more than advertised. Gaël Says:December 6th, 2017 at 16:26 What about tilting trains? https://en. m.wikipedia. org/wiki/Tilting_trainArent they a good way of getting more speed and comfort without straightening curves, at a reasonable cost? And Im not a mechanical engineer, but I feel like by putting the wheels above the vehicle rather than under it, you can achieve better passive tilting, since you could eventually let the vehicle rotate freely around the longitudinal axis of the track. I have no idea if this is actually considered for the MGV Peter L Says:December 6th, 2017 at 16:57 The UA TurboTrain had passive tilt. The Talgos all have passive tilt. The LRC had active tilt as does the Acela. All are bandaids for bad track. Put the money into the track. ant6n Says:December 6th, 2017 at 18:38 @GaelI didnt want to go into tilting. Yes tilting allow you smaller curve radius at similar comfort / speed levels. Its a way to trick the issues of physics/geometry/cost that I mentioned. But these tricks are possible with conventional rail as well. Firstly, its possible to use a lot of super-elevation on your tracks (i.e. tilt them). Since VIAs TGF wouldnt host freight trains, I believe the tracks could be very super-elevated. Secondly, tilting trains are possible as well (As Peter points out). We have to wonder whether having a frame within the train that the passively tilting carriage is hanging from is cheaper than having the whole track up on a frame. Further, passive tilting is a bit limited, because its reaction is delayed. Active tilting is preferable  again this is possible with either wheel configuration, but I think it narrows the difference in complexity of the two modes. Furthermore, theres a limit to tilting at high speed, because of the rotational jerk. (Edit: theres also a maximum tilt that you can subject to people before theyll get sea sick).So that brings us back to a similar issue as before: the tilting trick is possible with both wheel arrangements, so the question is whether the economics and the risk of developing a unique solution make sense. Fraser Pollock Says:December 14th, 2017 at 19:38 Many years ago when I was just out of university, I had job where I actually provided information for the public about other Gadgetbahn  systems, when a certain railway vehicle provider (which will remain nameless) was doing possible LRT and Subway/Metro projects. One of my many duties was to show people how many of these so called future public transport systems were really just poorly tested concepts. The reality of many of these pieces of technology were not that they didnt work but could not ever do many of the things that there proponents say they could do. For example, Low cost construction of the infrastructure usually meant that the people who designed the concept had a simple pre-built mass produced vehicle tech or precast concrete construction system in mind for the vehicle, vehicle rights of way (tracks) and stations. The reality was usually that the right of way construction technology was designed for a test vehicle that was 1/4  1/6 the actual size and weight of the production model. This meant whisper thin support poles shown in flashy pictures actually needed to be replaced with massive concrete pylons and decking, which are not cheap and or easy to build. Often they are very expensive to maintain over time. Or the building technology wouldnt work or couldnt legally be used for safety reasons in the particular operating and legal climate being proposed. My favorite was the constant claim of high speed (especially for Personal Rapid Transit Systems) when in the picture or diagram provided the station stops were in eye shot of each other. Forcing very high acceleration/deceleration if high speed was actually even possible. It goes back to the comfort level for the customer the article mentions. Maintenance and spare parts as well as other on going operational costs are often overlooked when gadgetbahn systems try to show how cheap they are. As with many proprietary transport systems the cost of things like, staff operations and maintenance training, spare parts packages as well as support equipment cost and supply are never included in the glossy promotion packages that possible operators get. This is assuming the company pitching the gadgetbahn technology even has these costs on file yet! Never mind climate can make or break new pieces of technology. For example, many of the new wireless power systems for LRT and other rail vehicles dont work well or at all in snowy or overly wet ground conditions, not to mention if its really cold. These are not even pieces of Gadgetbahn technology but technology provided by well known international rail vehicle providers. Bombardiers wireless ground based Pri-Move system for example, doesnt work at all in snow or function well in damp surface conditions and temperatures under 8 degrees C. The one place where has been sold and functions well is in Saudi Arabia, where it is normally bone dry and very hot. Bjarne Says:January 2nd, 2018 at 06:37 Grouping maglev, hyperloop and several other systems into one group and calling it gadgetbahn is a dishonest misrepresentation of these totally different systems. The equivalent would be to call conventional rail industry a dinosaur technology. That would also be condescending and meaningless. It would also be at the same level as calling rail and HSR gadgetbahn before they were invented or implemented many years ago. The list just goes on and on with more condescending expressions likegadgetbahn enthusiasts, pod-people and speculative technologyI guess he forgot to write maglev-fanboys or gadgetbahn-fanboys. It is a good sign of desperation and lack of arguments to write this. The rest of the article does not give much meaning because it mixes totally different systems. They often promise the sky in terms of reduced cost and increased speed and comfort, often with little consideration for capacity, risk, safety or realism. This may be true for systems that does not exist yet, but not for Transrapid 09 (picture in lower right corner).Here is a suggestion. Call HSR HSR. It exists. Call maglev maglev. It exists. Call Hyperloop Hyperloop. It does not exist. Its that it is a technology for the sake of technology, a shiny gizmo to brag about, with little regard to solving the actual transportation problem. This is just a nonsense sentence because these are not real arguments. It is irrelevant if the technology is shiny or worth bragging about. How do you know if a system can solve the actual transportation problem? That is the relevant question to ask. They are totally different systems! Try to do some testing on the following statement. a technology for the sake of the technologyUse the argument for HSR. Now you see it. One should be positive about new ideas and technology and evaluate them objectively! Not on the basis of them being shiny or not! HSR can be shiny too, can it not? Another round of misrepresentations in the article. This time there are 3 errors in the same sentence. But in the end, conventional high speed rail and maglev require similar geometries, a similar kind of infrastructure, and it turns out that the speeds are not that different (Shanghai Maglev up to 430km/h,conventional rail up to 350km/h,both with a maximum experimental speed of around 600km/h).They do not require similar geometries! Maglev only need half the curve-radius (if not even less) compared to HSR. Maglevs can also climb at a degree several times higher than HSR. They do not require a similar kind of infrastructure. Elevated HSR needs elements that are 5(!) times heavier than the ones for maglev. Think of all the concrete and CO2 we can save by building maglev. The speed is also dishonestly misrepresented. Shanghai maglev has a technical top speed of 500 km/h. The reason why it maxes out at 430 km/h is the length of the track (30 km).HSR reduces its speed to 250 km/h in Europe because of the exponentially rising tear and wear which leads to astronomical maintenance costs. French TGV has to replace its wheels several times a year. Not to mention the track and the overhead lines. The experimental top speed of HSR is of little or no interest. The fast TGV train was a heavily modified train and can not be called a normal HSR. The experimental top speeds of maglevs have been from normal maglevs. Not experimental. Another mistake from the article. At more than 300km/h, a lot of energy is spent to overcome the air drag of the trains themselves, which makes these speeds uneconomical either way. So the propulsion system is kind of moot. The propulsion system is important. The main reason to elevate the train from the track is to reduce the wear and tear of the wheels and track. This is, economically speaking, much more important than the use of energy. Furthermore it should be mentioned that the German Transrapid 09 maglev uses 30% less energy than HSR. Maglev has been developed over several decades. These two statements seem to contradict each other. The concept (maglev) has been around for ages. The Germans and Japanese have been developing the technologies for a long time. In the end it makes more economic sense to build from conventional technology which has been developed for a longer time,German maglev has been developed over several decades and is an optimally developed and well tested system. It is ready to be implemented anywhere in the world where high speed ground transportation is wanted and needed. The maglev line in Shanghai was built primarily to get to know the system. How do you think an elevated HSR line in Shanghai would have performed? There is a lot of corruption in China, including in rail. Google it. Check it out. The Chinese are also building a maglev which will probably be based on the German one. https://www. motor1. com/news/129207/china-maglev-train-due-2020/HSR can not necessarily run on existing rail lines through cities and stations. There may be capacity problems and differences in track types and width. The Japanese run their Shinkansen system on a totally separated system. The technical problem should really be solved through private investment, not public funding. How much governmental support has the railway industry got? Building a new infrastructure is a huge project with potential economical advantages for the society as a whole and huge challenges regarding the legal area that the private industry not necessarily will get involved in. An elevated maglev system may, to a large extent, use existing rights of way (motorways and rail), but it will be a huge challenge anyway. How will a new HSR infrastructure built on the ground perform in this area? Will it even work? Will it deliver on claims? Will it have sufficient capacity? Yes. Transrapid 08 has been working very well in Shanghai for over 12 years and has driven more than a million km with millions of passengers without major issues. Compare that to HSR which has an extensive maintenance program. The capacity is at least half a million passenger-km per hour for German maglev. How much will it cost to develop? How much will one kilometre cost once the technology developed? German maglev is already developed. German taxpayers, along with Thyssen-Krupp and Siemens have taken the costs for you and your country. It costs 20-25 million dollars per km. Will it be safe? What will safety requirements be? How fast will we be allowed to run? German maglev is safe. There are several safety systems built into the system. One can also mention elevation as an important safety measure. The train is certified to run 500 km/h in Europe. People can move freely around as in a conventional train. German elevated maglev may be built 3-4 times faster than HSR on the ground. The maglev will also not disturb other infrastructure, waterways, people and animals. The maintenance and operational costs will be much lower than for HSR. No wear and tear on the track. No driver needed. Less trains needed because of superior acceleration and top speed. Maglev can be build incrementally so one can familiarize with the system. HSR has already shown that over 200 km/h the wear and tear skyrockets exponentially with increased speed. The article is full of misrepresentations which accidentally favours HSR. The Transrapid TR09 is a smooth ride. Is there an agenda behind the article? Who is the author? What are his roles? Is the author representing any interests? ant6n Says:January 2nd, 2018 at 12:13 Dear Bjarne, I see you are very defensive about the whole issue, with all sorts of accusations of improper writing and even accusations of conspiracy. I think your anger is playing with your reading comprehension. I didnt mix up the different transportation concepts. You are. If you actually read my article carefully, youll see Im using conventional maglev merely as an example of a kind of technology that was hailed as the solution a long time ago but that didnt really pan out, to warn of supposed technology solutions being proposed today (MGV).Im not really interested in going into your technicals (except for a big citation needed). Suffice to say with all your numbers and claims in vigorous defense of maglev, youre not getting the point that after what, 30-40 years of development, we have 30km in production, maybe well be getting another two lines in another 10-20 years; whereas conventional technology exists in tens of thousands of km. In fact the Chinese build tens of thousands km basically from scratch, after testing maglev. Germany tried several times to build the Transrapid and it was always deemed uneconomical. Being able to incrementally build systems (something you claim is not a problem for maglev), is one of the main problems, as are the economics. But really, if youd spent five minutes of attention on actually reading the article, rather than huffing and puffing about it, youd see that my main concern is how in Quebec (my country) there are many who view the proposed suspended monorail concept as a realistic solution to link Quebec and Montreal. You seem to have completely missed that the article is mainly to warn about the dreamy solutions Gadtgetbahn enthusiasts dream up with the MGV. Oh an btw, I invite you to actually go take the Transrapid. Ive taken it, and it is indeed quite a rumble-ride. Malcolm Says:January 2nd, 2018 at 12:31 Its interesting that two of the pictured Gadgetbahns you head your article with, Ultra and Maglev, are actually in commercial operation. Perhaps, in view of your statement, The beauty of proposing a gadgetbahn is that since it doesnt exist, you may feell the need to either change the images or rephrase your statement? ant6n Says:January 2nd, 2018 at 13:16 When you propose a Gadgetbahn, it doesnt generally exist yet. Thats why you can make claims you dont need to back up with facts. Malcolm Says:January 2nd, 2018 at 15:03 Youre missing my point and, probably, Bjarnes. Ultra is operational. Maglev is operational. Apart from your claim they arent, you are spreading other false information in your opinions. Is it too much to ask for accuracy? Or does Gadgetbahn mean you can make claims where you dont need to back up with facts, even though it does exist? ant6n Says:January 3rd, 2018 at 21:19 @MalcomNo, youre missing the point. It doesnt matter whether the Gadgetbahn enthusiasts got some small sample project running somewhere. The issue isnt whether these technologies are impossible (and I made no such claim).The question is whether the supposed near magical properties pan out (especially compared to existing/conventional technology), how long it took to develop the new technology, and whether the technology makes economical sense in the real world. Malcolm Says:January 4th, 2018 at 10:28 Here is that paragraph in full:The beauty of proposing a gadgetbahn is that since it doesnt exist, proponents can make up all sorts of quasi-magical properties for their technology, which supposedly make it superior. Since there arent real-world examples, proponents can use the most optimistic theoretical scenarios they can come up with, and compare them with the actual performance of projects that have been built and which are bound to the constraints of the real world. !Re Ultra, Transrapid and, to some extent, JR SCM1 They do exist2 There are real word examples3 They dont use quasi-magical properties for their technology4 The technology is superior to rail transport in a lot of respects5 There are well respected sources of information to confirm this but youre not interested in going into the technicals, are you? However, I do take your point that your main concern is the proposal to build a suspended monorail in Quebec and Montreal. Here, you have more knowledge than most reading your post. Its a shame you diluted this concern with an attack on new, developing and developed forms of transport which you lump together under the Gadgetbahn title, compounded with false information. Yes, there are some proposals for transport which seem too good to be true, most of which dont have a prototype up and running, perhaps this is what you mean by Gadgetbahn? Malcolm Says:January 4th, 2018 at 10:30 PS Suspended monorail? Check out https://www. wuppertal. de/microsite/en/tourism/schwebebahn/102370100000140310. phpwhich has been running since 1901 Jarek Says:January 5th, 2018 at 05:08 (Of course, Wuppertalbahn is a local transit system, relatively low-speed and winding with lots of stops  not a high-speed intercity one being discussed in the post) Johannes Urbanski Says:January 5th, 2018 at 20:14 Re Ultra, Transrapid and, to some extent, JR SCM1 They do existThe UIC (basically the IATA of the rail industry) lists 40,378 km of HSR networks as operational (as of November 2017) and another 14,615 km as under construction [1]. How many km of lines exist for SCM, Transrapid or Ultra? 2 There are real word examplesThe UIC lists more than 300 HSR lines, of which more than half are already under operation [1]. What are the real world examples for SCM, Transrapid or Ultra  and even more: how many of them are already in intercity revenue service? 3 They dont use quasi-magical properties for their technologyNone of the claimed properties is quasi-magical  its the multitude of these properties which are claimed simultaneously (and the refusal to acknowledge the existence of any of the trade-offs which have been characterising every single transportation project ever constructed by humanity): lower construction costs, lower operating costs, lower noise, shorter construction period, less visual intrusion, higher operating speeds, lower energy consumption, higher capacity, lower construction-related emissions, higher public acceptance, lower ticket prices. Doesnt a transportation technology which has been tested and proven almost nowhere in the world (see 1 and 2), but comes with absolutely no trade-offs sound outlandish or quasi-magical to you? 4 The technology is superior to rail transport in a lot of respectsAgreed, but do they outweigh the economic, political and environmental disadvantages, such as being incompatible with the legacy rail system and therefore either forcing passengers to do additional transfers or to build punitively expensive downtown tunnels and countless underutilised branch lines? 5 There are well respected sources of information to confirm this but youre not interested in going into the technicals, are you? You mean like the obscure Motor1. com article presented by Bjarne (seriously, this is the quality of sources for which only Hyperloop fanboys are known)? Ive already acknowledged in countless discussions with you and the other Maglev fans (Bjarne, for instance) on the High Speed Rail America Club Facebook page that if you were building and planning an entire country for a population of 100 million in a dessert or on a different planet, Maglevs would be the mode of choice. When will you finally acknowledge that real-world transport planning is different to SimCity, as you have to respect the constraints imposed by decades and centuries of often inconsistent civil engineering construction for transportation infrastructure and other structures built under constantly changing priorities and land use policies? And speaking of the suspended monorail in Wuppertal: you know what I answered my boss when he asked me what do you Germans think about suspended monorails? ?I answered: We invented them over 100 years ago. We never built a second line. Ironically, I could say almost the same about Maglevs: We were one of the first when we started to develop Maglev trains 50 years ago and invested far more than a billion Dollars of our taxpayers money into the technology. In the end, the technology was always deemed to be too expensive and not adaptable enough to our existing transport networks and we therefore invested into HSR networks instead and eventually sold the technology to China where they did test this technology on one short suburban route before they decided to stick to HSR when expanding their intercity networks for basically the same reasons. [1] https://uic. org/IMG/pdf/20171101_high_speed_lines_in_the_world. pdf Fraser Pollock Says:January 18th, 2018 at 13:10 High Speed transport is a wonderful thing! However, if you have ever been on a high speed train you notice a few things. First the trains actually travel at normal speeds inside the built up areas of large cities. Why? They do this is because of safety regulations and people have to live there close to these lines, so the noise level must be kept down. Have you ever stood anywhere near a high speed railway vehicle right of way? I have stood underneath or near both High Speed Passenger Rail (HSR) lines and or a Maglev line with a train or vehicle traveling at or near there optimal cruse speeds, its very, very loud and uncomfortable due to the highly compressed air pressure wave caused by the passing vehicle! So much so, I can easily imagine most people really wouldnt want to live near it if they didnt had the choice to. The Chinese High Speed Maglev line that runs to Shanghais International Airport travels through mainly but not entirely suburban and rural areas. The Germans had required a 300 metre wide (150 metres each side of the line) safety/security barrier around their high speed Maglev test line which was in a suburban area. This was needed because of the air blasts, noise and safety issues. In China where the safety barrier is a mere 20-25 metres, local residents get the full effect of a passing maglev vehicle much closer to their homes. In more rural areas, debris the vehicles pick up and rain down on the crop fields of local collective farms are especially hated by the farmers and mangers a like, they really dont like that aspect of the Maglev lines. The 200 km long planned extension to the Shanghai Maglev system was to have significant underground sections to limit noise, electromagnetic radiation pollution and exposure. Thus, greatly increasing the cost of the extension. When a nearby HSR line was built at much lower cost I might add, the Maglev extension was immediately sidelined. My point is that even in China, where the environmental problems and the comfort level of local residents isnt always the prime consideration of Maglev officials. Even they have had to admit that, high speed transport has to have safety and noise considerations that greatly complicate and add costs to their projects. Imagine the long and drawn out Environmental Assessment Process required in a western country like ours for a very high speed Maglev system. I have seen and read through the older Environmental Assessment proposals for VIAs High Speed Rail lines here in Canada. These proposals have multiple volumes of hundreds of pages each. These are just the proposals not the full Environmental Assessments which would be longer and far more involved. Name (required)Mail (will not be published) (required)Website\n",
      "Sentences: 341\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "For some time, Ive been meaning to write about German transportation systems like whats an S-Bahn or whats a Stadtbahn. With the recent news out of Quebec, I figured Id instead talk about a transportation concept that doesnt actually transport anybody at all: the Gadgetbahn .The word is a portemanteau of the English Gadget and the German word bahn, which means rail or train. A gadgetbahn is a speculative transportation concept that proposes to solve planning and financial issues via some sort of magical techno-fix, likely some technology that doesnt even exist yet. Classic examples of gadgetbahns are: monorails, personal rapid transit, maglevs, or the newest addition to the family, the hyperloop. Proponents of these technologies may be referred to as gadgetbahn enthusiasts, or more derogatorily,  pod-people . They often promise the sky in terms of reduced cost and increased speed and comfort, often with little consideration for capacity, risk, safety or realism. Back to Quebec: the prime minister decided he wants a fast transit link between Montreal and Quebec City, but not a train, because  we can do much more modern things now .Instead, he wants something futuristic, something from the minds of Quebecers. (note from editor: Sorry to pop his bubble, but Quebec is not exactly known for having a long history on the bleeding edge of transit technology. )The problem of the gadgetbahn isnt necessarily that its a techno-fix. Its that it is a technology for the sake of technology, a shiny gizmo to brag about, with little regard to solving the actual transportation problem. The transportation minister clarified his position himself: he wants anything you can conceive of, any project; innovate, come up with new ideas! And somehow, some consortium sprung up ready with proposals, renderings, promises and deadlines to build a high speed monorail (MGV), and all they want is a quarter of a billion dollars to develop it. Cost, Speed, Comfort The problem is geometry, physics and the Right of WayThe beauty of proposing a gadgetbahn is that since it doesnt exist, proponents can make up all sorts of quasi-magical properties for their technology, which supposedly make it superior. Since there arent real-world examples, proponents can use the most optimistic theoretical scenarios they can come up with, and compare them with the actual performance of projects that have been built and which are bound to the constraints of the real world. For example, the high speed monorail promoters claim their system is cheaper than conventional rail, because they could just use existing highway medians, with an elevated rail system where vehicles are suspended from above. The MGV (monorail a grande vitesse), high-speed pods on elevated tracks, suspended from aboveWhen you look at its basic structure, compared to conventional rail, this monorail essentially differs in the method of propulsion: instead of two wheels on two rails below the train, we have two wheels on one rail above the train. The main conceptual difference between conventional rail and MGV: the location of the wheelsLike for any gadgetbahn, the claim is that this new technology provides more speed , more comfort at lower cost .But in the real world, these three aspects are always intricately connected and subject to tradeoffs  due to simple geometry and physics. Want faster transportation? Then you need very straight tracks. Dont have straight tracks but still want high speed? Then your trip will become a barf-ride, so less comfort. Want to build cheaply in an already existing highway median? Well the highway curves are made for cars going at 100km/h, so your choices are: slow down (less speed), run inside the existing geometry at higher speed (less comfort), straighten the curves (more expensive).These problems will always come together. At the end of the day, youre still pushing a metal can full of people at high speeds you cant get out of issues of geometry and physics by changing where you put the wheels. Going further, once it appears the basic problems have been overcome, the next issues become capacity, safety, energy and access (stations).For example, Elon Musk is proposing to build tunnels with his Boring Company, which will supposedly be cheap, because the tunnels will be relatively narrow. This reduces capacity, because only smaller vehicles will fit in the tunnel. To compensate, he may propose that vehicles will run super close together  which will represent a huge safety issue unless they run slowly. A small tunnel with a lot of vehicles that are barely smaller than the tunnel diameter, running at high speed may become a death trap in case of emergency, if there isnt enough space and facilities for egress. If somehow he manages to solve all these issues, theres still the problem of getting everybody into this tube of his. All these considerations come down to the one fundamental constraint on which everything else depends: the right of way  how much space do you have available, how straight is your path, how and where is the downtown access, and how much space is available at stations. The MaglevAn example of this issue is the maglev technology (essentially magnetically elevated monorails). The concept has been around for ages. The Germans and Japanese have been developing the technologies for a long time. In the early 2000s, the Germans were able to sell their Transrapid technology to China for the Shanghai Maglev airport connector , a 30-km line. It was a pilot project, with the hope to eventually cover the whole country with maglev network. The Maglev was seen as the next generation of trains, mostly by being faster. But there was also the hope that it could even be cheaper, by putting the whole track on stilts, without having large elevated bridge-like structures. But in the end, conventional high speed rail and maglev require similar geometries, a similar kind of infrastructure, and it turns out that the speeds are not that different (Shanghai Maglev up to 430km/h, conventional rail up to 350km/h, both with a maximum experimental speed of around 600km/h).At more than 300km/h, a lot of energy is spent to overcome the air drag of the trains themselves. Issues also become noise, and the required straightness of the infrastructure. And the issue that with stops, decreasing travel time by increasing maximum speeds becomes marginal. Overall, speeds above 300km/h tend to become uneconomical, no matter the propulsion system. From a distance, elevated conventional high speed rail (left) and maglev (right) appear strangely similar  the requirements on the tracks are due to physics and geometry, so the propulsion method alone wont give one system a clear advantage over the other in terms of infrastructure costAll of these issues come together to make the technological choice a bit of a wash. In the end it makes more economic sense to build from conventional technology which has been developed for a longer time, has multiple vendors, has existing infrastructure that can be built on top of. This way, you can upgrade lines for high speed service, while being able to run the new high speed trains on existing tracks inside cities or to other cities. This provides a huge economic advantage. So indeed, although the Shanghai Maglev works, in an economic sense its a failure. It convinced the Chinese to focus on high speed rail: ten years after the opening of the Maglev, the country still had the same 30km of maglev, but built 20,000km of high speed rail. (Oh and btw, Ive taken the maglev, its not a very smooth ride; it rumbles like a rollercoaster)The Real Issue: EconomicsThe real issue of using a gadgetbahn to solve a transportation problem is not really technical. Its that it re-frames the problem to build an infrastructure as the problem to develop a new technology  now youve got two problems to solve! The technical problem should really be solved through private investment, not public funding. Thus I view the offer to develop the high speed mono-rail for only 250M$ with a great amount of distrust. If the idea is viable and the people behind it are competent, it should have attracted private investment, as there should be a potential to make profit selling the technology. After all, the proposal has been around for 23 years .When proposing a completely new technology in order to solve a specific transportation problem, the major problem is Risk: Technological risks : Will it even work? Will it deliver on claims? Will it have sufficient capacity? Cost-related risks : How much will it cost to develop? How much will one kilometre cost once the technology developed? Regulatory/safety risks : Will it be safe? What will safety requirements be? How fast will we be allowed to run? Risks because timelines are not understood : How long will it take to develop? How long will it take to plan & construct? Operating Risks : How much will it cost to operate? How much will it cost to maintain the infrastructure? Will automated/unattended operation actually be possible? Risks because we need to build complete systems at once , rather than incrementally updating existing infrastructure. Also, new technology means relying on a single vendor , which is again extremely risky, both in terms of cost and availability of the technology in the future: Will the vendor exist in the future? How much will they charge us once we depend on them, given the lack of competition? For any technology thats already existed and been researched for decades, and that may have tens of thousands or hundreds of thousands of kilometers of infrastructure built today, all these question are much easier to answer. There will be much less risk, and thus less cost. Be distrustful of Gadgetbahn ConceptsAny time somebody in power proposes to solve an infrastructure problem by first developing some new technology, or by using some proposed technology that hasnt been delployed yet, we should be distrustful. Often, the proposal may simply be an excuse to not invest in infrastructure today, because tomorrow some techno-fix will come along and solve all our problems for free. This tactic may work especially well if the proposal includes an appeal to some futuristic dream or a nationalistic project. The gadgetbahn may really just be a big diversion. For example in Quebec, the most realistic scenario to get a fast link between Montreal and Quebec is the high-frequency train proposed by VIA rail , which would link the cities with conventional rail. After repeatedly proposing high speed rail for the last 40 years and getting no support from the governments, VIA decided to propose a system thats not true high speed rail  but it would use dedicated passenger rail tracks, allowing somewhat faster speed than today, and no interference with freight. This would hit an economic sweet spot for VIA, allowing them to almost finance and build the system themselves, with only relatively little public support. But the main problem is the access to Montreal  VIA hoped to use the Mont-Royal rail tunnel to access Gare Centrale. But with the REM light rail project pushed hard by the prime minister, the tunnel would be converted to light rail, an incompatible technology, which will cut VIA off . In some sense, the prime ministers announcement that he wants some gadgetbahn to Quebec City means VIAs rail project would become obsolete, and the conflict would become moot. So is it really just a diversionary tactic to hide the regional rail planning problems in Montreal? Interesting related reading (and with thanks for some inspirations) by Alon Levy: Loopy Ideas Are Fine, If Youre an EntrepreneurPublished on Sunday, December 3rd, 2017 at 20:30 Filed under Montréal , technology , trains . Comments RSS | trackback\n",
      "Sentences: 83\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "Whatâs a Gadgetbahn? December 3rd, 2017 by ant6nFor some time, Iâve been meaning to write about German transportation systems like whatâs an S-Bahn or whatâs a Stadtbahn. With the recent news out of Quebec, I figured Iâd instead talk about a transportation concept that doesnât actually transport anybody at all: the Gadgetbahn. The word is a portemanteau of the English âGadgetâ and the German word âbahnâ, which means rail or train. A gadgetbahn is a speculative transportation concept that proposes to solve planning and financial issues via some sort of magical techno-fix, likely some technology that doesnât even exist yet. Classic examples of gadgetbahns are: monorails, âpersonal rapid transitâ, maglevs, or the newest addition to the family, the âhyperloopâ. Proponents of these technologies may be referred to as gadgetbahn enthusiasts, or more derogatorily, â pod-people â. They often promise the sky in terms of reduced cost and increased speed and comfort, often with little consideration for capacity, risk, safety or realism. Back to ÂQuebec: the prime minister decided he wants a fast transit link between Montreal and Quebec City, but not a train, because â we can do much more modern things now â.Instead, he wants something âfuturisticâ, something from the minds of Quebecers. (note from editor: Sorry to pop his bubble, but Quebec is not exactly known for having a long history on the bleeding edge of transit technology. )The problem of the gadgetbahn isnât necessarily that itâs a techno-fix. Itâs that it is a technology for the sake of technology, a shiny gizmo to brag about, with little regard to solving the actual transportation problem. The transportation minister clarified his position himself: he wants anything you can conceive of, any project; innovate, come up with new ideas! And somehow, some consortium sprung up ready with proposals, renderings, promises and deadlines to build a high speed monorail (âMGVâ), and all they want is a quarter of a billion dollars to develop it. Cost, Speed, Comfort The problem is geometry, physics and the Right of WayThe beauty of proposing a gadgetbahn is that since it doesnât exist, proponents can make up all sorts of quasi-magical properties for their technology, which supposedly make it superior. Since there arenât real-world examples, proponents can use the most optimistic theoretical scenarios they can come up with, and compare them with the actual performance of projects that have been built and which are bound to the constraints of the real world. For example, the high speed monorail promoters claim their system is cheaper than conventional rail, because they could just use existing highway medians, with an elevated rail system where vehicles are suspended from above. The MGV (monorail a grande vitesse), high-speed pods on elevated tracks, suspended from aboveWhen you look at its basic structure, compared to conventional rail, this monorail essentially differs in the method of propulsion: instead of two wheels on two rails below the train, we have two wheels on one rail above the train. The main conceptual difference between conventional rail and âMGVâ:the location of the wheelsLike for any gadgetbahn, the claim is that this new technology provides more speed, more comfort at lower cost. But in the real world, these three aspects are always intricately connected and subject to tradeoffs  due to simple geometry and physics. Want faster transportation? Then you need very straight tracks. Donât have straight tracks but still want high speed? Then your trip will become a barf-ride, so less comfort. ÂWant to build cheaply in an already existing highway median? Well the highway curves are made for cars going at 100km/h, so your choices are:slow down (less speed),run inside the existing geometry at higher speed (less comfort),straighten the curves (more expensive).These problems will always come together. At the end of the day, youâre still pushing a metal can full of people at high speeds you canât get out of issues of geometry and physics by changing where you put the wheels. Going further, once it appears the basic problems have been overcome, the next issues become capacity, safety, energy and access (stations).For example, Elon Musk is proposing to build tunnels with his âBoring Companyâ, which will supposedly be cheap, because the tunnels will be relatively narrow. This reduces capacity, because only smaller vehicles will fit in the tunnel. To compensate, he may propose that vehicles will run super close together  which will represent a huge safety issue unless they run slowly. A small tunnel with a lot of vehicles that are barely smaller than the tunnel diameter, running at high speed may become a death trap in case of emergency, if there isnât enough space and facilities for egress. If somehow he manages to solve all these issues, thereâs still the problem of getting everybody into this tube of his. All these considerations come down to the one fundamental constraint on which everything else depends: the right of way  how much space do you have available, how straight is your path, how and where is the downtown access, and how much space is available at stations. The MaglevAn example of this issue is the maglev technology (essentially magnetically elevated monorails). The concept has been around for ages. The Germans and Japanese have been developing the technologies for a long time. In the early 2000s, the Germans were able to sell their Transrapid technology to China for the Shanghai Maglev airport connector , a 30-km line. It was a pilot project, with the hope to eventually cover the whole country with maglev network. The Maglev was seen as the next generation of trains, mostly by being faster. But there was also the hope that it could even be cheaper, by putting the whole track on stilts, without having large elevated bridge-like structures. But in the end, conventional high speed rail and maglev require similar geometries, a similar kind of infrastructure, and it turns out that the speeds are not that different (Shanghai Maglev up to 430km/h, conventional rail up to 350km/h, both with a maximum experimental speed of around 600km/h).At more than 300km/h, a lot of energy is spent to overcome the air drag of the trains themselves. Issues also become noise, and the required straightness of the infrastructure. And the issue that with stops, decreasing travel time by increasing maximum speeds becomes marginal. Overall, speeds above 300km/h tend to become uneconomical, no matter the propulsion system. From a distance, elevated conventional high speed rail (left) and maglev (right) appear strangely similar  the requirements on the tracks are due to physics and geometry, so the propulsion method alone wonât give one system a clear advantage over the other in terms of infrastructure costAll of these issues come together to make the technological choice a bit of a wash. In the end it makes more economic sense to build from conventional technology which has been developed for a longer time, has multiple vendors, has existing infrastructure that can be built on top of. This way, you can upgrade lines for high speed service, while being able to run the new high speed trains on existing tracks inside cities or to other cities. This provides a huge economic advantage. So indeed, although the Shanghai Maglev works, in an economic sense itâs a failure. It convinced the Chinese to focus on high speed rail: ten years after the opening of the Maglev, the country still had the same 30km of maglev, but built 20,000km of high speed rail. (Oh and btw, Iâve taken the maglev, itâs not a very smooth ride; it rumbles like a rollercoaster)The Real Issue: EconomicsThe real issue of using a gadgetbahn to solve a transportation problem is not really technical. Itâs that it re-frames the problem to build an infrastructure as the problem to develop a new technology  now youâve got two problems to solve! The technical problem should really be solved through private investment, not public funding. Thus I view the âofferâ to develop the âhigh speed mono-railâ for âonlyâ 250M$ with a great amount of distrust. If the idea is viable and the people behind it are competent, it should have attracted private investment, as there should be a potential to make profit selling the technology. After all, the proposal has been around for 23 years .When proposing a completely new technology in order to solve a specific transportation problem, the major problem is Risk:Technological risks: Will it even work? Will it deliver on claims? Will it have sufficient capacity? Cost-related risks: How much will it cost to develop? How much will one kilometre cost once the technology developed? Regulatory/safety risks: Will it be safe? What will safety requirements be? How fast will we be allowed to run? Risks because timelines are not understood: How long will it take to develop? How long will it take to plan & construct? Operating Risks: How much will it cost to operate? How much will it cost to maintain the infrastructure? Will automated/unattended operation actually be possible? Risks because we need to build complete systems at once, rather than incrementally updating existing infrastructure. Also, new technology means relying on a single vendor, which is again extremely risky, both in terms of cost and availability of the technology in the future: Will the vendor exist in the future? How much will they charge us once we depend on them, given the lack of competition? For any technology thatâs already existed and been researched for decades, and that may have tens of thousands or hundreds of thousands of kilometers of infrastructure built today, all these question are much easier to answer. There will be much less risk, and thus less cost. Be distrustful of Gadgetbahn ConceptsAny time somebody in power proposes to solve an infrastructure problem by first developing some new technology, or by using some proposed technology that hasnât been delployed yet, we should be distrustful. Often, the proposal may simply be an excuse to not invest in infrastructure today, because tomorrow some techno-fix will come along and solve all our problems âfor freeâ. This tactic may work especially well if the proposal includes an appeal to some futuristic dream or a nationalistic project. The gadgetbahn may really just be a big diversion. For example in Quebec, the most realistic scenario to get a fast link between Montreal and Quebec is the âhigh-frequency trainâ proposed by VIA rail , which would link the cities with conventional rail. After repeatedly proposing high speed rail for the last 40 years and getting no support from the governments, VIA decided to propose a system thatâs not true high speed rail  but it would use dedicated passenger rail tracks, allowing somewhat faster speed than today, and no interference with freight. This would hit an economic sweet spot for VIA, allowing them to almost finance and build the system themselves, with only relatively little public support. But the main problem is the access to Montreal  VIA hoped to use the Mont-Royal rail tunnel to access Gare Centrale. But with the REM light rail project pushed hard by the prime minister, the tunnel would be converted to light rail, an incompatible technology, which will cut VIA off . In some sense, the prime ministerâs announcement that he wants some gadgetbahn to Quebec City means VIAs rail project would become obsolete, and the conflict would become moot. So is it really just a diversionary tactic to hide the regional rail planning problems in Montreal?\n",
      "Sentences: 84\n",
      "\n",
      "\n",
      "Article: https://source.android.com/security/bulletin/2019-02-01.html\n",
      "NodeRank:\n",
      "Published February 4, 2019The Android Security Bulletin contains details of security vulnerabilitiesaffecting Android devices. Security patch levels of 2019-02-05 or later addressall of these issues. To learn how to check a device's security patch level, seeCheck and update your Android version. Android partners are notified of all issues at least a month beforepublication. Source code patches for these issues have been released to theAndroid Open Source Project (AOSP) repository and linked from this bulletin. This bulletin also includes links to patches outside of AOSP. The most severe of these issues is a critical security vulnerability inFramework that could allow a remote attacker using a specially craftedPNG file to execute arbitrary code within the context of a privileged process. The severity assessment is based on the effect that exploiting thevulnerability would possibly have on an affected device, assumingthe platform and service mitigations are turned off for developmentpurposes or if successfully bypassed. We have had no reports of active customer exploitation or abuse of these newlyreported issues. Refer to theAndroid and Google Play Protect mitigationssection for details on theAndroid security platform protectionsand Google Play Protect, which improve the security of the Android platform. Note: Information on the latest over-the-air update (OTA) andfirmware images for Google devices is available in theFebruary 2019Pixel Update Bulletin. This is a summary of the mitigations provided by theAndroid security platformand service protections such asGoogle PlayProtect. These capabilities reduce the likelihood that securityvulnerabilities could be successfully exploited on Android. Exploitation for many issues on Android is made more difficult byenhancements in newer versions of the Android platform. We encourage all usersto update to the latest version of Android where possible. The Android security team actively monitors for abuse throughGoogle PlayProtect and warns users aboutPotentiallyHarmful Applications. Google Play Protect is enabled by default on deviceswith Google MobileServices, and is especially important for users who install apps fromoutside of Google Play. In the sections below, we provide details for each of the securityvulnerabilities that apply to the 2019-02-01 patch level. Vulnerabilities aregrouped under the component they affect. There is a description of theissue and a table with the CVE, associated references,type of vulnerability,severity,and updated AOSP versions (where applicable). When available, we link the publicchange that addressed the issue to the bug ID, such as the AOSP change list. Whenmultiple changes relate to a single bug, additional references are linked tonumbers following the bug ID. The most severe vulnerability in this section could enable a remote attackerusing a specially crafted PNG file to execute arbitrary code within the contextof a privileged process. CVEReferencesTypeSeverityUpdated AOSP versionsCVE-2019-1986A-117838472 [2]RCECritical9CVE-2019-1987A-118143775 [2]RCECritical7. 0, 7.1.1, 7.1.2, 8.0, 8.1, 9CVE-2019-1988A-118372692RCECritical8. 0, 8.1, 9The most severe vulnerability in this section could enable a remote attackerusing a specially crafted file to execute arbitrary code within the context ofan unprivileged process. CVEReferencesTypeSeverityUpdated AOSP versionsCVE-2017-17760A-78029030*RCEHigh7. 0, 7.1.1, 7.1.2, 8.0, 8.1, 9CVE-2018-5268A-78029634*RCEHigh7. 0, 7.1.1, 7.1.2, 8.0, 8.1, 9CVE-2018-5269A-78029727*RCEHigh7. 0, 7.1.1, 7.1.2, 8.0, 8.1, 9CVE-2017-18009A-78026242*IDModerate7. 0, 7.1.1, 7.1.2, 8.0, 8.1, 9The most severe vulnerability in this section could enable a remote attackerusing a specially crafted transmission to execute arbitrary code within thecontext of a privileged process. CVEReferencesTypeSeverityUpdated AOSP versionsCVE-2019-1991A-110166268RCECritical7. 0, 7.1.1, 7.1.2, 8.0, 8.1, 9CVE-2019-1992A-116222069RCECritical7. 0, 7.1.1, 7.1.2, 8.0, 8.1, 9CVE-2019-1993A-119819889EoPHigh8. 0, 8.1, 9CVE-2019-1994A-117770924EoPHigh8. 0, 8.1, 9CVE-2019-1995A-32589229 [2]IDHigh7. 0, 7.1.1, 7.1.2, 8.0, 8.1, 9CVE-2019-1996A-111451066IDHigh8. 0, 8.1, 9CVE-2019-1997A-117508900IDHigh7. 0, 7.1.1, 7.1.2, 8.0, 8.1, 9CVE-2019-1998A-116055338 [2]DoSHigh9In the sections below, we provide details for each of the securityvulnerabilities that apply to the 2019-02-05 patch level. Vulnerabilities aregrouped under the component they affect and include details such as theCVE, associated references, type of vulnerability,severity,component (where applicable), and updated AOSP versions (where applicable). Whenavailable, we link the public change that addressed the issue to the bug ID,such as the AOSP change list. When multiple changes relate to a single bug,additional references are linked to numbers following the bug ID. The most severe vulnerability in this section could enable a local maliciousapplication to execute arbitrary code within the context of a privilegedprocess. CVEReferencesTypeSeverityComponentCVE-2018-10879A-116406063Upstream kernelEoPHighext4 filesystemCVE-2019-1999A-120025196*EoPHighBinder driverCVE-2019-2000A-120025789*EoPHighBinder driverCVE-2019-2001A-117422211*IDHighiomemThe most severe vulnerability in this section could enable a remote attackerusing a specially crafted file to execute arbitrary code within the context ofa privileged process. CVEReferencesTypeSeverityComponentCVE-2018-6271A-80198474*RCECriticallibnvomxCVE-2018-6267A-70857947*EoPHighlibnvomxCVE-2018-6268A-80433161*EoPHighlibnvomxCVE-2016-6684A-117423758*IDHighkernel logThese vulnerabilities affect Qualcomm components and are described infurther detail in the appropriate Qualcomm security bulletin or security alert. The severity assessment of these issues is provided directly by Qualcomm. CVEReferencesTypeSeverityComponentCVE-2018-11262A-76424945QC-CR#2221192N/ACriticalbootloaderCVE-2018-11280A-109741776QC-CR#2185061N/AHighModemCVE-2018-11275A-74409078QC-CR#2221256 [2]N/AHighBootloaderCVE-2018-13900A-119052051QC-CR#2287499N/AHighModemCVE-2018-13905A-119052050QC-CR#2225202N/AHighGraphicsThese vulnerabilities affect Qualcomm components and are described infurther detail in the appropriate Qualcomm security bulletin or security alert. The severity assessment of these issues is provided directly by Qualcomm. CVEReferencesTypeSeverityComponentCVE-2018-11289A-109678453*N/ACriticalClosed-source componentCVE-2018-11820A-111089815*N/ACriticalClosed-source componentCVE-2018-11938A-112279482*N/ACriticalClosed-source componentCVE-2018-11945A-112278875*N/ACriticalClosed-source componentCVE-2018-11268A-109678259*N/AHighClosed-source componentCVE-2018-11845A-111088838*N/AHighClosed-source componentCVE-2018-11864A-111092944*N/AHighClosed-source componentCVE-2018-11921A-112278972*N/AHighClosed-source componentCVE-2018-11931A-112279521*N/AHighClosed-source componentCVE-2018-11932A-112279426*N/AHighClosed-source componentCVE-2018-11935A-112279483*N/AHighClosed-source componentCVE-2018-11948A-112279144*N/AHighClosed-source componentCVE-2018-5839A-112279544*N/AHighClosed-source componentCVE-2018-13904A-119050566*N/AHighClosed-source componentThis section answers common questions that may occur after reading thisbulletin. 1. How do I determine if my device is updated to address theseissues? To learn how to check a device's security patch level, seeCheck and update your Android version. Security patch levels of 2019-02-01 or later address all issues associatedwith the 2019-02-01 security patch level. Security patch levels of 2019-02-05 or later address all issues associatedwith the 2019-02-05 security patch level and all previous patch levels. Device manufacturers that include these updates should set the patch stringlevel to:[ro. build. version. security_patch]:[2019-02-01][ro. build. version. security_patch]:[2019-02-05]2. Why does this bulletin have two security patch levels? This bulletin has two security patch levels so that Android partners have theflexibility to fix a subset of vulnerabilities that are similar across allAndroid devices more quickly. Android partners are encouraged to fix all issuesin this bulletin and use the latest security patch level. Devices that use the 2019-02-01 security patch level must include allissues associated with that security patch level, as well as fixes for allissues reported in previous security bulletins. Devices that use the security patch level of 2019-02-05 or newer mustinclude all applicable patches in this (and previous) securitybulletins. Partners are encouraged to bundle the fixes for all issues they are addressingin a single update. 3. What do the entries in the Type column mean? Entries in the Type column of the vulnerability details tablereference the classification of the security vulnerability. AbbreviationDefinitionRCERemote code executionEoPElevation of privilegeIDInformation disclosureDoSDenial of serviceN/AClassification not available4. What do the entries in the References column mean? Entries under the References column of the vulnerability details tablemay contain a prefix identifying the organization to which the reference valuebelongs. PrefixReferenceA-Android bug IDQC-Qualcomm reference numberM-MediaTek reference numberN-NVIDIA reference numberB-Broadcom reference number5. What does a * next to the Android bug ID in the Referencescolumn mean? Issues that are not publicly available have a * next to the Android bug ID inthe References column. The update for that issue is generallycontained in the latest binary drivers for Pixel devicesavailable from theGoogleDeveloper site. 6. Why are security vulnerabilities split between this bulletin anddevice&hairsp;/&hairsp;partner security bulletins, such as thePixel bulletin? Security vulnerabilities that are documented in this security bulletin arerequired to declare the latest security patch level on Androiddevices. Additional security vulnerabilities that are documented in thedevice&hairsp;/&hairsp;partner security bulletins are not required fordeclaring a security patch level. Android device and chipset manufacturers areencouraged to document the presence of other fixes on their devices throughtheir own security websites, such as theSamsung,LGE, orPixel security bulletins. VersionDateNotes1. 0February 4, 2019Bulletin published\n",
      "Sentences: 84\n",
      "\n",
      "\n",
      "Dragnet:\n",
      "Published February 4, 2019The Android Security Bulletin contains details of security vulnerabilities affecting Android devices. Security patch levels of 2019-02-05 or later address all of these issues. To learn how to check a device's security patch level, see Check and update your Android version .Android partners are notified of all issues at least a month before publication. Source code patches for these issues have been released to the Android Open Source Project (AOSP) repository and linked from this bulletin. This bulletin also includes links to patches outside of AOSP. The most severe of these issues is a critical security vulnerability in Framework that could allow a remote attacker using a specially crafted PNG file to execute arbitrary code within the context of a privileged process. The severity assessment is based on the effect that exploiting the vulnerability would possibly have on an affected device, assuming the platform and service mitigations are turned off for development purposes or if successfully bypassed. We have had no reports of active customer exploitation or abuse of these newly reported issues. Refer to the Android and Google Play Protect mitigations section for details on the Android security platform protections and Google Play Protect, which improve the security of the Android platform. Note: Information on the latest over-the-air update (OTA) and firmware images for Google devices is available in the February 2019 Pixel Update Bulletin .Android and Google service mitigationsThis is a summary of the mitigations provided by the Android security platform and service protections such as Google Play Protect . These capabilities reduce the likelihood that security vulnerabilities could be successfully exploited on Android. Exploitation for many issues on Android is made more difficult by enhancements in newer versions of the Android platform. We encourage all users to update to the latest version of Android where possible. The Android security team actively monitors for abuse through Google Play Protect and warns users about Potentially Harmful Applications . Google Play Protect is enabled by default on devices with Google Mobile Services , and is especially important for users who install apps from outside of Google Play. 2019-02-01 security patch level vulnerability detailsIn the sections below, we provide details for each of the security vulnerabilities that apply to the 2019-02-01 patch level. Vulnerabilities are grouped under the component they affect. There is a description of the issue and a table with the CVE, associated references, type of vulnerability , severity , and updated AOSP versions (where applicable). When available, we link the public change that addressed the issue to the bug ID, such as the AOSP change list. When multiple changes relate to a single bug, additional references are linked to numbers following the bug ID. The most severe vulnerability in this section could enable a remote attacker using a specially crafted PNG file to execute arbitrary code within the context of a privileged process. CVE References Type Severity Updated AOSP versions CVE-2019-1986 A-117838472 [ 2 ] RCE Critical 9 CVE-2019-1987 A-118143775 [ 2 ] RCE Critical 7.0, 7.1.1, 7.1.2, 8.0, 8.1, 9 CVE-2019-1988 A-118372692 RCE Critical 8.0, 8.1, 9The most severe vulnerability in this section could enable a remote attacker using a specially crafted file to execute arbitrary code within the context of an unprivileged process. CVE References Type Severity Updated AOSP versions CVE-2017-17760 A-78029030 * RCE High 7.0, 7.1.1, 7.1.2, 8.0, 8.1, 9 CVE-2018-5268 A-78029634 * RCE High 7.0, 7.1.1, 7.1.2, 8.0, 8.1, 9 CVE-2018-5269 A-78029727 * RCE High 7.0, 7.1.1, 7.1.2, 8.0, 8.1, 9 CVE-2017-18009 A-78026242 * ID Moderate 7.0, 7.1.1, 7.1.2, 8.0, 8.1, 9The most severe vulnerability in this section could enable a remote attacker using a specially crafted transmission to execute arbitrary code within the context of a privileged process. CVE References Type Severity Updated AOSP versions CVE-2019-1991 A-110166268 RCE Critical 7.0, 7.1.1, 7.1.2, 8.0, 8.1, 9 CVE-2019-1992 A-116222069 RCE Critical 7.0, 7.1.1, 7.1.2, 8.0, 8.1, 9 CVE-2019-1993 A-119819889 EoP High 8.0, 8.1, 9 CVE-2019-1994 A-117770924 EoP High 8.0, 8.1, 9 CVE-2019-1995 A-32589229 [ 2 ] ID High 7.0, 7.1.1, 7.1.2, 8.0, 8.1, 9 CVE-2019-1996 A-111451066 ID High 8.0, 8.1, 9 CVE-2019-1997 A-117508900 ID High 7.0, 7.1.1, 7.1.2, 8.0, 8.1, 9 CVE-2019-1998 A-116055338 [ 2 ] DoS High 92019-02-05 security patch level vulnerability detailsIn the sections below, we provide details for each of the security vulnerabilities that apply to the 2019-02-05 patch level. Vulnerabilities are grouped under the component they affect and include details such as the CVE, associated references, type of vulnerability , severity , component (where applicable), and updated AOSP versions (where applicable). When available, we link the public change that addressed the issue to the bug ID, such as the AOSP change list. When multiple changes relate to a single bug, additional references are linked to numbers following the bug ID. Kernel componentsThe most severe vulnerability in this section could enable a local malicious application to execute arbitrary code within the context of a privileged process. CVE References Type Severity Component CVE-2018-10879 A-116406063 Upstream kernel EoP High ext4 filesystem CVE-2019-1999 A-120025196 * EoP High Binder driver CVE-2019-2000 A-120025789 * EoP High Binder driver CVE-2019-2001 A-117422211 * ID High iomemNVIDIA componentsThe most severe vulnerability in this section could enable a remote attacker using a specially crafted file to execute arbitrary code within the context of a privileged process. CVE References Type Severity Component CVE-2018-6271 A-80198474 * RCE Critical libnvomx CVE-2018-6267 A-70857947 * EoP High libnvomx CVE-2018-6268 A-80433161 * EoP High libnvomx CVE-2016-6684 A-117423758 * ID High kernel logQualcomm componentsThese vulnerabilities affect Qualcomm components and are described in further detail in the appropriate Qualcomm security bulletin or security alert. The severity assessment of these issues is provided directly by Qualcomm. CVE References Type Severity Component CVE-2018-11262 A-76424945 QC-CR#2221192 N/A Critical bootloader CVE-2018-11280 A-109741776 QC-CR#2185061 N/A High Modem CVE-2018-11275 A-74409078 QC-CR#2221256 [ 2 ] N/A High Bootloader CVE-2018-13900 A-119052051 QC-CR#2287499 N/A High Modem CVE-2018-13905 A-119052050 QC-CR#2225202 N/A High GraphicsQualcomm closed-source componentsThese vulnerabilities affect Qualcomm components and are described in further detail in the appropriate Qualcomm security bulletin or security alert. The severity assessment of these issues is provided directly by Qualcomm. CVE References Type Severity Component CVE-2018-11289 A-109678453 * N/A Critical Closed-source component CVE-2018-11820 A-111089815 * N/A Critical Closed-source component CVE-2018-11938 A-112279482 * N/A Critical Closed-source component CVE-2018-11945 A-112278875 * N/A Critical Closed-source component CVE-2018-11268 A-109678259 * N/A High Closed-source component CVE-2018-11845 A-111088838 * N/A High Closed-source component CVE-2018-11864 A-111092944 * N/A High Closed-source component CVE-2018-11921 A-112278972 * N/A High Closed-source component CVE-2018-11931 A-112279521 * N/A High Closed-source component CVE-2018-11932 A-112279426 * N/A High Closed-source component CVE-2018-11935 A-112279483 * N/A High Closed-source component CVE-2018-11948 A-112279144 * N/A High Closed-source component CVE-2018-5839 A-112279544 * N/A High Closed-source component CVE-2018-13904 A-119050566 * N/A High Closed-source componentCommon questions and answersThis section answers common questions that may occur after reading this bulletin. 1. How do I determine if my device is updated to address these issues? To learn how to check a device's security patch level, see Check and update your Android version . Security patch levels of 2019-02-01 or later address all issues associated with the 2019-02-01 security patch level. Security patch levels of 2019-02-05 or later address all issues associated with the 2019-02-05 security patch level and all previous patch levels. Device manufacturers that include these updates should set the patch string level to: [ro. build. version. security_patch]:[2019-02-01] [ro. build. version. security_patch]:[2019-02-05]2. Why does this bulletin have two security patch levels? This bulletin has two security patch levels so that Android partners have the flexibility to fix a subset of vulnerabilities that are similar across all Android devices more quickly. Android partners are encouraged to fix all issues in this bulletin and use the latest security patch level. Devices that use the 2019-02-01 security patch level must include all issues associated with that security patch level, as well as fixes for all issues reported in previous security bulletins. Devices that use the security patch level of 2019-02-05 or newer must include all applicable patches in this (and previous) security bulletins. Partners are encouraged to bundle the fixes for all issues they are addressing in a single update. 3. What do the entries in the Type column mean? Entries in the Type column of the vulnerability details table reference the classification of the security vulnerability. Abbreviation Definition RCE Remote code execution EoP Elevation of privilege ID Information disclosure DoS Denial of service N/A Classification not available4. What do the entries in the References column mean? Entries under the References column of the vulnerability details table may contain a prefix identifying the organization to which the reference value belongs. Prefix Reference A- Android bug ID QC- Qualcomm reference number M- MediaTek reference number N- NVIDIA reference number B- Broadcom reference number5. What does a * next to the Android bug ID in the References column mean? Issues that are not publicly available have a * next to the Android bug ID in the References column. The update for that issue is generally contained in the latest binary drivers for Pixel devices available from the Google Developer site .6. Why are security vulnerabilities split between this bulletin and device&hairsp;/&hairsp;partner security bulletins, such as the Pixel bulletin? Security vulnerabilities that are documented in this security bulletin are required to declare the latest security patch level on Android devices. Additional security vulnerabilities that are documented in the device&hairsp;/&hairsp;partner security bulletins are not required for declaring a security patch level. Android device and chipset manufacturers are encouraged to document the presence of other fixes on their devices through their own security websites, such as the Samsung , LGE , or Pixel security bulletins.\n",
      "Sentences: 66\n",
      "\n",
      "\n",
      "Boilerpipe:\n",
      "Android Security Bulletin  February 2019Published February 4, 2019The Android Security Bulletin contains details of security vulnerabilities affecting Android devices. Security patch levels of 2019-02-05 or later address all of these issues. To learn how to check a device's security patch level, see Check and update your Android version .Android partners are notified of all issues at least a month before publication. Source code patches for these issues have been released to the Android Open Source Project (AOSP) repository and linked from this bulletin. This bulletin also includes links to patches outside of AOSP. The most severe of these issues is a critical security vulnerability in Framework that could allow a remote attacker using a specially crafted PNG file to execute arbitrary code within the context of a privileged process. The severity assessment is based on the effect that exploiting the vulnerability would possibly have on an affected device, assuming the platform and service mitigations are turned off for development purposes or if successfully bypassed. We have had no reports of active customer exploitation or abuse of these newly reported issues. Refer to the Android and Google Play Protect mitigations section for details on the Android security platform protections and Google Play Protect, which improve the security of the Android platform. Note: Information on the latest over-the-air update (OTA) and firmware images for Google devices is available in the February 2019 Pixel Update Bulletin .Android and Google service mitigationsThis is a summary of the mitigations provided by the Android security platform and service protections such as Google Play Protect . These capabilities reduce the likelihood that security vulnerabilities could be successfully exploited on Android. Exploitation for many issues on Android is made more difficult by enhancements in newer versions of the Android platform. We encourage all users to update to the latest version of Android where possible. The Android security team actively monitors for abuse through Google Play Protect and warns users about Potentially Harmful Applications . Google Play Protect is enabled by default on devices with Google Mobile Services , and is especially important for users who install apps from outside of Google Play. 2019-02-01 security patch level vulnerability detailsIn the sections below, we provide details for each of the security vulnerabilities that apply to the 2019-02-01 patch level. Vulnerabilities are grouped under the component they affect. There is a description of the issue and a table with the CVE, associated references, type of vulnerability , severity , and updated AOSP versions (where applicable). When available, we link the public change that addressed the issue to the bug ID, such as the AOSP change list. When multiple changes relate to a single bug, additional references are linked to numbers following the bug ID. FrameworkThe most severe vulnerability in this section could enable a remote attacker using a specially crafted PNG file to execute arbitrary code within the context of a privileged process. CVE\n",
      "Sentences: 22\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "RENDER_ENDPOINT_AWS = 'https://xxxxxxxxxxxxxxx.xxxxxxxxxx.xxx/v1/'\n",
    "\n",
    "for url in articles:\n",
    "    \n",
    "    print('Article:', url)\n",
    "    rurl = RENDER_ENDPOINT_AWS + url\n",
    "    html = \"\"\n",
    "    \n",
    "    try:\n",
    "        html = extract_html(rurl, timeout=60)\n",
    "        if \"Internal server error\" in html:\n",
    "            raise Exception('Internal server error')\n",
    "    except:\n",
    "        html = extract_html(url, timeout=60)\n",
    "    \n",
    "    results[url] = {}\n",
    "    \n",
    "    # NodeRank\n",
    "    print('NodeRank:')\n",
    "    try:\n",
    "        noderank_content = extract_content_noderank(html=html)\n",
    "    except:\n",
    "        print('Error')\n",
    "        noderank_content = \"\"\n",
    "    print(noderank_content)\n",
    "    noderank_sentences = nltk.sent_tokenize(noderank_content)\n",
    "    print('Sentences:',len(noderank_sentences))\n",
    "    results[url]['noderank_sentences'] = len(noderank_sentences)\n",
    "    print('\\n')\n",
    "    \n",
    "    # Dragnet\n",
    "    print('Dragnet:')\n",
    "    try:\n",
    "        dragnet_content = prepare_sentences(extract_content(html))\n",
    "    except:\n",
    "        print('Error')\n",
    "        dragnet_content = \"\"\n",
    "    dragnet_sentences = nltk.sent_tokenize(dragnet_content)\n",
    "    print(dragnet_content)\n",
    "    print('Sentences:',len(dragnet_sentences))\n",
    "    results[url]['dragnet_sentences'] = len(dragnet_sentences)\n",
    "    print('\\n')  \n",
    "    \n",
    "    # NodeRank\n",
    "    print('Boilerpipe:')\n",
    "    try:\n",
    "        boilerpipe_content = prepare_sentences(Extractor(extractor='ArticleExtractor', html=html).getText())\n",
    "    except:\n",
    "        print('Error')\n",
    "        boilerpipe_content = \"\"\n",
    "    boilerpipe_sentences = nltk.sent_tokenize(boilerpipe_content)\n",
    "    print(boilerpipe_content)\n",
    "    print('Sentences:',len(boilerpipe_sentences))\n",
    "    results[url]['boilerpipe_sentences'] = len(boilerpipe_sentences)\n",
    "    print('\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"http://www.cat-bus.com/2017/12/gadgetbahn/\": {\n",
      "        \"boilerpipe_sentences\": 84,\n",
      "        \"dragnet_sentences\": 83,\n",
      "        \"noderank_sentences\": 341\n",
      "    },\n",
      "    \"http://www.greatdisasters.co.uk/the-de-havilland-comet/\": {\n",
      "        \"boilerpipe_sentences\": 164,\n",
      "        \"dragnet_sentences\": 163,\n",
      "        \"noderank_sentences\": 163\n",
      "    },\n",
      "    \"http://www.randomhacks.net/2005/10/11/amb-operator/\": {\n",
      "        \"boilerpipe_sentences\": 73,\n",
      "        \"dragnet_sentences\": 29,\n",
      "        \"noderank_sentences\": 18\n",
      "    },\n",
      "    \"https://alexanderperrin.com.au/triangles/ballooning/\": {\n",
      "        \"boilerpipe_sentences\": 0,\n",
      "        \"dragnet_sentences\": 0,\n",
      "        \"noderank_sentences\": 1\n",
      "    },\n",
      "    \"https://blog.parse.ly/post/7689/analyst-demystify-traffic-google-sends-publishers/\": {\n",
      "        \"boilerpipe_sentences\": 86,\n",
      "        \"dragnet_sentences\": 83,\n",
      "        \"noderank_sentences\": 83\n",
      "    },\n",
      "    \"https://blog.wolfram.com/2019/02/01/the-data-science-of-mathoverflow/\": {\n",
      "        \"boilerpipe_sentences\": 28,\n",
      "        \"dragnet_sentences\": 142,\n",
      "        \"noderank_sentences\": 146\n",
      "    },\n",
      "    \"https://e360.yale.edu/digest/arborists-have-cloned-ancient-redwoods-from-their-massive-stumps\": {\n",
      "        \"boilerpipe_sentences\": 17,\n",
      "        \"dragnet_sentences\": 13,\n",
      "        \"noderank_sentences\": 18\n",
      "    },\n",
      "    \"https://eng.uber.com/introducing-ludwig/\": {\n",
      "        \"boilerpipe_sentences\": 51,\n",
      "        \"dragnet_sentences\": 89,\n",
      "        \"noderank_sentences\": 96\n",
      "    },\n",
      "    \"https://ephemeralnewyork.wordpress.com/2019/02/11/the-bobbed-hair-bandit-on-the-run-in-brooklyn/\": {\n",
      "        \"boilerpipe_sentences\": 41,\n",
      "        \"dragnet_sentences\": 42,\n",
      "        \"noderank_sentences\": 91\n",
      "    },\n",
      "    \"https://github.com/Jeff-Ciesielski/synesthesia\": {\n",
      "        \"boilerpipe_sentences\": 42,\n",
      "        \"dragnet_sentences\": 50,\n",
      "        \"noderank_sentences\": 50\n",
      "    },\n",
      "    \"https://kishorepv.github.io/The-value-of-Incremental_learning/\": {\n",
      "        \"boilerpipe_sentences\": 93,\n",
      "        \"dragnet_sentences\": 93,\n",
      "        \"noderank_sentences\": 94\n",
      "    },\n",
      "    \"https://medium.com/@shnatsel/how-rusts-standard-library-was-vulnerable-for-years-and-nobody-noticed-aebf0503c3d6\": {\n",
      "        \"boilerpipe_sentences\": 121,\n",
      "        \"dragnet_sentences\": 120,\n",
      "        \"noderank_sentences\": 120\n",
      "    },\n",
      "    \"https://opensource.zalando.com/blog/2019/02/Open-Source-Harassment-Policy/\": {\n",
      "        \"boilerpipe_sentences\": 22,\n",
      "        \"dragnet_sentences\": 21,\n",
      "        \"noderank_sentences\": 5\n",
      "    },\n",
      "    \"https://remysharp.com/2019/02/12/cern-day-1\": {\n",
      "        \"boilerpipe_sentences\": 35,\n",
      "        \"dragnet_sentences\": 30,\n",
      "        \"noderank_sentences\": 24\n",
      "    },\n",
      "    \"https://source.android.com/security/bulletin/2019-02-01.html\": {\n",
      "        \"boilerpipe_sentences\": 22,\n",
      "        \"dragnet_sentences\": 66,\n",
      "        \"noderank_sentences\": 84\n",
      "    },\n",
      "    \"https://techcrunch.com/2019/02/11/amazon-is-buying-home-mesh-router-startup-eero/\": {\n",
      "        \"boilerpipe_sentences\": 13,\n",
      "        \"dragnet_sentences\": 13,\n",
      "        \"noderank_sentences\": 13\n",
      "    },\n",
      "    \"https://techcrunch.com/2019/02/11/google-docs-gets-an-api-for-task-automation/\": {\n",
      "        \"boilerpipe_sentences\": 8,\n",
      "        \"dragnet_sentences\": 8,\n",
      "        \"noderank_sentences\": 8\n",
      "    },\n",
      "    \"https://techcrunch.com/2019/02/11/us-iphone-users-spent-79-last-year-up-36-from-2017/\": {\n",
      "        \"boilerpipe_sentences\": 26,\n",
      "        \"dragnet_sentences\": 26,\n",
      "        \"noderank_sentences\": 26\n",
      "    },\n",
      "    \"https://www.cbc.ca/news/technology/mars-one-bankrupt-1.5014522\": {\n",
      "        \"boilerpipe_sentences\": 4,\n",
      "        \"dragnet_sentences\": 21,\n",
      "        \"noderank_sentences\": 26\n",
      "    },\n",
      "    \"https://www.cnbc.com/2019/02/12/google-facebook-apple-news-should-be-regulated-uk-government-report.html\": {\n",
      "        \"boilerpipe_sentences\": 24,\n",
      "        \"dragnet_sentences\": 5,\n",
      "        \"noderank_sentences\": 1\n",
      "    },\n",
      "    \"https://www.jasonhickel.org/blog/2019/2/3/pinker-and-global-poverty\": {\n",
      "        \"boilerpipe_sentences\": 161,\n",
      "        \"dragnet_sentences\": 100,\n",
      "        \"noderank_sentences\": 165\n",
      "    },\n",
      "    \"https://www.nowpublishers.com/article/Details/RBE-0092\": {\n",
      "        \"boilerpipe_sentences\": 22,\n",
      "        \"dragnet_sentences\": 1,\n",
      "        \"noderank_sentences\": 26\n",
      "    },\n",
      "    \"https://www.nytimes.com/2019/02/11/health/artificial-intelligence-medical-diagnosis.html\": {\n",
      "        \"boilerpipe_sentences\": 64,\n",
      "        \"dragnet_sentences\": 15,\n",
      "        \"noderank_sentences\": 60\n",
      "    },\n",
      "    \"https://www.nytimes.com/2019/02/11/travel/northern-lights-tourism-in-sweden.html\": {\n",
      "        \"boilerpipe_sentences\": 99,\n",
      "        \"dragnet_sentences\": 0,\n",
      "        \"noderank_sentences\": 127\n",
      "    },\n",
      "    \"https://www.nytimes.com/2019/02/12/magazine/climeworks-business-climate-change.html\": {\n",
      "        \"boilerpipe_sentences\": 305,\n",
      "        \"dragnet_sentences\": 91,\n",
      "        \"noderank_sentences\": 297\n",
      "    },\n",
      "    \"https://www.zdnet.com/article/microsoft-security-chief-ie-is-not-a-browser-so-stop-using-it-as-your-default/\": {\n",
      "        \"boilerpipe_sentences\": 14,\n",
      "        \"dragnet_sentences\": 37,\n",
      "        \"noderank_sentences\": 39\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(results, indent=4, sort_keys=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
